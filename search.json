[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Fourth Handbook Hackathon\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\nChris Hartgerink, Lena Karvovskaya, Jolien Scholten, Tycho Hofstra, Elisa Rodenburg, Stephanie van de Sandt\n\n\n\n\n\n\n\n\n\n\n\n\nWhy was rdm.vu.nl down for ten hours?\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nChris Hartgerink, Lena Karvovskaya\n\n\n\n\n\n\n\n\n\n\n\n\nThird Handbook Hackathon\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nAlex van der Jagt, FGB, sec. KNOP, Chris Hartgerink (host), Diogenes Cruz de Arcelino, Elisa Rodenburg, UBVU, Jessica Hrudey, FGB, Jolien Scholten, UB, Lena Karvovskaya, UBVU, Stephanie van de Sandt, UBVU, Tycho Hofstra, UBVU\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Handbook Hackathon\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2024\n\n\nAlex van der Jagt, Chris Hartgerink, Elisa Rodenburg, Jens de Bruijn, Jolien Scholten, Joy Jiayi Cheng, Lena Karvovskaya, Meron Vermaas, Peter Vos, Stephanie van de Sandt, Tycho Hofstra\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Handbook Hackathon\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2024\n\n\nAlex van der Jagt, Chris Hartgerink, Dimitri Unger, Elisa Rodenburg, Jessica Hrudey, Jolien Scholten, Lena Karvovskaya, Lucy O’ Shea, Mar Barrantes-Cepas, Meron Vermaas, Peter Vos, Stephanie van de Sandt\n\n\n\n\n\n\n\n\n\n\n\n\nHello world!\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "topics/data-and-software-licensing.html",
    "href": "topics/data-and-software-licensing.html",
    "title": "Data Licensing",
    "section": "",
    "text": "A data licence agreement is a legal instrument that lets others know what they can and cannot do with your research data (and any documentation. scripts and metadata that are published with the data). It is important to consider what kind of limitations are relevant. An important component can be a guideline on how people should cite the dataset. Other components could be:\n\nCan people make copies or even distribute copies\nWho should be contacted if you need access to re-use data\nEtc.\n\n\n\n\nAn image of open data, made up of public domain icons\n\n\nIn principle, Dataverse allows you to choose your terms of use. Some data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition). If you need help with drawing up license agreements, you can contact the IXA office.\nAdditional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE license selection wizard, which Pawel Kamocki (et al.) released under an open source license.1"
  },
  {
    "objectID": "topics/data-and-software-licensing.html#licensing-the-data",
    "href": "topics/data-and-software-licensing.html#licensing-the-data",
    "title": "Data Licensing",
    "section": "",
    "text": "A data licence agreement is a legal instrument that lets others know what they can and cannot do with your research data (and any documentation. scripts and metadata that are published with the data). It is important to consider what kind of limitations are relevant. An important component can be a guideline on how people should cite the dataset. Other components could be:\n\nCan people make copies or even distribute copies\nWho should be contacted if you need access to re-use data\nEtc.\n\n\n\n\nAn image of open data, made up of public domain icons\n\n\nIn principle, Dataverse allows you to choose your terms of use. Some data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition). If you need help with drawing up license agreements, you can contact the IXA office.\nAdditional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE license selection wizard, which Pawel Kamocki (et al.) released under an open source license.1"
  },
  {
    "objectID": "topics/data-and-software-licensing.html#footnotes",
    "href": "topics/data-and-software-licensing.html#footnotes",
    "title": "Data Licensing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the source code, see https://github.com/ufal/public-license-selector/↩︎"
  },
  {
    "objectID": "topics/research-data-management.html",
    "href": "topics/research-data-management.html",
    "title": "Research Data Management (RDM)",
    "section": "",
    "text": "RDM concerns the organisation, documentation, storage, archiving and sharing of digital and analogue data. Data management applies throughout the entire research data life cycle, which is visualised in the circle above. RDM aims to ensure reliable verification of results, and permits new and innovative research built on existing information. RDM is also part of the research process and is intended to make the research process as efficient as possible. The VU CS Department RDM and Open Science Handbook provides guidance on research data management planning, data storage and protection, data archiving, and other resources. The Data Management Plan provides information on how these activities will be carried out during the research project.\nGood data management will heighten the quality of your own research (data) as well as your institution’s scientific output, and it contributes significantly to your field as a whole.\nGood data management:\n\nPromotes the integrity of your research,\nIncreases the impact of your research,\nImproves the quality of your data,\nSupports future use of your research data, and\nComplies with internal and external regulations."
  },
  {
    "objectID": "topics/data-storage.html",
    "href": "topics/data-storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "VU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by the VU;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup."
  },
  {
    "objectID": "topics/data-storage.html#storage-during-research",
    "href": "topics/data-storage.html#storage-during-research",
    "title": "Data Storage",
    "section": "",
    "text": "VU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by the VU;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup."
  },
  {
    "objectID": "topics/data-storage.html#standard-services-offered-by-the-vu",
    "href": "topics/data-storage.html#standard-services-offered-by-the-vu",
    "title": "Data Storage",
    "section": "Standard services offered by the VU",
    "text": "Standard services offered by the VU\nVU IT offers several services for employees to store their files. Examples are:\n\n🔒 OneDrive: personal storage for all VU employees and part of the Microsoft 365 platform. OneDrive allows you to store files locally and in the Microsoft cloud, and share folders and documents with colleagues. Since this is personal storage, tied to someone’s personal VU account, we don’t usually recommend storing research data in OneDrive: if the account holder leaves the VU, the account and all the data on it, disappear.\n🔒 Teams. Faculties, divisions and departments have their own Team - part of the Microsoft 365 platform - where they store shared documents and where they can interact and chat. Projects may also request a project team. But note that Teams is not always the best location to store your research data and has several limitations, especially when it comes to working with non-Microsoft file formats, large volumes of data, interacting with data, and collaborating with partners outside of the VU. Contact the RDM Support Desk to find out more about the suitability of Teams for your project.\n🔒 Surfdrive: is a personal cloud storage service for the Dutch education and research community, offering staff, researchers and students an easy way to store, synchronise and share files in the secure and reliable SURF community cloud. All users receive storage space of up to 500 GB. Surfdrive is automatically offered to all VU employees. Since Surfdrive is personal storage, like OneDrive, we do not usually recommend it for research data"
  },
  {
    "objectID": "topics/data-storage.html#research-data-specific-storage-options",
    "href": "topics/data-storage.html#research-data-specific-storage-options",
    "title": "Data Storage",
    "section": "Research data-specific storage options",
    "text": "Research data-specific storage options\nThe options above are standard data storage options at the VU to which all employees have access. But the VU also offers storage specifically for research data. Some of them are hosted locally at the VU, while others are SURF cloud services. When selecting a cloud-based service it is important to remember to check where the data will be hosted. If the research project involves sensitive data it may be necessary to choose cloud-based options that guarantee that the data will stay in the EEA or on servers based in the EEA.\n\n🔒 SciStor (short for ‘Storage for Scientists’): This is storage hosted by IT for Research (ITvO) and allows for inexpensive storage of large volumes of data. There are various levels of security possible and various ways to get access to the files. SciStor is only intended for ongoing research, not for archiving.\nYoda (short for Your Data) is a cloud storage at SURF and is suitable for storing large-scale and sensitive datasets. Yoda also supports collaborating on projects in and outside the VU and adding contextual information (metadata) about your dataset as you go. Yoda is usually the best choice if your research data are very sensitive.\n🔒 Research Drive is a cloud storage at Surf for research projects and is suitable for collaboration in and outside the VU, for storing sensitive data and large-scale research projects. You can also encrypt data in Research Drive using several tools. You are able to request storage space in Research Drive via a 🔒 web form in the selfservice portal (VU employees only). Research Drive is the best choice if you need to manage access rights on a folder level. More general information about Research Drive can be found here, and its wiki pages, including tutorials, are here.\n\nThere are differences between Research Drive and Yoda and each one may support certain projects better than others. The storage finder can help you to get an idea of what would be the best choice for your project, but get in touch with the RDM Support Desk for more details."
  },
  {
    "objectID": "topics/data-storage.html#sending-research-data-to-partners",
    "href": "topics/data-storage.html#sending-research-data-to-partners",
    "title": "Data Storage",
    "section": "Sending research data to partners",
    "text": "Sending research data to partners\nSome projects may require data sharing with partners. Although Research Drive and Yoda support sharing data all through the project, it may also be the case that some data only need to be sent to a partner once. There are some secure options to send data to research partners:\n\n🔒 Surf Filesender: cloud service that allows you to send files of 1 Terabyte to other researchers and encrypted files of up to 250 GB.\n🔒 Zivver: All employees of Vrije Universiteit Amsterdam can use Zivver, the encryption programme that allows you to send email or data (sensitive or otherwise) securely by email. Attachments will also be encrypted and can be several Terabytes in size (max = 5 TB). Specific information on how to get and use Zivver are available on the selfservice portal. General explanations on how to use it are available at the Zivver website."
  },
  {
    "objectID": "topics/persistent-identifier.html",
    "href": "topics/persistent-identifier.html",
    "title": "Persistent identifier",
    "section": "",
    "text": "A persistent identifier (PID) is a durable reference to a digital dataset document, website or other object. It is a kind of ISBN for digital files. By using a persistent identifier, you make sure that your dataset will be findable well into the future. A DOI or Handle are the commonly used PIDs. The data archiving options at the VU commonly offer DOIs.\nMost data archives or repositories offer a persistent identifier and generate this automatically when research data are archived. For example, this is the case for DataverseNL at the VU. In Yoda at the VU, assigning a PID is possible, but does not happen automatically. Please get in touch with the RDM Support Desk if you have questions about assigning a PID when you archive data in Yoda."
  },
  {
    "objectID": "topics/data-citation.html",
    "href": "topics/data-citation.html",
    "title": "Data Citation",
    "section": "",
    "text": "Citing data is not different from citing a journal publication. Similar to citing a journal publication, it helps to give and receive credit, and show the impact of the original source.\nMake sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Data Publication).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\n\n\nStephens, William, 2020, “Resiliences to Radicalisation - QSort Data”, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\n\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11\n\nRelevant is also the Citation File Format (CFF)."
  },
  {
    "objectID": "topics/data-citation.html#citation-elements",
    "href": "topics/data-citation.html#citation-elements",
    "title": "Data Citation",
    "section": "",
    "text": "Citing data is not different from citing a journal publication. Similar to citing a journal publication, it helps to give and receive credit, and show the impact of the original source.\nMake sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Data Publication).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\n\n\nStephens, William, 2020, “Resiliences to Radicalisation - QSort Data”, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\n\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11\n\nRelevant is also the Citation File Format (CFF)."
  },
  {
    "objectID": "topics/data-protection.html",
    "href": "topics/data-protection.html",
    "title": "Data Protection",
    "section": "",
    "text": "Protection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‘security’ is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‘when are the measures that you take secure enough?’. In order to answer this, please be aware that there are three entities that have an opinion about what is ‘secure enough’, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‘GDPR and Privacy’ under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of the VU. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents’ contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management."
  },
  {
    "objectID": "topics/data-protection.html#what-is-data-protection",
    "href": "topics/data-protection.html#what-is-data-protection",
    "title": "Data Protection",
    "section": "",
    "text": "Protection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‘security’ is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‘when are the measures that you take secure enough?’. In order to answer this, please be aware that there are three entities that have an opinion about what is ‘secure enough’, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‘GDPR and Privacy’ under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of the VU. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents’ contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management."
  },
  {
    "objectID": "topics/data-protection.html#data-protection",
    "href": "topics/data-protection.html#data-protection",
    "title": "Data Protection",
    "section": "Data Protection",
    "text": "Data Protection\nThere can be many reasons why the data of a project needs to be kept protected:\n\nSensitivity of the data collected\nProtection of the research data from competition\nCommercial reasons / Intellectual property\nEtc.\n\n\n\n\nAn image of a lock composed of code in a matrix green style.\n\n\nThere are also many levels of security that may be implemented, depending on the needs. Sometimes it will be enough to use a password-protected cloud-based server. In extreme cases encryption may be needed and also when data is transmitted between researchers or organisations. You should contact the RDM Support Desk to discuss available options, who may connect you to legal experts where sensitive data is concerned. Check the Data Storage topic for links to find out more on campus solutions and cloud-based options.\n\n\n\n\n\n\nTip\n\n\n\nSee also the Safe Data Transfer topic for more information on how to transport and transfer data."
  },
  {
    "objectID": "topics/yoda.html",
    "href": "topics/yoda.html",
    "title": "Yoda",
    "section": "",
    "text": "Yoda is an application for institutions that supports RDM throughout the entire research cycle: from the safe and easy storage and sharing of data during the research process, to the sharing of data within research groups and projects and ultimately to research data archiving and publication."
  },
  {
    "objectID": "topics/yoda.html#yoda-and-fair-data",
    "href": "topics/yoda.html#yoda-and-fair-data",
    "title": "Yoda",
    "section": "Yoda and FAIR data",
    "text": "Yoda and FAIR data\nYoda helps the researcher make their data “FAIR” by providing a solution that enables data discovery and sharing (i.e., findable, accessible). In addition, it facilitates and enforces the use of metadata, thereby, contributing to data interoperability and reusability. Yoda provides a platform for the implementation of standard workflows that can ensure metadata quality satisfying (institutional) policy requirements, for example, for data archiving and publication. In addition, Yoda’s integration with iRODS forms a single platform that accommodates both researchers with data heavy requirements, as well as those seeking an accessible, user-friendly solution."
  },
  {
    "objectID": "topics/yoda.html#yoda-for-the-researcher",
    "href": "topics/yoda.html#yoda-for-the-researcher",
    "title": "Yoda",
    "section": "Yoda for the researcher",
    "text": "Yoda for the researcher\nYoda makes use of iRODS as a foundation technology. However, it is much easier to use thereby making it more accessible to a wide range of researchers.\nYoda presents researchers a comfortable, easy-to-use solution for securely storing, sharing and organising their research data that follows the internationally adopted FAIR data principles. Many research institutions and funding organisations (such as NWO and ZonMw) require researchers to make their data FAIR. You can invite both VU and non-VU collaborators to work with you in Yoda.\nThe following features make Yoda particularly attractive for researchers (end-users):\n\nEasy-to-use (web-based, graphical user interface and the WebDAV interface)\nSupports the entire research process from data acquisition to publication\nSupports standardised workflows for the storage and archiving of research data\nDirect coupling between data and metadata (including flexible metadata schemas)\nData publication and the associated minting of Digital Object Identifiers (DOIs)\nA flexible integration with iRODS that also caters for (very) data-intensive research.\nSecure authentication using institutional accounts (both national and international) or eduID\n\nYoda is open source software developed and maintained by Utrecht University for the Yoda Consortium, of which the VU is a member."
  },
  {
    "objectID": "topics/yoda.html#access-to-yoda",
    "href": "topics/yoda.html#access-to-yoda",
    "title": "Yoda",
    "section": "Access to Yoda",
    "text": "Access to Yoda\nYoda is available for every VU researcher. If you need space to store data for your research project, you can request it via the linked form.\nOnce the project space has been created you can start to invite your collaborators."
  },
  {
    "objectID": "topics/yoda.html#getting-started",
    "href": "topics/yoda.html#getting-started",
    "title": "Yoda",
    "section": "Getting Started",
    "text": "Getting Started\nThe VU Yoda website has practical information on the use of the Yoda for users starting with Yoda.\nMore information can be found on the SURFsara Knowledge Base\nThe Yoda site of Utrecht University also contains useful information and is being redeveloped so it can also be used by the VU and the other Consortium Partners."
  },
  {
    "objectID": "topics/data-documentation.html",
    "href": "topics/data-documentation.html",
    "title": "Data documentation",
    "section": "",
    "text": "By creating documentation about your research data you can make it easier for yourself or for others to manage, find, assess and use your data. The process of documenting means to describe your data and the methods by which they were collected, processed and analysed. The documentation or descriptions are also referred to as metadata, i.e. data about data. These metadata can take various forms and can describe data on different levels.\nAn example that is frequently used to illustrate the importance of metadata is the use of the label on a can of soup. The label tells you what kind of soup the can contains, what ingredients are used, who made it, when it expires and how you should prepare the soup for consumption.\nWhen you are documenting data, you should take into account that there are different kinds of metadata and that these metadata are governed by various standards. These include, but are not limited to:\nThe CESSDA has made very detailed guidance available for creating documentation and metadata for your data."
  },
  {
    "objectID": "topics/data-documentation.html#fair-data-principles",
    "href": "topics/data-documentation.html#fair-data-principles",
    "title": "Data documentation",
    "section": "FAIR data principles",
    "text": "FAIR data principles\nThe FAIR data principles provide guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. The principles emphasise machine-actionability, i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention.\nMore information can be found in the section about the FAIR data principles."
  },
  {
    "objectID": "topics/data-documentation.html#unstructured-metadata",
    "href": "topics/data-documentation.html#unstructured-metadata",
    "title": "Data documentation",
    "section": "Unstructured metadata",
    "text": "Unstructured metadata\nMost data documentation is an example of unstructured metadata. Unstructured metadata are mainly intended to provide more detailed information about the data and is primarily readable for humans. The type of research and the nature of the data influence what kind of unstructured metadata is necessary. Unstructured metadata are attached to the data in a file. The format of the file is chosen by the researcher. More explanation about structured metadata can be found on the metadata page.\n\nREADME\nA README file provides information about data and is intended to ensure that data can be correctly interpreted, by yourself or by others. A README file is required whenever you are archiving or publishing data.\nExample of READMEs\n\nGuidelines for creating a README file – 4TU.ResearchData\nGuide to writing “readme”-style metatada - Cornell Data Services\nGuidelines for researchers of the VU Faculty of Behavioural and Movement Sciences on what a README file should contain\n\n\n\nCodebook\nA Codebook is another way to describe the contents, structure and layout of the data. A well documented codebook is intended to be complete and self-explanatory and contains information about each variable in a data file. A codebook must be submitted along with the data.\nThere are several guides for creating a codebook available:\n\nCreating a codebook - Kent State University\nCreating a codebook - for researchers at the VU Faculty for Behavioural and Movement Sciences\nCodebook - Amsterdam Public Health\nDDI-Codebook - Data Documentation Initiative Alliance"
  },
  {
    "objectID": "topics/data-publication.html",
    "href": "topics/data-publication.html",
    "title": "Data Publication",
    "section": "",
    "text": "Open Access publishing means that you make your publication freely accessible online to everyone without restrictions. The Vrije Universiteit believes that government-funded research should be available free of charge to as many people as possible. To that end, the VU Library offers a guide on how to go about Open Access Publishing.\nOpen Access publishing is one component of Open Science. UNESCO defines Open Science as\n\na set of principles and practices that aim to make scientific research from all fields accessible to everyone for the benefits of scientists and society as a whole. Open science is about making sure not only that scientific knowledge is accessible but also that the production of that knowledge itself is inclusive, equitable and sustainable\n\nThis includes making openly available research data, methods, and documentation where possible. As such, RDM and the practices outlined in the VU CS Department RDM and Open Science Handbook are a precondition of Open Science.\nYou can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science and join the Open Science Community Amsterdam, the community of VU employees interested in Open Science (joint with the University of Amsterdam and Amsterdam University of Applied Sciences)."
  },
  {
    "objectID": "topics/data-publication.html#open-access-and-open-science",
    "href": "topics/data-publication.html#open-access-and-open-science",
    "title": "Data Publication",
    "section": "",
    "text": "Open Access publishing means that you make your publication freely accessible online to everyone without restrictions. The Vrije Universiteit believes that government-funded research should be available free of charge to as many people as possible. To that end, the VU Library offers a guide on how to go about Open Access Publishing.\nOpen Access publishing is one component of Open Science. UNESCO defines Open Science as\n\na set of principles and practices that aim to make scientific research from all fields accessible to everyone for the benefits of scientists and society as a whole. Open science is about making sure not only that scientific knowledge is accessible but also that the production of that knowledge itself is inclusive, equitable and sustainable\n\nThis includes making openly available research data, methods, and documentation where possible. As such, RDM and the practices outlined in the VU CS Department RDM and Open Science Handbook are a precondition of Open Science.\nYou can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science and join the Open Science Community Amsterdam, the community of VU employees interested in Open Science (joint with the University of Amsterdam and Amsterdam University of Applied Sciences)."
  },
  {
    "objectID": "topics/data-publication.html#publishing-your-data-in-a-data-journal",
    "href": "topics/data-publication.html#publishing-your-data-in-a-data-journal",
    "title": "Data Publication",
    "section": "Publishing your data in a data journal",
    "text": "Publishing your data in a data journal\nIn addition to archiving research data in a data repository, you may choose to publish an article about your data collection. This is not necessarily common for all disciplines. Some examples of data journals where you can publish your data and dataset, are:\n\nScientific Data - Nature\nGeoscience Data Journal\nGigascience\nJournal of Physical and Chemical Reference Data\nEarth System Science Data\nJournal of Open Archaeology Data\nJournal of Open Psychology Data\n\nArchiving your data in a repository or publishing a data article are both ways to get a Persistent Identifier for your data. It is also important to consider Data Licensing and Software Licensing."
  },
  {
    "objectID": "topics/data-collection.html",
    "href": "topics/data-collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "Data collection may consist of the re-use of existing data and/or the generation of new data.\nFor data to be considered valid and reliable, data collection should occur consistently and systematically throughout the course of the research project. Within disciplines, there are established methodologies, procedures and techniques that help researchers ensure high quality of collected data. In general, important aspects of data collection include:\nSystematic data collection is essential for ensuring the reproducibility of research. When data is collected in a consistent and organized manner, it improves the quality and reliability of the research, making the data easier to share and reproduce by others. High-quality data also contributes to making data FAIR (Findable, Accessible, Interoperable, and Reusable), as well-organized and well-documented data is more likely to be reused effectively. The principles of making data FAIR are discussed in detail under the topic FAIR Principles."
  },
  {
    "objectID": "topics/data-collection.html#data-collection-tools",
    "href": "topics/data-collection.html#data-collection-tools",
    "title": "Data Collection",
    "section": "Data Collection Tools",
    "text": "Data Collection Tools\nThe tools being used in research to collect data are immensely diverse. For that reason, we will not provide an exhaustive overview here. What is important for data collection tools in relation to RDM is where such tools store the data that you collect and in which format. The storage location is particularly important when you are working with personal data. For example, the privacy legislation in the United States is very different from the European General Data Protection Regulation (GDPR). Hence, personal data collected in a Dutch research institute may not be stored on American servers. It is important to keep that in mind when you are contemplating which tool to use for your data collection.\nIf you are collecting personal data and you decide to use a tool for which no contract exists between VU Amsterdam and the provider of the software or tool, a service agreement and a processing agreement must be drawn up. Contact the 🔒 privacy champion of your faculty for more information and a model processing agreement.\n\nQuestionnaire tools\nThe Faculty of Behavioural and Movement Sciences has developed a document with tips for safe use of the questionnaire tools Qualtrics and Survalyzer. The document was made for FGB researchers specifically but can also be helpful for others. Consult this document if you need a questionnaire tool to collect your data."
  },
  {
    "objectID": "topics/data-collection.html#data-collection-in-collaboration",
    "href": "topics/data-collection.html#data-collection-in-collaboration",
    "title": "Data Collection",
    "section": "Data Collection in Collaboration",
    "text": "Data Collection in Collaboration\nSome research projects involve the participation of multiple organisations or institutes and may include even cross-border co-operation. When data is collected by several organisations, a Data Management Plan should provide information on who is responsible for which part of the data collection and storage. It should also provide information on how specific data collections are related to which part(s) of the research goal(s). Describing this precisely will help you to determine if a consortium agreement or joint controller agreement is necessary. You see a general example of such a specification in the table below:\n\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nResponsible organization for collection\nData origin\nData purpose\n\n\n\n\nRaw data\nCommunity level surveys\nVrije Universiteit Amsterdam\nAmsterdam, The Hague, Rotterdam\nIdentifying perceived problems, System responsiveness\n\n\nRaw data\nTrials & Focus Group Interviews\nLondon School of Hygiene and Tropical Medicine (LSHTM)\nGermany, Switzerland\nTrials to evaluate programs on . . ., Focus Group interviews to identify barriers to . . .\n\n\nRaw data\nPollution measurements using fish\nOceanographic Institute of Sweden\nCoastal waters, Northeast Spain\nEstablish pollution levels of plastic"
  },
  {
    "objectID": "topics/data-collection.html#data-collection-protocols",
    "href": "topics/data-collection.html#data-collection-protocols",
    "title": "Data Collection",
    "section": "Data Collection Protocols",
    "text": "Data Collection Protocols\nRegardless of the field of study or preference for defining data (quantitative, qualitative), accurate data collection is essential to maintaining the integrity (structure) of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors.\nThere are two approaches for reducing and/or detecting errors in data which can help to preserve the integrity of your data and ensure scientific validity. These are:\n\nQuality assurance - activities that take place before data collection begins\nQuality control - activities that take place during and after data collection\n\nQuality assurance precedes data collection and its main focus is ‘prevention’ (i.e., forestalling problems with data collection). Prevention is the most cost-effective activity to ensure the integrity of data collection. This proactive measure is best demonstrated by the standardization of protocol developed in a comprehensive and detailed procedures manual for data collection.\nWhile quality control activities (detection/monitoring and action) occur during and after data collection, the details should be carefully documented in the procedures manual. A clearly defined communication structure is a necessary pre-condition for monitoring and tracking down errors. Quality control also identifies the required responses, or ‘actions’ necessary to correct faulty data collection practices and also minimise future occurrences.\nSome sources for protocols:\n\nHANDS Handbook for Adequate Natural Data Stewardship by the Federation of Dutch University Medical Centers (UMCs)\nProtocols.io - an open access repository of protocols\nProtocols Online - website with protocols available on the internet, sorted by discipline.\nSpringer Protocols - free and subscribed protocols collected by Springer."
  },
  {
    "objectID": "topics/bazis.html",
    "href": "topics/bazis.html",
    "title": "High-Performance Computing Facilities",
    "section": "",
    "text": "The BAZIS is a compute cluster for research at VU Amsterdam. It provides a service between the general facilties at the SURF HPC center and the desktop. It is a heterogenious system composed of clusters and servers from research departments.\nIn this topic you will find information to get you started and best practices. Clustercomputing can be very powerfull and useful skill to add to your toolbox and get more science done.\nAlso take a look at the SURF wiki about Snellius, it contains a lot of information which applies to Bazis as well.\nInformation and how to get an account can be found on 🔒 VU Service Portal under IT &gt; Research &gt; HPC Cluster Computing\nBAZIS is maintained by IT for Research.\n\n\n\nVU BAZIS Cluster"
  },
  {
    "objectID": "topics/bazis.html#bazis-hpc",
    "href": "topics/bazis.html#bazis-hpc",
    "title": "High-Performance Computing Facilities",
    "section": "",
    "text": "The BAZIS is a compute cluster for research at VU Amsterdam. It provides a service between the general facilties at the SURF HPC center and the desktop. It is a heterogenious system composed of clusters and servers from research departments.\nIn this topic you will find information to get you started and best practices. Clustercomputing can be very powerfull and useful skill to add to your toolbox and get more science done.\nAlso take a look at the SURF wiki about Snellius, it contains a lot of information which applies to Bazis as well.\nInformation and how to get an account can be found on 🔒 VU Service Portal under IT &gt; Research &gt; HPC Cluster Computing\nBAZIS is maintained by IT for Research.\n\n\n\nVU BAZIS Cluster"
  },
  {
    "objectID": "topics/bazis.html#requesting-access",
    "href": "topics/bazis.html#requesting-access",
    "title": "High-Performance Computing Facilities",
    "section": "Requesting access",
    "text": "Requesting access\nTo access the cluster you can contact your department cluster manager, they will request an account for you with the IT for Research team (ITvO)."
  },
  {
    "objectID": "topics/bazis.html#setup",
    "href": "topics/bazis.html#setup",
    "title": "High-Performance Computing Facilities",
    "section": "Setup",
    "text": "Setup\n\nYour first time\nWhen you get your account you will receive a temporary password, which you can use when you connect to the cluster using ssh. To log in to the cluster you first need to log in to the VU network (ssh.data.vu.nl) and then “hop” to the actual cluster (bazis.labs.vu.nl). For ssh.data.vu.nl you use your VUnetID and VUnetID password. For bazis.labs.vu.nl, you use your VUnetID and cluster password (given by ITvO). This does NOT work in the browser, instead use the tools below.\n\n\nConnecting with SSH\nUse your favourite SSH client to login at \\&lt;VUNETID\\&gt;@bazis.labs.vu.nl. Example clients are MobaXterm (Windows) and iTerm2 (macOS). Direct access is only possible from the Campus or from SURF. From other network locations, first connect to the stepstone \\&lt;VUNETID\\&gt;@ssh.data.vu.nl, or use eduVPN Institutional Access (below). Here we provide the steps to connect to the cluster using MobaXterm.\n\nDownload and install MobaXTerm from mobatek.net.\nOpen MobaXTerm and click on Session\nEnter the Basic SSH settings as follows:\n\nRemote host: bazis.labs.vu.nl\nTick “specify username”\nEnter your username (VUnetID)\n\nGo to “Network settings” and click SSH gateway (jump host)\nEnter the following details and click OK\n\nGateway host: ssh.data.vu.nl\nEnter your username (VUnetID)\n\nSave the settings. When not connected automatically, you can click on User sessions, and double click “bazis.labs.vu.nl (VUNETID)”\nYou will be asked for a password for “ssh.data.vu.nl”. This is your NORMAL VUNETID password, NOT the cluster password your received from ITVO.\nIf not connected automatically, connect to the server again. Now you will see a terminal window which asks for another password. Here you should enter the CLUSTER password that you received from ITVO. Note that you will not see the cursor moving when typing. This is normal. When you finished typing your password hit ENTER.\nYou will now see a terminal window, similar to this one. On connecting for the first time, you will be asked to change your CLUSTER password. Enter your CLUSTER password, then enter a new password (this can be identical to your VUNETID password).\nCongratulations, you are now connected to the cluster! The next steps are to set up your scripts and data!\n\nAs an alternative, you can use eduVPN to connect to the VU network, however the above configuration is recommended and generally works best in the long term. You can find the client on the eduVPN pages. Start eduVPN and choose Institute Access to connect. You will need to enable Multi-Factor Authentication (MFA). Students can activate MFA at the servicedesk (🔒 kb-item 11809).\n\n\nRunning your first Python script\nHere, we go through some steps to run your first script. We provide instructions for Python. If you run R or another language we still recommend you to follow these steps first. They do not require a complex setup. Then once you got that down, to start working with your own. First we will run a simple Python script that prints “hello world!” to the console, and ensure that it runs before we go to more advanced methods.\n\nRight click on the file explorer of MobaXTerm on the left, and click “New empty file”.\nCall the file test.py and click OK.\nDouble click the new file in the file explorer, after which the MobaTextEditor will open. In the file you can write print(\"hello world!\").\nSave the file, click “Yes” to sync the file to the server, and wait for it to synchronize.\nIn the terminal type python test.py.\nYou should see the output hello world! printed to the console.\n\nYour first script was just executed on the login “node”. A node can be thought of as a single computer of the cluster. This login node should only be used for doing very minor things, and any command you execute only runs while you are connected to the server. In the next steps you will learn how to execute a so-called “job” on one of the other nodes using slurm. A job can execute in the background, and if configured correctly, you will receive an email when the job is done.\n\nCreate a new file called test.sh (just like you did before), and enter the following in the file. Make sure to replace &lt;YOUR-EMAIL&gt;@vu.nl with your own email. When you copy the contents of this file on Windows you may need to set the “format” to Unix, which can be done using the top menu of the MobaXtermEditor (format -&gt; UNIX).\n\n#!/bin/bash\n#SBATCH --job-name=my_job\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1\n#SBATCH --time=05:00:00\n#SBATCH --mem-per-cpu=9000\n#SBATCH --output=output.out\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=&lt;YOUR-EMAIL&gt;@vu.nl\n\nsource ~/.bashrc\n\npython test.py\n\nIn additon, edit the Python script as follows. This ensures that the script will run for a while (60 seconds). This is to simulate a long running job.\n\nimport time\nprint(\"starting job...\")\ntime.sleep(60)\nprint(\"hello world!\")\n\nNow, in the terminal type sbatch test.sh, which should give the following output “Submitted batch job &lt;JOBID&gt;”.\nYour job is now submitted and executed on the more powerful computational nodes. You can check whether it is running by typing “squeue”. You should see a short or long list of jobs from all users on the cluster. Yours should be included in the list. In addtional, you should have received an email that the job started.\nWhile your job is running you can check the output of your job in the file “output.out”, as specified in the test.sh-script (see above). This file should contain “starting job…” and after 60 seconds, “hello world!” should also appear, and you will receive another email. Your job is now finished.\nCongratulations, you can now use the cluster! You can adapt this to your own scripts. The rest of this manual shows more useful slurm commands, advanced set up of job scripts, installing of custom Python and R versions, data management etc. etc.\n\n\n\nData transfer\nMobaXterm has an integrated FTP file browser. Once you have logged in to the cluster, you will see the file browser to the left of the terminal window, where it shows the contents of your home folder. You can browse through these folders, and drag-and-drop files and folders between this FTP file browser and the Windows File Explorer. Alternatively, you can use the download/upload buttons at the top of the FTP file browser window. A green refresh button is also located there to refresh the contents of the current folder. You can also open files in the FTP file browser to edit them directly. Upon saving, you’ll be asked if you want change these files on the HPC system.\nMobaXterm works fine for most small data transfers. If you want to transfer a large number of files, or large files, you may want to use a dedicated FTP client. We recommend using WinSCP, Cyberduck or FileZilla, which you can configure as follows.\n\nWinSCP\n\nDownload and install WinSCP from winscp.net.\nClick the “New Session” button.\nFill in the hostname bazis.labs.vu.nl and your username (VUNETID).\nClick the “Advanced” button, and go to the “Connection” -&gt; “SSH” -&gt; “Tunnels” section.\nTick the “Connect through SSH tunnel” box and fill in the host name ssh.data.vu.nl and your username. Ensure that the given port is 22.\nClick “OK” and “Save” to save the session. Do this BEFORE you click “Login”, otherwise all your settings are forgotten next time.\nYou will be asked for your passwords. Similar to connecting with MobaXterm, you will first be asked for your VUNETID password (when connecting to ssh.data.vu.nl), and then for your cluster password (when connecting to bazis.labs.vu.nl)."
  },
  {
    "objectID": "topics/bazis.html#using-slurm",
    "href": "topics/bazis.html#using-slurm",
    "title": "High-Performance Computing Facilities",
    "section": "Using Slurm",
    "text": "Using Slurm\n\nUseful commands\nSee jobs in the queue for a given user\nsqueue -u username\nShow available node features\nsinfo -o \"%20N  %10c  %10m  %25f  %10G \"\nSubmit a job\nsbatch script\nShow the status of a currently running job\nsstat -j jobID\nShow the final status of a finished job\nsacct -j jobID\nCancel a job\nscancel jobid\nCancel all jobs for a user\nscancel -u username\nDisplays some statistics of a running or previously running job, including maximum memory usage. This can be useful especially to learn about the amount of memory required to run the job (again)\nseff &lt;JOBID&gt;\nDisplays some control stats of running job, e.g., number of CPUs used\nscontrol show job &lt;JOBID&gt;"
  },
  {
    "objectID": "topics/bazis.html#input-and-output",
    "href": "topics/bazis.html#input-and-output",
    "title": "High-Performance Computing Facilities",
    "section": "Input and output",
    "text": "Input and output\nAny script you run on the cluster, can read all the files in your home directory, and also create files in the home directory. These files are stored on a network drive, and thus file operations can be slow. For small files this is no issue, but when you work with large files, it is recommended to use the scratch directory for intermediate files that are not needed after the job is done. The scratch directory is a fast disk, which is not backed up. The scratch directory location can be found in the environment variable $TMPDIR. You can access this environment variable in your scripts, for example in Python:\nimport os\nwith open(os.getenv(\"TMPDIR\") + \"/test.txt\", \"w\") as f:\n  f.write(\"hello world!\")\nYou can also all or a selection of files from the scratch directory to your home directory after the job is done. This can be done by including the cp command in your slurm-file. For example, to copy all files from the scratch directory to your home directory, you can use the following command:\ncp $TMPDIR/* ~/"
  },
  {
    "objectID": "topics/bazis.html#data-recovery",
    "href": "topics/bazis.html#data-recovery",
    "title": "High-Performance Computing Facilities",
    "section": "Data recovery",
    "text": "Data recovery\nMost departments have a backup system in place, but configurations may vary. For example, an automatic backup may be made each day with a rentention period of 7 days, and a weekly backuup with a retention period of 5 weeks.\nYou can find all thour data under .zfs/snapshot. You can then copy your data from there to your home folder.\ncd ~/.zfs/snapshot\nls\nNote that the folder is hidden, so you need to use ls -a to see it, or configure your file browser to show hidden files."
  },
  {
    "objectID": "topics/bazis.html#data-deletion",
    "href": "topics/bazis.html#data-deletion",
    "title": "High-Performance Computing Facilities",
    "section": "Data deletion",
    "text": "Data deletion\nYou can only access the cluster with you VUNETID and thus when you are no longer affiliated with the VU, your account will be disabled. However, it is good practice to clean up your data when you are done with it. Depending on the department, data may be deleted after a certain period.\n\nBest practices\n\nMake use of fast scratch directory ($TMPDIR).\nDon’t run large computation on the login nodes! It negatively impacts all cluster users. Grab a compute node with srun --pty bash option.\n\n\n\nConstraints\nThe SLURM constraint option allows for further control over which nodes your job can be scheduled on in a particular parition/queue. You may require a specific processor family or network interconnect. The features that can be used with the sbatch constraint option are defined by the system administrator and thus vary among HPC sites.\nConstraints available on BAZIS are cpu architecture and gpu. Example (single constraint):\n#SBATCH --constraint=zen2\nExample combining constraints:\n#SBATCH --constraint=\"zen2|haswell\"\n\n\nComputer architecture\nThe parts of a modern computer we need to understand to apply to running jobs are listed here. (Note: This is way oversimplified and intended to give a basic overview for the purposes of understanding how to request resources from Slurm, there are a lot of resources out there to dig deeper into computer architecture.)\n\nBoard\nA physical motherboard which contains one or more of each of Socket, Memory bus and PCI bus.\n\n\nSocket\nA physical socket on a motherboard which accepts a physical CPU part.\n\n\nCPU\nA physical part that is plugged into a socket.\n\n\nCore\nA physical CPU core, one of many possible cores, that are part of a CPU.\n\n\nHyperThread\nA virtual CPU thread, associated with a specific Core. This can be enabled or disabled on a system. On BAZIS hyperthreading is typically enabled. Compute intensive workloads will benefit to disable hyperthreading.\n\n\nMemory Bus\nA communication bus between system memory and a Socket/CPU.\n\n\nPCI Bus\nA communication bus between a Socket/CPU and I/O controllers (disks, networking, graphics,…) in the server.\nSlurm complicates this, however, by using the terms core and cpu interchangeably depending on the context and Slurm command. –cpus-per-taks= for example is actually specifying the number of cores per task.\n\n\n\nSlurm example jobs\n\nSimple job\n#!/bin/bash -l\n#SBATCH -J MyTestJob\n#SBATCH -N 1\n#SBATCH -p defq\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir. : ${TMPDIR}\"\n\ncd $TMPDIR\n# Your more useful application can be started below!\nhostname"
  },
  {
    "objectID": "topics/bazis.html#workspace",
    "href": "topics/bazis.html#workspace",
    "title": "High-Performance Computing Facilities",
    "section": "Workspace",
    "text": "Workspace\n\nscratch tmpdir\nEach slurm job will have a fast scratch dir allocated on the nodes which is deleted after finishing the job. use the $TMPDIR virable to use this space for example to store intermediate results or work on many files.\n\n\nscratch-shared (experimental)\nUse the workspace tools on the beegfs parallel filesystem to create a project space. The project space is a directory, with an associated expiration date, created on behalf of a user, to prevent disks from uncontrolled filling. The beegeefs parallel filesystem is faster than your usual NFS home space, but not backed up, so ideal for data which is easily recreated.\nYour project space lives in the filesystem under: /scratch-shared/ws\nThe project space is managed with the hpc-workspace tooling You can add them to your environment with: module load hpc-workspace\nExample: setup a workspace “MyData” in a batchjob for 10 days.\nSCR=$(ws_allocate MyData 10)\ncd $SCR\nCheck your workspaces\n$ ws_list \nid: MyData\nworkspace directory  : /scratch-shared/ws/username-MyData\nremaining time       : 9 days 23 hours\ncreation time        : Wed Mar 13 23:51:57 2013\nexpiration date      : Sat Mar 23 23:51:57 2013\navailable extensions : 15\nRelease the project space with\nws_release MyData\nFor user guide see https://github.com/holgerBerger/hpc-workspace/blob/master/user-guide.md"
  },
  {
    "objectID": "topics/bazis.html#python-virtual-environments",
    "href": "topics/bazis.html#python-virtual-environments",
    "title": "High-Performance Computing Facilities",
    "section": "Python virtual environments",
    "text": "Python virtual environments\nPython has many powerfull packages. In scientific computing many packages may be used in a single project. To manage many python packages often a package manager as conda is used.\nOn a HPC system we do not prefer conda as it does not use optimised binaries and the cache can take up a lot of space, but we understand it is usefull in some cases and try to help.\nWorking with virtual environments further makes the python environment better to manage\n\nA virtual environment is a named, isolated, working copy of Python that that maintains its own files,\ndirectories, and paths so that you can work with specific versions of libraries or Python itself without affecting other Python projects.\nVirtual environmets make it easy to cleanly separate different projects and avoid problems with different dependencies and version requirements across components.\n\nIn short:\n\nuse virtualenv (preferred) or conda\ncreate an isolated environment\nInstall packages\nActivate a virtual environment\nDeactivate a virtual environment\nDelete a virtual environment\n\n\nAdding a requirements file\nPython requirements files are a great way to keep track of the Python modules. It is a simple text file that saves a list of the modules and packages required by your project. By creating a Python requirements.txt file, you save yourself the hassle of having to track down and install all of the required modules manually.\nA reuirements file is a simple text file, which looks like this.\ntensorflow==2.3.1\nuvicorn==0.12.2\nfastapi==0.63.0\nInstalling modules from a requirements file is easy as.\npip install -r requirements.txt\nA requirements file can also be generated with:\npip freeze &gt; requirements.txt\nSee the article referenced below for more information.\n\n\nPortable scripts\nThe first line in a script usually starts the interpreter and is called the Shebang. It is recommended to use /usr/bin/env, which can interpret your $PATH. This makes scripts more portable than hard coded paths..\n#!/usr/local/bin/python\nWill only run your script if python is installed in /usr/local/bin.\n#!/usr/bin/env python\nWill interpret your $PATH, and find python in any directory in your $PATH.\nSo your script is more portable, and will work without modification on systems where python is installed as /usr/bin/python, or /usr/local/bin/python, or even custom directories (that have been added to $PATH), like /opt/local/bin/python.\nFurther Reading\n\npython virtual environments primer\npython requirements.txt file"
  },
  {
    "objectID": "topics/bazis.html#r-environment",
    "href": "topics/bazis.html#r-environment",
    "title": "High-Performance Computing Facilities",
    "section": "R environment",
    "text": "R environment\nR has many powerfull scientific packages and a strong community. Installing and maintaining packages for R can be hard. On BAZIS the Bioconductor suite is installed and can be loaded with the appropiate module environment.\nWhen first running R on a Cluster some changes in the workflow are required making the transition from working interactively from a terminal to scripts in batchmode.\n\nErrors\nPretty much all the time we get errors. Errors can be simple e.g.syntax error, R/python version error or more complex e.g. a problem in our data. In either case, please pay attention to what the error says carefully, because often the solution is in that message or at least it is the starting point of the solution while debugging your code. If it is an error you have not seen before, simply google it. Often you will find a solution in websites like stackoverflow.\n\n\nTips and Caveats\n\nProducing PNG graphics and X11 related plotting errors\nThe png() default device used the X11 driver, which is not avaialble in batch mode or remote operation. Adding the type=“cairo” option to your code solves this issue.\nExample:\npdf(file = \"testR.pdf\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\npng(file = \"testR.png\", type=\"cairo\", width = 4, height = 4)\nplot(x = 1:10, y = 1:10)\nabline(v = 0 )\ntext(x=0, y=1, labels = \"random text\")\ndev.off()\nReferences\n\nHow do I produce PNG graphics in batch mode?"
  },
  {
    "objectID": "topics/bazis.html#matlab",
    "href": "topics/bazis.html#matlab",
    "title": "High-Performance Computing Facilities",
    "section": "Matlab",
    "text": "Matlab\nMatlab has several features to work in batch mode on a HPC cluster. Assuming you know how to create matlab scripts we start simply by executing matlab interactively on a compute node\n\nInteractive\nRequest resources (1 node, 1 cpu) in a partition\nsrun -N 1 -p defq --pty /bin/bash\nmodule load matlab/R2023a\ncd your/data/\nHere is an example of a trivial MATLAB script (hello_world.m):\nfprintf('Hello world.\\n')\nRun with matlab using only one computational thread.\n$ matlab -nodisplay -singleCompThread -r hello_world\nHello world.\n&gt;&gt;\nMatlab waits at the end of the script if there is no exit. In an compute job this would keep the job running untill the wallclocklimit so we add an exit at the end. The convenient “-batch” option combines these options.\n-batch MATLAB_command   - Start MATLAB and execute the MATLAB command(s) with no desktop\n                              and certain interactive capabilities disabled. Terminates\n                              upon successful completion of the command and returns exit\n                              code 0. Upon failure, MATLAB terminates with a non-zero exit.\n                              Cannot be combined with -r.\nmatlab -batch hello_world\n\n\nBatch mode\nCombining this in a slurm script we can queue matlab workloads.\n#!/bin/bash -l\n#SBATCH -J MyMatlab\n#SBATCH -N 1\n#SBATCH --cpus-per-task=1\n#SBATCH -p defq   \n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=&lt;YOUR EMAIL&gt;\n\n# Note: for parallel operations increase cpus-per-task above\n# Note 2: output and error logs can be given absolute paths \n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir. : ${TMPDIR}\"\n\n# cd $TMPDIR\n# or change to a project folder with matlab file e.g. hello_World.m\n# cd your/data\n\n# Load matlab module\nmodule load 2022 matlab/R2023a\n\n# execute\nmatlab -batch hello_world\n\n\nParpool\n\n\nReading\n\nmathworks-parpool"
  },
  {
    "objectID": "topics/bazis.html#acknowledging-the-cluster",
    "href": "topics/bazis.html#acknowledging-the-cluster",
    "title": "High-Performance Computing Facilities",
    "section": "Acknowledging the cluster",
    "text": "Acknowledging the cluster\nWhen you use the cluster for your research, please acknowledge the cluster in your publications. You can use the following text:\n\nWe kindly acknowledge sponsoring by the VU HPC Council and the IT for Research (ITvO) BAZIS Linux computational cluster at VU Amsterdam."
  },
  {
    "objectID": "topics/bazis.html#advanced-topics",
    "href": "topics/bazis.html#advanced-topics",
    "title": "High-Performance Computing Facilities",
    "section": "Advanced topics",
    "text": "Advanced topics\n\nUsing SSH keys\nSSH keys are an alternative method for authentication to obtain access to remote computing systems. They can also be used for authentication when transferring files or for accessing version control systems like github.\nThe cluster uses ssh keys to manage batch jobs.\nOn your workstation create ssh key pair ssh-keygen -t ed25519 -a 100\n\n-a (default is 16): number of rounds of passphrase derivation; increase to slow down brute force attacks.\n-t (default is rsa): specify the cryptographic algorithm. ed25519 is faster and shorter than RSA for comparable strength.\n-f (default is /home/user/.ssh/id_algorithm): filename to store your keys. If you already have SSH keys, make sure you specify a different name: ssh-keygen will overwrite the default key if you don’t specify!\n\nIf ed25519 is not available, use the older (but strong and trusted) RSA cryptography: ssh-keygen -a 100 -t rsa -b 4096\nWhen prompted, enter a strong password that you will remember.\nNote: on windows you can use MobaKeyGen from MobaXterm, but on Windows 11 Powershell or Command Prompt works as well.\nIn your ~/.ssh directory you will find a public and private key. Make sure to keep the private key safe as anyone with the private key has access.\nNow, when you add your public key to the ~/.ssh/authorized_keys file in a remote system, your key will be used to login.\nYou can either use copy-paste or the ssh-copy-id command:\n$ ssh-copy-id user@remote-host\nThe authenticity of host 'remote-host (192.168.111.135)' can't be established.\nECDSA key fingerprint is SHA256:hXGpY0ALjXvDUDF1cDs2N8WRO9SuJZ/lfq+9q99BPV0.\nAre you sure you want to continue connecting (yes/no)? yes\n/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed\n/usr/bin/ssh-copy-id: INFO: 2 key(s) remain to be installed -- if you are prompted now it is to install the new keys\nuser@remote-host's password:\n\nNumber of key(s) added: 2\nNow try logging into the machine, with: “ssh ‘user@remote-host’” and check to make sure that only the key(s) you wanted were added."
  },
  {
    "objectID": "topics/bazis.html#forwarding-x",
    "href": "topics/bazis.html#forwarding-x",
    "title": "High-Performance Computing Facilities",
    "section": "Forwarding X",
    "text": "Forwarding X"
  },
  {
    "objectID": "topics/bazis.html#fix-warning-remote-host-identification-has-changed",
    "href": "topics/bazis.html#fix-warning-remote-host-identification-has-changed",
    "title": "High-Performance Computing Facilities",
    "section": "Fix Warning: Remote Host Identification Has Changed",
    "text": "Fix Warning: Remote Host Identification Has Changed\nIf you are sure that it is harmless and the remote host key has been changed in a legitimate way, you can skip the host key checking by sending the key to a null known_hosts file:\nssh -o \"UserKnownHostsFile=/dev/null\" -o \"StrictHostKeyChecking=no\" username@bazis.labs.vu.nl\nTo make the change permanent remove the offending host key from your ~/.ssh/known_hosts file with:\nssh-keygen -R \"hostname\"\n\nReferences:\n\nhpc carpentry\nComparing SSH keys\npublic key cryptography for non geeks\ndisable ssh host key checking\n\n\nConnect to VU Yoda/iRODS with icommands\nYoda is a data storage solution for research data at the Vrije Universiteit Amsterdam. You can use the icommands to upload and download data from Yoda directly from the cluster.\nTo do so, you need to configure iRODS, such that it links to your account on Yoda by taking the following steps:\n\nPoint the cluster to the Yoda server by creating a file called .irods/irods_environment.json in this folder. The file should look like this, replacing &lt;EMAIL-ADDRESS&gt; with your VU email:\n{\n   \"irods_host\": \"portal.yoda.vu.nl\",\n   \"irods_port\": 1247,\n   \"irods_home\": \"/vu/home\",\n   \"irods_user_name\": \"\\&lt;EMAIL-ADDRESS\\&gt;\",\n   \"irods_zone_name\": \"vu\",\n   \"irods_authentication_scheme\": \"pam\",\n   \"irods_encryption_algorithm\": \"AES-256-CBC\",\n   \"irods_encryption_key_size\": 32,\n   \"irods_encryption_num_hash_rounds\": 16,\n   \"irods_encryption_salt_size\": 8,\n   \"irods_client_server_negotiation\": \"request_server_negotiation\"\n}\nThen, you need to set a password on the Yoda portal, and use this password to authenticate the iRODS connection. Go to https://portal.yoda.vu.nl/user/data_access, and generate a password.\nYou can connect to Yoda using the iinit command. You will be prompted for the password you just generated.\n\nFinally, you can use the icommands to navigate and transfer data between the cluster and Yoda. These commands are generally similar to Linux commands .\nFor example, to list the contents of your home directory on Yoda, you can use the ils command:\nils\nThis will show you the contents of your home directory on Yoda. The project folders you have access to is located at /vu/home.\nTo set the current working directory to a specific folder on Yoda, you can use the icd command (make sure to replace &lt;PROJECT_FOLDER&gt; with the folder you want to navigate to):\nicd /vu/home/&lt;PROJECT_FOLDER&gt;\nTo upload a file to Yoda, you can use the iput command with . being the current working directory (you can also set the destination folder by replacing . with the desired folder):\niput file.txt .\nThen, to delete the file from the cluster, you can use the irm command:\nirm file.txt\nTo synchronize a local directory with a directory on Yoda, you can use the irsync command. In this command i: refers to the iRODS server. Optionally specify -r to recursively copy the contents of the subdirectories. The following example recursively copies the contents of the local directory test to the directory i:test on Yoda:\nirsync -r test i:test\nYou can find an overview of all icommands here."
  },
  {
    "objectID": "topics/data-management-plan.html",
    "href": "topics/data-management-plan.html",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "A Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research lifecycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use."
  },
  {
    "objectID": "topics/data-management-plan.html#what-is-a-dmp",
    "href": "topics/data-management-plan.html#what-is-a-dmp",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "A Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research lifecycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use."
  },
  {
    "objectID": "topics/data-management-plan.html#dmponline",
    "href": "topics/data-management-plan.html#dmponline",
    "title": "Data Management Plan (DMP)",
    "section": "DMPonline",
    "text": "DMPonline\nVU Amsterdam offers the online tool DMPonline for writing Data Management Plans. DMPonline is a platform that offers a range of templates, ensuring that researchers can create DMPs to meet the standards of diverse funders and institutions associated with their projects. DMPonline makes it easy to work on a DMP together with colleagues, advisors, or other stakeholders. VU Amsterdam researchers can use the request feedback function of DMPonline to get their DMP reviewed by a faculty data steward or RDM Support Desk colleague.\nIf you have questions about DMPonline, or encounter problems when using the tool, please get in touch with rdm@vu.nl."
  },
  {
    "objectID": "topics/data-management-plan.html#choosing-the-right-template",
    "href": "topics/data-management-plan.html#choosing-the-right-template",
    "title": "Data Management Plan (DMP)",
    "section": "Choosing the right template",
    "text": "Choosing the right template\nVarious templates exist in which you can set up your DMP. We strongly recommend that you use the VU template, which is called VU DMP template 2021 (NWO & ZonMw certified) v1.4. Below you’ll find an explanation of how to access this template. If you need to write a DMP for funding agencies NWO, ZonMw or ERC, you can use the VU template as well.\n\nVU template\nYou can find the VU DMP template in DMPonline. It includes concise guidance on how to complete your DMP.\nYou can select the VU template by taking the following steps (see also the picture below).\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\nNote: Follow these steps as well if you receive funding from NWO or ZonMw (see also below).\n\n\n\nScreenshot of a form for creating a data management plan, asking for the research project, where it is being done, who is funding it and what template you would like to use.\n\n\nIf you’re aiming to write a full DMP based on the VU DMP template, please make sure you don’t select the GDPR registration form.\n\n\n\nA screenshot highlighting to not use the VU GDPR Registration form\n\n\n\n\nFunder template\nWe recommend researchers to use the VU DMP template whenever possible, especially for researchers who work with personal data. The VU DMP template includes questions that serve as input for the GDPR record of processing activities. This means that when you write a DMP based on the VU DMP template, you simultaneousely comply with the VU requirement to register the personal data you use in your research.\nHowever, it is also possible to use other templates in DMPonline. If your funder or partner organization requires you to use a certain template, it is possible to select that template in DMPonline. Please follow the steps below to select a funder’s template.\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nIn the field under Select the primary funding organisation, start typing the name of your funder and select their template.\n\n\n\n\nScreenshot of a filled out form for creating a data management plan, with example data included.\n\n\nResearchers who don’t work with personal data and who wish to use another DMP template than the VU template, can also follow the steps above."
  },
  {
    "objectID": "topics/data-management-plan.html#register-your-processing-activities",
    "href": "topics/data-management-plan.html#register-your-processing-activities",
    "title": "Data Management Plan (DMP)",
    "section": "Register your processing activities",
    "text": "Register your processing activities\n\nHow does registration of personal data processing work in research?\nIf your research is subject to the GDPR, then you need to register information on your research in a central VU registry. This central registry lists all personal data processing activities carried out at the VU. The registry indicates why and how personal data are processed, and with whom they are shared. The registry helps the VU demonstrate compliance with the GDPR and in the case of a data breach, the registry helps with monitoring and acting swiftly to inform all relevant stakeholders.\nFor research projects, the VU registers data processing via DMPonline. You can create your registration by logging into DMPonline and following the following instructions:\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don’t have to select the check box for mock testing).\nSelect Vrije Universiteit Amsterdam as your primary research organisation.\nFor the question on primary funding organisation, select the check box on the right, saying that no funder is associated with your plan.\n\n\n\n\nScreenshot of a form for creating a data management plan, asking for the research project, where it is being done, who is funding it and what template you would like to use.\n\n\nOnce you get to the two VU templates, you can fill in the VU DMP template 2021 v1.4 if you need to write a DMP anyway; the information you include in this DMP template will be used for the registry. If you don’t need to write a (new) DMP, you can use the separate VU GDPR registration form for research v1.1. Your faculty’s 🔒 Privacy Champion can help you with your registration.\nIf your research is primarily led by Amsterdam UMC, location VUmc, your research will be registered using their own separate system.\n\n\nRegister before you start your data collection\nIf you use personal data in your research, you should register your data processing activities before you start data collection. If you are not sure whether your research data are subject to the GDPR, contact your faculty’s 🔒 Privacy Champion. Your privacy champion can also assist you if your research is already running, but has not yet been registered."
  },
  {
    "objectID": "topics/data-management-plan.html#what-is-data",
    "href": "topics/data-management-plan.html#what-is-data",
    "title": "Data Management Plan (DMP)",
    "section": "What is data",
    "text": "What is data\nResearch data is any information that has been collected, observed, generated or created to validate original research findings. Examples of data could be interview recordings, experiment results, physical measurement, notes from focus group’s meetings, notes from fieldwork, observations captured in photographs, film or audio, text files extracted from a corpus, image of archival items or artworks, scraped websites, responses to survey questions. Algorithms, simulations, code, scripts and software are often also considered as research data. There is also physical data: (biological) samples, collections, artifacts etc.\nAdministrative documents, like informed consent forms and key files should be acknowledged as important elements of research data as well."
  },
  {
    "objectID": "topics/data-management-plan.html#data-assets",
    "href": "topics/data-management-plan.html#data-assets",
    "title": "Data Management Plan (DMP)",
    "section": "Data Assets",
    "text": "Data Assets\nAt the VU, we sometimes use the term ‘Data Assets’. You can think of data assets as small ‘parcels’ of data that can change form or format throughout the research. For example, if you’re sending out surveys for your research, the survey responses are considered a data asset. If, in addition to the surveys, you’re also holding focus groups, the data collected from the focus group are also considered a data asset, separate from the survey results. Most projects will have more than one data asset per data stage. It is common to provide data assets based on the data stage such as raw, processed, or analysed. Raw Data refers to original data collected, Processed Data is data that has undergone some level of transformation or organisation. Processing involves cleaning, formatting, and structuring raw data to make them more understandable and suitable for analysis. Analysed Data usually results from statistical methods, detailed examination or interpretation.\nHere are some examples of data assets in research data management:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nType of data\nFormat\n\n\n\n\nRaw data\nInterviews\nAudio files\nMP3\n\n\n\nSpectographic analysis\nText files\nCSV\n\n\nProcessed data\nTranscription of interviews\nText files\nDocx\n\n\n\nData spreadsheet\nSPSS files\nSAV\n\n\nAnalysed data\nRegression graphic\nGraph\nPNG\n\n\n\nData table\nWord file\nDocx\n\n\nOther\nPoster presentation\nPowerpoint\nPPS\n\n\n\nProject Website\nHTML\n\n\n\n\nAnalysis code\nText files\nPython\n\n\n\nNote that these data assets also change in the different phases of the research! While the interview data are audio files in the raw stage, they are transcribed and become text files in the processed stage."
  },
  {
    "objectID": "topics/data-management-plan.html#dmp-elements",
    "href": "topics/data-management-plan.html#dmp-elements",
    "title": "Data Management Plan (DMP)",
    "section": "DMP Elements",
    "text": "DMP Elements\nThe VU DMP template consists of seven sections with questions. In DMPonline, there is guidance available for all sections, as well as example answers. When you are writing your DMP, you can consult this information directly in DMPonline. Below we provide references to information and support available for various RDM-related aspects.\n\nLegal and ethical requirements, codes of conduct\nIf you have questions about working with personal data in research, please get in touch with the Privacy Champion of your faculty. The 🔒 overview of Privacy Champions can be found on the VU website. Make sure to contact your Privacy Champion in the following situations:\n\nIf you need to carry out a DPIA, or if you’re unsure if you need to do one\nIf you work with special category personal data, or otherwise very sensitive data\nIf you are collaborating with other parties\nIf you need software for which no licence is set up on behalf of the VU\nIf you wish to reuse existing data containing personal data\n\nIt is impossible to provide an overview of tasks to be carried out to ensure compliance with the GDPR that fits all research projects. For that reason, it is important to contact your Privacy Champion. They will be able to identify what needs to be arranged to adhere to the GDPR.\nEthical aspects of research should be addressed in the ethics procedure of your faculty. Each faculty has their own ethics committee. The webpages of all committees are listed below. Please go to the page of the ethics committee of your faculty to find instructions for ethical review procedures for your study.\n\nACTA: ACTA Institutional Review Board (IRB)\nBeta: Research ethics review committee Faculty of Science (BETHCIE)\nFGB: 🔒 Scientific and Ethical Review Board (VCWE)\nFGW: Ethische Toetsingscommissie Onderzoek (EtCO)\nFSW: 🔒 Research Ethics Review Committee (RERC)\nRCH (Faculty of Law): Ethics Committee\nVUmc: METc (Medical Ethical Review Committee)\n\n\n\nStorage and backup during the research process\nAn overview of storage facilities at the VU is available in the Data Storage Finder. You can use this as a starting point to navigate storage solutions.\nIf you have questions about data storage and backup, send an email to rdm@vu.nl.\n\n\nData archiving and publishing\nIf your research data contains personal data and you’re unsure about which data may be published, please contact your 🔒 Privacy Champion."
  },
  {
    "objectID": "group-guidelines.html",
    "href": "group-guidelines.html",
    "title": "Group-specific Guidelines",
    "section": "",
    "text": "What is this page about?\n\n\n\nThe CS department is one of the largest departments of the VU featuring research of all kinds. Here you can find group-specific guidelines that the dept. data stewards wrote in collabration with the RDM contact person of each group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUser-centric Data Science (UCDS)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "RDM-guidelines/RDM-resources.html#support",
    "href": "RDM-guidelines/RDM-resources.html#support",
    "title": "What research resources are available for VU researchers related to research data management?",
    "section": "Support",
    "text": "Support\n\nResearch Data Services\n\n\n\nImage showing the relations between VU Research Data Support Offices, at the center with grants, legal, library, security, IT for Research, and IXA around it.\n\n\nResearch Data Management is supported by various departments at the VU. These departments will help all VU researchers. There are also faculty specific support departments for research data support; they support their own faculty members.\nHere you find references to other organisational units and departments that can help you with matters related to collecting and managing data.\nVU research data support (for all researchers)*\n\nGrant office\nLibrary\nLegal\n🔒 IT for Research\nIXA"
  },
  {
    "objectID": "guides/discover-and-initiate.html",
    "href": "guides/discover-and-initiate.html",
    "title": "How can you discover and reuse existing research data?",
    "section": "",
    "text": "Re-using Existing Data\nAnything that can be used for analysis can be considered “data(sets)”. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g. raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your 🔒 Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only).\n\n\nSources for Finding Existing Datasets\nThe number of datasets that are available grows rapidly. Datasets are made available in many formats, by many people or organizations. Some datasets are raw files and some are specifically organised and formatted as databases that require a licence or subscription to use them. The library of the Vrije Universiteit Amsterdam has collected links to some of the data repositories used and has licensed several databases.\n\nPopular Free and Licensed Databases: These can be found with LibSearch Advanced.\n\nIf you need help finding & using free or licensed sources you can contact the Research Data Services Helpdesk. For students and personnel in the fields of economics, finance, or organisation science a separate LibGuide has been created to help them find and use/re-use data.\nYou can also start looking for data in these four places:\n\nThe literature. Research articles may point you to the data that they are based on. Sometimes, (part of) the data are added to the article as supplementary files, and sometimes the data are published separately in a data repository. In the latter case, the article usually provides a clear reference to the published dataset. Some datasets may even be specifically published in Data Journals.\nScientific data repositories. Data repositories are platforms used to access and archive research data. Universities often provide a repository for data archiving, but other platforms arranged by discipline or by country also exist. Some repositories are only accessible to consortium members, whereas others are free of charge. Many universities in the Netherlands use DataverseNL to archive datasets for the mid-term. Long-term archiving is provided by the national research data archives DANS and 4TU.Research Data. In Europe, B2SHARE and Zenodo are platforms used to access research data. Data repositories can be accessed by searching by topic or country using Re3data, a data repository registry. The VU has its own research portal, PURE, where researchers register their datasets. You can find instructions on how to register your own dataset in PURE on the Dataset Registration page of this LibGuide.\nData search engines. Search engines allow you to quickly browse data sets and supplementary data files published by researchers. They cover data sets from many sources. This makes them useful for quick orientation on a topic. Example of a search engines are: DataCite, Google DataSet Search.\nData portals of (governmental) organisations. Organisations that regularly collect (statistical) data sometimes offer these data through their own portal. An example is Eurostat, which collects and disseminates statistics at the European level, by country and by theme. Some of these websites have been linked in the Finding data LibGuide.\n\n\n\nData Sources for VU Researchers\nResearchers from the Vrije Universiteit Amsterdam have also developed some databases containing data collected during research. See here for some examples:\n\nNederlands Tweelingenregister (Netherlands Twin Register) The database contains data on twins and their families and was created to do research on the relationship between genetics and growth, development, personality, behaviour, diseases, mental health and all kinds of risks.\nGeoplaza VU - the portal for all matters related to GIS (Geographical Information Systems) and geodata at the VU University Amsterdam. It offers students and employers a platform to exchange, examine and download digital map material.\nDutch monasteries - database with information about Dutch monasteries of the Middle Ages.\nSlave owners in Amsterdam 1863 - the place of living of owners of slaves in Amsterdam in 1863, visualized in GeoPlaza.\nDeaths at the Borders Database - collection of official, state-produced evidence on people who died while attempting to reach southern EU countries from the Balkans, the Middle East, and North & West Africa, and whose bodies were found in or brought to Europe.\nDatasets published by VU Researchers can be found at the VU Research Portal.\n\n\n\nCitation Elements\nCiting data is not different from citing a journal publication. Similar to citing a journal publication, it helps to give and receive credit, and show the impact of the original source.\nMake sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Data Publication).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\n\nExample data citation\nStephens, William, 2020, “Resiliences to Radicalisation - QSort Data”, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\n\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11\n\nRelevant is also the Citation File Format (CFF)."
  },
  {
    "objectID": "guides/data-lifecycle.html#support",
    "href": "guides/data-lifecycle.html#support",
    "title": "What research data services and support are available for VU researchers?",
    "section": "Support",
    "text": "Support\n\nResearch Data Services\n\n\n\nImage showing the relations between VU Research Data Support Offices, at the center with grants, legal, library, security, IT for Research, and IXA around it.\n\n\nResearch Data Management is supported by various departments at the VU. These departments will help all VU researchers. There are also faculty specific support departments for research data support; they support their own faculty members.\nHere you find references to other organisational units and departments that can help you with matters related to collecting and managing data.\nVU research data support (for all researchers)*\n\nGrant office\nLibrary\nLegal\n🔒 IT for Research\nIXA\n\nGeneral Faculty research support and management guidelines are available in the section Policies & Regulations.\n\n\nThis LibGuide\nQuestions regarding your data or the information on this website?\nThe RDS Support team can help you with all your question on data management plan, data archiving, data store or data privacy & security.\n\n\nVU IT for Research\nThe VU has an IT team specifically devoted to research: ITvO (from ‘IT voor Onderzoek’ in Dutch). They provide the following services:\n\n🔒 Bazis HPC cluster computing: access to your own linux computational cluster at the VU\n🔒 SciCloud: a service with which you lease virtual server capacity for research purposes\n🔒 SciStor: inexpensively store large sets with research data\nAdvice/consultancy: ITvO will help you to find a suitable technical solution or support for your research group or project\nHousing: rack space in the server room for (remotely managed) equipment of research groups\n\nRead more about and get in touch with 🔒 IT for Research\n\n\nExternal sources\n\n Research Data Netherlands\n\nResearch Data Netherlands is an alliance between 4TU.Centre for Research Data, Data Archiving and Networked Services (DANS) and SURFsara. With this coalition, which is also open to other parties, the three data archives join forces in the area of long-term data archiving.\n\n Landelijk Coördinatiepunt Research Data Management\n\nThe foundation of co-operating Dutch universities (VSNU) pointed out a need for a co-ordinated and decisive approach to Research Data Management through a dedicated Centre at SURF. The SURF Foundation is an organisation that facilitates education and research in the Netherlands.\n\nThe LCRDM aims to support the preparation, development and monitoring of Research Data Management policies for scientific research in the Netherlands. Important elements of this central approach are close co-operation with researchers in the field and the exchange of knowledge and experience.\n\n The Netherlands eScience Center\n\nThe eScience Center develops software for academic research specifically. Their software and tools enhance the use of digital methods in scientific research across all disciplines. Researchers - including those at the VU - can choose to collaborate on projects funded by the NWO or, if they already have funding, ask the eScience Center to collaborate with them.\n\n CESSDA\n\nThe Data Management Expert Guide of the Consortium of European Social Science Data Archives is a practical guide for researchers, addressing many issues they may have"
  },
  {
    "objectID": "guides/data-lifecycle.html#surfsara",
    "href": "guides/data-lifecycle.html#surfsara",
    "title": "What research data services and support are available for VU researchers?",
    "section": "SURFsara",
    "text": "SURFsara\n\n\n\nSURFsara logo\n\n\nSURF is the collaborative organisation for ICT in Dutch education and research. SURF offers advanced ICT services specifically for researchers. You can start using some of these services right away with your VU credentials. For others you have to get in touch with SURF yourself. Please check SURF’s website and the pages about the specific services below for more information.\nSURF’s services are listed on this page.\n\nData services\nSURF offers a wide range of services for different phases in the life cycle of your research data. Everything for the secure storage, management, sharing and reuse of data.\n\nResearch Drive: securely and easily store and share research data.\n\nDoes your research team need large storage quotas, a secure environment to store personal and/or sensitive data, and work collaboratively with other educational and governmental institutions or external private parties? Research Drive is a cloud-based shared-storage environment specifically designed for these requirements.\n\nSURFfilesender: send large files securely and encrypted.\n\nWant to send and receive files quickly, securely and easily? With SURFfilesender, you can send large files, such as research data. The files are stored in the Netherlands. Encryption provides added security.\n\nStore and share your files securely in the cloud with SURFdrive.\n\nStore, synchronise and share your documents easily with SURFdrive. SURFdrive is a personal cloudservice for the Dutch education and research. Your documents are kept safe and sound in our community cloud.\n\nSecure, long-term storage with Data Archive.\n\nThe Data Archive is the centralised location for data archiving and (long-term) storage. You can securely store research data there, even in volumes running into the petabytes. The archive provides quick access to SURFsara’s computing facilities.\n\nData Persistent Identifier: data always findable by permanent references.\n\nPersistent identifiers (PIDs) ensure the findability of your data, now and always. PIDs are comparable to the ISBN numbers assigned to books. Even if the location or underlying infrastructure changes, the reference path remains intact. SURFsara offers the PID service in cooperation with the European Persistent Identifier Consortium (EPIC).\n\n\n\n\nData processing and analysis (see also ‘Computing’)\n\nJupyter Notebook: accessible and interactive data analysis for research and education.\n\nA Jupyter Notebook is an interactive web application that you can use to create documents, known as notebooks, that contain computer code, formatted text, comparisons and visualisations. The code can be executed in the environment and you can even create streaming applications and dashboards.\n\n\n\n\nComputing\nDo you encounter limitations with your own systems? SURF offers researchers a wide range of services in the field of high performance computing (HPC): thousands of times faster than your PC.\n\nCartesius: National Super Computer.\n\nIt is the most comprehensive system in the field of capability computing in the Netherlands. Cartesius is especially in high demand for its combination of fast processors and internal network, large storage capacity and the ability to process large datasets.\n\nLisa Compute Cluster: extra processing power for research.\n\nLisa Compute Cluster combines processing power with user friendliness. Are the limits of your own system inhibiting your research? This service lets you upscale to a higher level. Lisa Compute Cluster is preconfigured with a range of software packages, meaning you can start working right away.\n\nHPC Cloud: your flexible compute infrastructure.\n\nHPC Cloud gives you and your project team complete control over your computing infrastructure. The infrastructure ranges from a single work station to a complete cluster and can be expanded to suit your needs. You can use your own operating system and analysis software. HPC Cloud is housed in SURF’s own data centre.\n\nGrid: for processing and storing large datasets.\n\nDo you want to process and store large- amounts of data? The Grid may very well be suitable for your project. The grid infrastructure consists of a large number of clusters for computing and data storage, which are interconnected via a fast network.\n\nVisualisation: more insight into your data.\n\nDo you want to analyse, process or visualise complex research data or big data? These services give you insight into your research data. SURF’s Visualisation service allows you to visualise your own datasets on your desktop. This makes it easy to identify connections between data or gain other insight into your datasets. SURFsara offers a powerful remote visualisation service that combines high performance with ease of use.\n\nCollaboratorium: a visualization and presentation space for science and industry.\n\n\n\nExpertise, advice and training\nSURF offers advice and training on their services. Their training sessions are announced in the SURF Agenda.\n\nSURF Training courses for research.\n\nWant to get started with SURF systems but lack the necessary knowledge? SURF regularly organizes hands-on systems training courses at their offices in Utrecht and Amsterdam or at your education or research institution. You can also include the training courses in the educational programme of your institution.\n\nSURF Consultancy on ICT solutions for researchers.\n\nSURFsara possesses a wealth of experience in the field of ICT services for researchers. If you need help developing/improving your application or designing your infrastructure, then you’ve come to the right place. SURF experts will be happy to lend their expertise to support your research."
  },
  {
    "objectID": "guides/plan-and-design.html",
    "href": "guides/plan-and-design.html",
    "title": "How can you set up research data management from the start?",
    "section": "",
    "text": "Grant programmes from organisations like NWO, ZonMw and ERC require you to think about the method of data collection, the journey of the data in your research project and how to protect or share data during and after the research project. It is important to bear in mind the specific laws and regulations that apply to the kind of data that is collected. If a project involves data on individuals and organisations this impacts the design of the necessary IT infrastructure. A more detailed description of this will later be captured in the data management plan.\nWhen writing your research proposal the following items are important:\n\nFill in the Data Management Section if your funder requires this\nPlanning: One of the early deliverables will be a detailed Data Management Plan\nBudget: Take into account the costs (labour and material) for data storage during and data archiving after your project.\nWriting: Funders that distribute grants like to maximise the effectiveness of this investment. It is therefore highly recommended that the data will be made Findable, Accessible, Interoperable and Re-usable (FAIR Principles). This does not mean that the data have to be open: Laws, licenses and contracts regarding personal and sensitive data may limit the possibility to share the data publicly.\n\nThe [RDM Support Desk[(https://vu.nl/en/about-vu/more-about/rdm-support-desk) provides advice and help when writing a Data Management Section as part of the research proposal. Also make sure to reach out to the VU Grants Office (IXA-GO) for advice and practical aid for your grant in general as early as possible.\n\n\nMany funders require researchers to include a section in their project proposal about Research Data Management, in which they explain whether existing data will be reused, whether new data will be collected or generated during the project, and how they plan to structure, archive and share their data. Depending on requirements of the funder, the paragraph can be short or more extensive.\nFunders may have different requirements for the data management section in the project proposal. Always check what your funder asks for. Below is a list of information on data management sections from main Dutch funding bodies.\n\nNWO\nZonMw\n\nWe recommend you to ask advice from the RDM Support Desk when writing your data management section."
  },
  {
    "objectID": "guides/plan-and-design.html#research-proposal",
    "href": "guides/plan-and-design.html#research-proposal",
    "title": "How can you set up research data management from the start?",
    "section": "",
    "text": "Grant programmes from organisations like NWO, ZonMw and ERC require you to think about the method of data collection, the journey of the data in your research project and how to protect or share data during and after the research project. It is important to bear in mind the specific laws and regulations that apply to the kind of data that is collected. If a project involves data on individuals and organisations this impacts the design of the necessary IT infrastructure. A more detailed description of this will later be captured in the data management plan.\nWhen writing your research proposal the following items are important:\n\nFill in the Data Management Section if your funder requires this\nPlanning: One of the early deliverables will be a detailed Data Management Plan\nBudget: Take into account the costs (labour and material) for data storage during and data archiving after your project.\nWriting: Funders that distribute grants like to maximise the effectiveness of this investment. It is therefore highly recommended that the data will be made Findable, Accessible, Interoperable and Re-usable (FAIR Principles). This does not mean that the data have to be open: Laws, licenses and contracts regarding personal and sensitive data may limit the possibility to share the data publicly.\n\nThe [RDM Support Desk[(https://vu.nl/en/about-vu/more-about/rdm-support-desk) provides advice and help when writing a Data Management Section as part of the research proposal. Also make sure to reach out to the VU Grants Office (IXA-GO) for advice and practical aid for your grant in general as early as possible.\n\n\nMany funders require researchers to include a section in their project proposal about Research Data Management, in which they explain whether existing data will be reused, whether new data will be collected or generated during the project, and how they plan to structure, archive and share their data. Depending on requirements of the funder, the paragraph can be short or more extensive.\nFunders may have different requirements for the data management section in the project proposal. Always check what your funder asks for. Below is a list of information on data management sections from main Dutch funding bodies.\n\nNWO\nZonMw\n\nWe recommend you to ask advice from the RDM Support Desk when writing your data management section."
  },
  {
    "objectID": "guides/plan-and-design.html#rdm-costs",
    "href": "guides/plan-and-design.html#rdm-costs",
    "title": "How can you set up research data management from the start?",
    "section": "RDM Costs",
    "text": "RDM Costs\n\nCosts & Data Management\nMany research funders encourage applicants to include data management and sharing costs in research proposals. Some funders will provide advice on costs related to data management. Some remarks on costs are provided here:\n\nThe Data Management Plan should describe the activities that incur costs and provide justification for the allocation of resources (example: acquisition of a programmer who will write software needed to capture the data).\nNo expenditure can be ‘double funded’, i.e. a service that is centrally supported by indirect costs must not be included as a direct cost as well (example: computers that are already provided to employees and paid for by the university may not be included).\nThe budget and justification should broadly indicate where RDM costs will be incurred, where possible. E.g. data capture and cleaning, data curation and preservation, data sharing.\nInclude budget for long-term storage if data are expected to be deposited in a repository not funded by the university or external funders (VU repositories are: DataverseNL, Yoda). 🔒 VU has an internal breakdown of costs for storage and archiving for VU-managed storage and repositories.\n\nA practical costing tool is available from the UK Data Archive. Based on this costing tool, Utrecht University has developed a guide to calculate the costs of data management. You can use those guides as well to estimate the costs needed specifically for RDM.\nMost material costs of the storage solutions offered by the VU are covered centrally (up to 500 GB), but if you need to specify the costs for your project, look at the 🔒 Research & Archiving Storage Cost Model\nExamples to put in a data management plan:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset\nType of data\nCosts\n\n\n\n\nRaw data\nInterviews\nAudio files\nAudio equipment rental\n\n\n\n\n\nLocation rental costs\n\n\n\n\n\nData storage & backup\n\n\nProcessed data\nTranscription of interviews\nWord files\nPersonnel costs: hiring research assistants for manual entry\n\n\n\n\n\nData storage & backup\n\n\n\nAnalysis software\nR script\nPersonnel costs: programmer to write a programme to mine the data\n\n\nAnalysed data\nRegression graphic\nPhotoshop files\nSoftware costs\n\n\n\nProject Website\nHTML, Java\nHosting fee\n\n\n\n\n\nPersonnel to build initial website"
  },
  {
    "objectID": "guides/plan-and-design.html#rdm-requirements",
    "href": "guides/plan-and-design.html#rdm-requirements",
    "title": "How can you set up research data management from the start?",
    "section": "RDM Requirements",
    "text": "RDM Requirements\nIf you do research at the VU, you may be subject to the requirements for Research Data Management formulated by various parties. Please check which requirements apply to your research project.\nMany funders have specific requirements for RDM. The exact requirements vary by funder. They usually include a Data Management Section in the project proposal and a Data Management Plan (DMP) after funding has been granted. As funding agencies invest financially in your research project, they often have demands concerning research integrity, data quality, data publication and reusability. As research output, data are often compared to a kind of public good that should be made available to the community for re-use if possible. Always check what demands are set by a funder before you apply.\n\nFunding agencies\n\nData management section in project proposal\nAt a grant application, some funders request a short data section in your project proposal or an outline of a Data Management Plan. Without these your proposal will not be eligible for review.\n\nNWO: Data management section\nZonMw: Orientation of data management in project proposal\n\n\n\nData Management Plan\nIn a Data Management Plan (DMP; see also the section Data Management Plan) you explain how you will handle your research data. Check with your funder at what stage a DMP has to be submitted and how it should be composed. VU has a DMP template that has been acknowledged by NWO, ZonMw and ERC. We recommend you to use this VU template. See the DMP page for more information and instructions on how to select this template in DMPonline.\nThe tool DMPonline can be used to access and fill in a DMP template. You can also write a DMP in collaboration and invite a third party to comment or give feedback on your DMP. You can use the button ‘Request feedback’ to ask for feedback from a data steward. In order to write a DMP, you need to create your own account.\n\n\nOverview of funders’ RDM requirements and DMP templates\nThe Consortium of European Social Science Data Archives (CESSDA) presents a comprehensive overview of data management requirements and templates of the main Dutch and European funding bodies. This is helpful if you want to quickly find more information. However, make sure you always check the details that you receive in the documentation of your actual funding agency, so that you are aware of all up-to-date requirements.\n\nNational: NWO, ZonMw\n\n\n\nPublishing your data and terms of use\nNormally a funder requires you to publish your data in a data repository at the end of the project (unless this is prohibited by legislation). For that reason, DMP templates usually include the following questions:\n\nwhere your dataset can be found\nwhether your dataset has a Persistent Identifier\nhow your data are documented\nwhether your data may be reused freely or not and which terms and conditions apply\n\nPlease consider your funder’s data publishing requirements, so that you can take the necessary steps before and during your research project. For example, if you are working with personal data and you want to publish them in a data repository, this needs to be included in the informed consent forms that your participants have to sign.\n\n\n\nLocal requirements from your university and faculty\nThe VU is committed to support research that meets the highest requirements of replicability and transparency. The FAIR data principles, the purpose of which is to render research data Findable, Accessible, Interoperable and Reusable, the General Data Protection Regulation (GDPR) and the principles of Open Science are at the foundation of the Research Data Management (RDM) policy of the VU.\nIn addition to the central policy for RDM, faculties of the VU also have developed their own implementation of this policy.\nPlease check the relevant local policies and Standard Operating Procedures relevant for your faculty or department before you start your research project. An overview of all available policy documents can be found in the section VU policies and regulations.\n\n\nConsortium partners\nPartner institutions in a consortium may also have research data management requirements, for example with respect to data security. They may ask for:\n\ncertification in relation to data security of the VU’s infrastructure\nstatements from the IT department about the IT systems being used at the VU\n\nThe RDM Support Desk or your faculty’s research support office can help you with this."
  },
  {
    "objectID": "guides/plan-and-design.html#collaboration",
    "href": "guides/plan-and-design.html#collaboration",
    "title": "How can you set up research data management from the start?",
    "section": "Collaboration",
    "text": "Collaboration\nSome research projects involve more than one partner organisation. Be sure to indicate exactly who is responsible for collecting and managing the data in each case, where, and how. If more than one organisation is involved, it may also be necessary to create a Consortium Agreement. Depending on the area or sector of each project and of the degree of technical complexity that is involved, the Consortium Agreement usually contains the following information:\n\nprovisions on the governance structure of the consortium;\ntechnical provisions (e.g. the tasks of each party and the project schedule, description of the data collection responsibilities);\nfinancial provisions (e.g. the distribution of funds among participants, the financial plan, etc).\n\nThe agreement can include a section on who is ultimately responsible for the data and whether the data will be shared afterwards or whether certain restrictions on re-use apply. These restrictions can also be related to copyright issues or pending patent requests. IXA can help you to draw up a consortium agreement. The RDM Support Desk at the University Library can also help with questions about legal matters.\nIf you are working with personal data, GDPR requires that all parties working with the data sign a joint controller agreement. You can ask your 🔒 Privacy Champion for advice about this. For multi-centre clinical research, a Clinical Trial Agreement is recommended.\nFor projects funded by the European Union, several sources are available:\n\nFor Horizon 2020 projects a document is available, called “Guidance How to draw up your consortium agreement”."
  },
  {
    "objectID": "guides/plan-and-design.html#data-security",
    "href": "guides/plan-and-design.html#data-security",
    "title": "How can you set up research data management from the start?",
    "section": "Data Security",
    "text": "Data Security\n\nData classification\n‘Security’ is often regarded as a fixed state. Therefore, people tend to think of security measures as fixed solutions in the form of technological measures. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe value of data or applications is established through classification in Confidentiality, Integrity and Availability (CIA) or in Dutch Beschikbaarheid, Integriteit en Vertrouwelijkheid (BIV).\nTraditionally, this classification assesses the value of an entity (data or application) to an organisation. For research data, however, the value to the University is in all cases the same. The value of each research project is the same. Does that mean that there is no need to classify research data? Referring back to the definition of security, it is the assessment of the level of protection against a certain threat and its accuracy depends on the value of (in this case) data. The reason to classify research data is that there is a huge variety in potential risks in case of data loss or theft.\nThe reason that VU and its reseachers need to classify data is to understand the variety in risk that exists in order to assess if security measures are accurate.\nData classification is about the level of sensitivity (low, medium or high) of your data assets so you can judge the risks to your research (group). This will help you when deciding what security and protection measures you need to take for handling the data or parts of the data.\n\nData classification criteria\nIn order to classify your data collection or data processing (in categories from low, to medium, or high), the following properties are considered.\n\nAvailability: what risks are associated with accessibility to data (i.e. how readily do the data need to be available for use and how damaging would it be to your research if data are lost), what measures should you take to prevent data loss?\nIntegrity: what do you do to prevent measurement or data entry errors, corruption of stored data or unauthorised changes to the stored data?\nConfidentiality: how securely do data need to be managed to prevent sharing of data with unauthorised individuals? The necessity for confidentiality depends on the sensitivity of the information, either as sensitive personal information or confidential business information, as well as the vulnerability of the subjects from whom the data is collected and the laws that apply to the data being collected and analysed. In some cases, confidentiality can be very high; when the confidentiality is high or very high, please contact the RDM Support Desk.\n\nFor all of these aspects, the damage impact should be considered, i.e. te risks to all parties involved (i.e. participants, but also the VU as an institute, the researchers, any collaborators etc.). Untoward outcomes could be loss of privacy/secrecy, reputation damage, financial costs, fraud, mental, social or physical harm.\n\n\nExamples of Highly classified data\nYour data are classified as ‘high’ when you collect or process the following data:\n\npersonal data\nstate secrets\ncompetitive corporate information\nanimal-testing data\n\n\n\nPersonal data\nDo not confuse the risks of data loss with the need to comply to legal regulations. Data security is part of risk management and is aimed at balancing protection against productivity, investments against profit. The General Data Protection Regulation is a European Law in the legal area of Human Rights and concerns the use of personal data. Personal data are a type of data that is commonly processed in many fields of scientific research. You collect or process personal data when the data can be linked to a unique individual, either directly through direct identifiers such as name, address, IP-address etc., or indirectly through a combination of information. Personal data need to be protected. More information about personal data, data protection and the GDPR can be found in the section GDPR & Privacy.\n\n\nData Classification tool for researchers\nTo help you to determine the data classification for your research data assets, the VU has developed a tool that will help you to assess and classify the availability, integrity and confidentiality risks of these assets. Based on your results from using the tool, you may need to seek further advice from VU Security and Privacy Experts (see below). Some basic security tips were compiled by the data steward of the Faculty of Behavioural and Movement Sciences.\n\n\nVU Security and Privacy experts\nVU Security and Privacy experts can help you with the details on these aspects.\n\nGeneral questions about information security: RDM Support Desk. If you need advice when determining the data classification of your data assets, you can contact them.\nReporting a (potential) data breach: IT Servicedesk. A data breach is an incident in which the possibility exists that the confidentiality, integrity or availability of information or data processing systems has been potentially threatened, for example attempts to gain unauthorised access to information or systems (hacking), the loss of a USB stick with sensitive information, data theft of hardware.\nTailored advice or support: The RDM Support Desk can assist researchers in the process of requesting capacity at IT for setting up and/or assessing of information security plans or paragraphs. An information security plan is particularly important in projects with a complex infrastructure (e.g. international collaboration, use of various data sources and databases), tailored solutions and requirements from funding agencies or external partners.\n\nRead more practical information about this below in the section Data Protection & Security, or the GDPR support section.\n\n\n\nData Protection & Security\nWhere sensitive information is collected, the researcher must consider the following:\n\nwho has access to the data during the study, and how the data will be made available after publication\nwhat security regimes apply to sensitive data, and how data are protected\nhow data access during and after the project will be managed\nhow to deal with sensitive information\nwhether informed consent is required and how the forms will be accessed and stored\n\nOn the 🔒 VU Intranet information is available on Security, data loss and reporting incidents. Legal experts also can help you if you have questions about working with personal data and/or if you have to perform a Data Protection Impact Assessment. On the VU website you can find more information about 🔒 DPIAs at the VU. The data steward for the Faculty of Behavioural and Movement Sciences has also created a guide about data encryption."
  },
  {
    "objectID": "guides/plan-and-design.html#gdpr-privacy",
    "href": "guides/plan-and-design.html#gdpr-privacy",
    "title": "How can you set up research data management from the start?",
    "section": "GDPR & Privacy",
    "text": "GDPR & Privacy\n\nGDPR in Practice\n\nImportant definitions\n\nPersonal data refers to any information relating to an identified or identifiable natural person (‘data subject’). See also the definition of ’personal data’ according to the official text of the GDPR.\nData processing refers to any action performed on data, such as collecting, storing, modifying, distributing, deleting data. See also the definition of ‘processing’ according the official text of the GDPR.\nDirect and indirect identification: Some identifiers enable you to single out an indiviual directly, such as name, address, IP-address etc. Individuals can also be identifed indirectly through:\n\na combination of information that uniquely singles out an individual (e.g. a male with breast cancer in a breast cancer registry, a pregnant individual over 50 etc.), this includes information in one record and information across different data files or datasets\nunique information or patterns that are specific to an individual (e.g. genomic data, a very specific occupation, such as the president of a large company, repeated physical measurements or movement patterns that create a unique profile of an individual or measurements that are extreme and could be linked to subjects such as high-level athletes)\ndata that are linked to directly identifying information through a random identification code or number\n\nPseudonymous data: Data that are indirectly identifiable are generally considered to be pseudonymous; this means that they are NOT anonymous and still qualify as personal data. Therefore privacy laws, such as the GDPR, do in fact apply to these data. This is for example the case when direct identifiers are removed from the research data and put into a key file (or what is usually called a subject identification log in medical research) with which the direct identifiers can be mapped to the research data through unique codes, so that reidentification is possible. These data are therefore pseudonymous, and not anonymous. The LCRDM has made a reference card that illustrates the difference between pseudonymous and anonymous data.\n\n\n\nBackground information\n\nPrivacy in research - Privacy five-step plan\nWhere research requires the collection of personal data, the researcher has to follow the Privacy five-step plan to make sure to carry out the research in line with the GDPR.\n\n\nVSNU Code of Conduct for using personal data in research\nThe VSNU’s Code of Conduct for Research Integrity (Dutch, English, 2018) includes a reference to the GDPR and its Dutch implementation law UAVG. An updated Code of Conduct for Using Personal Data in Research which complies with GDPR is still work in progress.\n\n\n\nSupport within your faculty: Privacy Champions\nEach faculty has one or more Privacy Champions, who are the first point of contact for questions relating to privacy and the GDPR. The Privacy Champions can help you with completing a Data Protection Impact Assessment, registering your research in the record of processing activities, designing informed consent forms and other questions relating to the GDPR. The 🔒 list of Privacy Champions can be found on VU’s website. It is important that you make an overview of what data you are collecting. Your privacy champion can help you with this.\nAn important issue in informed consent forms, is the possible future (re-)use of the data. The Privacy Champion of the Faculty of Behavioural and Movement Sciences prepared a checklist for what to consider when creating an informed consent form. An important issue in informed consent forms, is the possible future (re-)use of the data. You should always ask your 🔒 Privacy Champion for advice when drawing up an informed consent form.\n\n\n\nComplete a Data Protection Impact Assessment (DPIA)\nWhen scientific research includes the processing of personal data, conducting a Data Protection Impact Assessment (DPIA) may be a legal requirement under the GDPR. If it is not a legal requirement, conducting a DPIA is always a helpful exercise to make sure that you address all legal aspects that need to be addressed. It is the best way to GDPR-proof your research."
  },
  {
    "objectID": "guides/plan-and-design.html#what-is-a-dpia",
    "href": "guides/plan-and-design.html#what-is-a-dpia",
    "title": "How can you set up research data management from the start?",
    "section": "What is a DPIA?",
    "text": "What is a DPIA?\nA DPIA is an assessment to identify the risks of processing personal data. It consists of a number of questions on the basis of which you determine whether the processing of personal data in your research project is legitimate and which measures should be taken to make sure this processing takes place within the boundaries of the GDPR. A DPIA doesn’t deliver an automatic report at the end, but it rather makes you think about all relevant topics you need to address before starting the processing of personal data. The outcome of a DPIA should be used to determine appropriate measures to mitigate the identified risks, such as data minimisation (not collecting more data than necessary), pseudonymising data, selecting appropriate tools for data storage and data sharing."
  },
  {
    "objectID": "guides/plan-and-design.html#when-is-a-dpia-required",
    "href": "guides/plan-and-design.html#when-is-a-dpia-required",
    "title": "How can you set up research data management from the start?",
    "section": "When is a DPIA required?",
    "text": "When is a DPIA required?\nA DPIA is required when the processing of personal data is likely to result in a “high risk” for the participants of your research project. This is for example most likely the case when scientific research includes the processing of special categories of personal data, such as data concerning health, religious or philosophical beliefs, political opinions or criminal convictions and offences (see Privacy in Research - 10 key rules for more information about special categories of personal data).\nThere are two DPIA lists which describe situations in which a DPIA is required:\n\nThe Dutch data protection authority (Autoriteit Persoonsgegevens) has published a list of 17 “high risk” situations in which a DPIA is mandatory.\nThe European data protection authorities have together published a list of 9 criteria which can be used to determine whether there is a “high risk”.\n\nYou should consult your 🔒 Privacy Champion to determine whether a PreDPIA is required in your situation."
  },
  {
    "objectID": "guides/plan-and-design.html#how-can-i-complete-a-dpia",
    "href": "guides/plan-and-design.html#how-can-i-complete-a-dpia",
    "title": "How can you set up research data management from the start?",
    "section": "How can I complete a DPIA?",
    "text": "How can I complete a DPIA?\nThe VU has a DPIA template based on a form provided by the Dutch Government (see the original template if you wish to have more background information, only available in Dutch).\nYou should request the template from your 🔒 Privacy Champion.\nPlease complete a DPIA at least before you start collecting personal data. In some cases, it might be useful to have a look at the DPIA template at the stage of writing a research proposal.\nIf you are not sure whether it is required to conduct a DPIA or if you need help completing a DPIA, please contact your faculty’s 🔒 Privacy Champion. If needed they can contact the legal specialists of Institutional and Legal Affairs."
  },
  {
    "objectID": "guides/plan-and-design.html#policies-regulations",
    "href": "guides/plan-and-design.html#policies-regulations",
    "title": "How can you set up research data management from the start?",
    "section": "Policies & Regulations",
    "text": "Policies & Regulations\n\nVU General Policies and Regulations\n\nResearch data management policy\nVU Amsterdam considers the careful handling of research data to be very important. The university has therefore formulated a Research Data Management policy which provides guidance for researchers and policy officers at VU Amsterdam.\n\nVU RDM policy (2020)\n\nSince the VU policy for RDM is formulated in general terms, faculties have worked out more detailed guidelines for their own faculty. These faculty-specific guidelines can be found below.\n\nACTA RDM policy, Academisch Centrum Tandheelkunde Amsterdam (2020, in Dutch)\nBeta RDM policy, Faculty of Science (2022)\nFGB RDM policy, Faculty of Behavioural and Movement Sciences (2023)\nFGW RDM policy , Faculty of Humanities (2023)\nFRT RDM policy, Faculty of Religion and Theology (2024)\n🔒 FSW RDM policy, Faculty of Social Sciences (2023)\nRCH RDM policy, Faculty of Law (2021)\nSBE RDM policy, School of Business and Economics (2023)\n\nFor RDM policies and guidelines at Amsterdam UMC, location VUmc, please get in touch with Research Data Management Support at Amsterdam UMC.\n\n\nAcademic integrity complaints procedure\nBoth VU Amsterdam and Amsterdam UMC, location VUmc, employ a joint policy for the handling academic integrity complaints. This policy outlines the steps to be taken in the event of a complaint, the officers who play a role in this procedure, and what should be expected once a complaint has been lodged.\n\n\nConfidential counselors\nThe VU has a number of confidential counsellors who handle academic integrity issues.\n\n\nData breach incident report\nFrom 2016 onwards, any data security breaches (particularly those that have, or are likely to have, serious adverse consequences to the protection of personal data) should be reported immediately to the IT Servicedesk. Read the 🔒 protocol reporting a data breach.\n\n\nRegulations and Guidelines\nSome faculties and departments have their own guidelines for RDM. You can find an overview of such guidelines below.\n\nACTA Research Code\nAmsterdam Public Health Quality Handbook\nFGB:\n\nCode of Ethics for Research in the Social and Behavioural Sciences Involving Human Participants\nCollection of guidelines and Standard Operating Procedures (SOPs)\n\nSBE Research Ethics Regulations for Researchers and route map for research data management\n\n\n\n\nEthics Committees\nIn cases where research involves human or animal participants, a research proposal may need to be reviewed by an ethics committee. VU and Amsterdam UMC, location VUmc, have several ethics committees, which are listed below. Please note that researchers at the VU also have to go to the METc at VUmc if their research is subject to the WMO, which is not restricted to research at VUmc.\n\nACTA: ACTA Institutional Review Board (IRB)\nBeta: Research ethics review committee Faculty of Science (BETHCIE)\nFGB: 🔒 Scientific and Ethical Review Board (VCWE)\nFGW: Ethische Toetsingscommissie Onderzoek (EtCO)\nFSW: 🔒 Research Ethics Review Committee (RERC)\nRCH (Faculty of Law): Ethics Committee\nVUmc (Amsterdam UMC): Medical Ethical Review Committee (METc)\n\n\n\nNetherlands Code of Conduct for Scientific Integrity\nDutch scientists are required to comply with the Netherlands Code of Conduct for Research Integrity (VSNU, 2018). The principles of proper scientific and scholarly research, according to the Code of Conduct are:\n\nHonesty\nScrupulousness\nTransparency\nIndependence\nResponsibility\n\nThe principles of honesty and transparency state explicit guidelines on the way in which you treat your research data:\n\nHonesty: you should refrain from fabricating or falsifying data\nTransparency:\n\nYou should ensure that it is clear to others what data your research is based on, how the data were obtained, what the results are and how you got to these results\nAll steps in your research process must be verifiable (e.g. choice of research question, research design, methodology, sources used), so that it is clear to others how your research was conducted\n\n\nTo live up to these general principles, the Code of Conduct provides the following standards, which are addressed in a DMP, for good research practices related to data management:\n\nProvide a description of the way in which the collected research data are organised and classified, so that they can be verified and re-used (standard 3.2.10)\nMake research data public upon completion of your research project; if this is not possible, explain why (standards 3.2.11 and 3.4.45)\nDescribe the data you have collected and used in your research honestly, scrupulously and transparently (standard 3.3.23)\nManage your data carefully and store both the raw and processed versions for a period appropriate for your discipline (standard 3.3.24)\nContribute towards making data FAIR, where possible (standard 3.3.25)\nBe transparent about your methods and working procedures by using e.g. research protocols, logs, lab journals or reports to describe these processes (standard 3.4.35)\n\n\n\nStrategy Evalution Protocol\nThe Strategy Evaluation Protocol 2021-2027 (SEP) from the VSNU is used to assess the quality of research at Dutch universities, NWO and Academy institutes. It promotes the handling and storing of raw and processed data with care and integrity.\nThe SEP formulates questions on how a research institute deals with and stores raw and processed data. It also assesses the output of research institutes, including datasets, and the use of such output by peers and societal target groups.\nBy registering your datasets in the VU Research Portal, you contribute to an overview of datasets of your department, faculty and the VU as a whole.\n\n\nNWO Data Policy\nNWO aims to ensure that all the research it funds is openly accessible to everyone as part of it’s Open Science policy. Researchers are therefore expected to preserve the data resulting from their projects for at least ten years, unless legal provisions or discipline-specific guidelines dictate otherwise. As much as possible, research data should be made publicly available for re-use. As a minimum, NWO requires that the data underpinning research papers should be made available to other researchers at the time of the article’s publication, unless there are valid reasons not to do so.\nThe guiding principle here is ‘as open as possible, as closed as necessary.’ Due consideration is given to aspects such as privacy, public security, ethical limitations, property rights and commercial interests. In relation to research data, NWO recognizes that software (algorithms, scripts and code developed by researchers in the course of their work) may be necessary to access and interpret data. In such cases, the data management plan will be expected to address how information about such items will be made available alongside the data.\nMore information on Data Management is also available on the NWO website where a NWO Data Management Template is made available. The VU Data Management template in DMP Online is certified by both NWO and ZonMW and can also be used by VU researchers for projects funded by both organizations."
  },
  {
    "objectID": "guides/document-and-preserve.html",
    "href": "guides/document-and-preserve.html",
    "title": "How can you ensure research data is FAIR?",
    "section": "",
    "text": "There is a difference between storing and archiving data. Storing refers to putting the data in a safe location while the research is ongoing. Because you are still working on the data, the data still change from time to time: they are cleaned, and analysed, and this analysis generates output. As the image below illustrates, storing could be like cooking a dish: you are cleaning and combining ingredients.\nArchiving, on the other hand, refers to putting the data in a safe place after the research is finished. The data are in a fixed state, they don’t change anymore. Archiving is done for verification purposes: so others can check that your research is sound. Or: it is done so that others can reuse the resulting dataset. There is also a difference between archiving and publishing, but in essence, archiving and publishing happen at a similar moment and for both, data do not change anymore.\n\n\n\nA Scriberia illustration showing storage on the left, in a kitchen space with storage making things available, and archiving on the right, in a museum where it is available for viewing.\n\n\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\nSelecting Data for Archiving\nThere are various reasons to archive your data: replication, longitudinal research, data being unique or expensive to collect, re-usability and acceleration of research inside or outside your own discipline. It is VU policy to archive your data for (at least) 10 years after the last publication based on the dataset. Part of preparing your dataset for archiving is appraising and selecting your data.\n\nMake a selection before archiving your data\nDuring your research you may accumulate a lot of data, some of which will be eligible for archiving. It is impossible to preserve all data infinitely. Archiving all digital data leads to high costs for storage itself and for maintaining and managing this ever-growing volume of data and their metadata; it may also lead to decline in discoverability (see the website of the Digital Curation Centre). For those reasons, it is crucial that you make a selection.\n\n\nRemove redundant and sensitive data\nSelecting data means making choices about what to keep for the long term, and what data to archive securely and what data to publish openly. This means that you have to decide whether your dataset contains data that need to be removed or separated. Reasons to exclude data from publishing include (but are not limited to):\n\ndata are redundant\ndata concern temporary byproducts which are irrelevant for future use\ndata contain material that is sensitive, for example personal data in the sense of the GDPR, like consent forms, voice recordings, DNA data; state secrets; data that are sensitive to competition in a commercial sense. These data need to be separated from other data and archived securely\npreserving data for the long term is in breach of contractual arrangements with your consortium partners or other parties involved\n\nIn preparing your dataset for archiving, the first step is to determine which parts of your data are sensitive, which can then be separated from the other data. Redundant data can be removed altogether.\n\n\nDifferent forms of datasets for different purposes\nOnce you have separated the sensitive data from the rest of your dataset, you have to think about what to do with these sensitive materials. In some cases they may be destroyed, but you may also opt for archiving multiple datasets. For example, you may want to archive your dataset in more than one form depending on the purpose. For example:\n\nOne for reusability to share\nA second one that contains the sensitive data, and needs to be handled differently.\n\nFor the first, the non-sensitive data can be stored in an archive under restricted or open access conditions, so that you can share it and link it to publications. For the second, you need to make a separate selection, so the sensitive part can be stored safely in a secure archive (a so-called offline or dark archive). In the metadata of both archives you can create stable links between the two datasets using persistent identifiers.\n\n\nWhat to appraise for archiving\nThere are several factors that determine what data to select for archiving. For example, whether data are unique, expensive to reproduce, or if your funder requires that you make your data publicly available. This might also help you or your department to think about a standard policy or procedures for what needs to be kept, what is vital for reproducing research or reuse in future research projects.\nMore information on selecting data:\n\nTjalsma, H. & Rombouts, J. (2011). Selection of research data: Guidelines for appraising and selecting research data. Data Archiving and Networked Services (DANS).\nDigital Curation Centre (DCC): Whyte, A. & Wilson, A. (2010). How to appraise and select research data for curation. DCC How-to Guides. Edinburgh: Digital Curation Centre.\nResearch Data Netherlands: Data selection.\n\n\n\n\nData Set Packaging: Which Files should be Part of my Dataset?\nA dataset consists of the following documents:\n\nRaw or cleaned data (if the cleaned data has been archived, the provenance documentation is also required)\nProject documentation\nCodebook or protocol\nLogbook or lab journal (when available, dependent on the discipline)\nSoftware (& version) needed to open the files when no preferred formats for the data can be provided\n\nSee the topic Metadata for more information about documenting your data.\nDepending on the research project it may be that more than one dataset is stored in more than one repository. Make sure that each consortium partner that collects data also stores all necessary data that is required for transparency and verification. A Consortium Agreement and Data Management Plan will include information on who is responsible for archiving the data.\nBy creating documentation about your research data you can make it easier for yourself or for others to manage, find, assess and use your data. The process of documenting means to describe your data and the methods by which they were collected, processed and analysed. The documentation or descriptions are also referred to as metadata, i.e. data about data. These metadata can take various forms and can describe data on different levels.\nAn example that is frequently used to illustrate the importance of metadata is the use of the label on a can of soup. The label tells you what kind of soup the can contains, what ingredients are used, who made it, when it expires and how you should prepare the soup for consumption.\nWhen you are documenting data, you should take into account that there are different kinds of metadata and that these metadata are governed by various standards. These include, but are not limited to:\n\nFAIR data principles: a set of principles to make data Findable, Accessible, Interoperable and Reusable.\nGuidelines for unstructured metadata: mostly research domain-specific guidelines on how to create READMEs or Codebooks to describe data.\nStandards for structured metadata: generic or research domain-specific standards to describe data.\n\nThe CESSDA has made very detailed guidance available for creating documentation and metadata for your data.\n\n\n\nA layered diagram with the FAIR principles as the outermost layer, followed by an inner layer for Metadata. Within Metadata there are two separate cores, one for unstructured and one for structured metadata. Unstructured metadata contains README and codebook; Structured metadata contains Generic and Specific.\n\n\n\n\nFAIR data principles\nThe FAIR data principles provide guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. The principles emphasise machine-actionability, i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention.\nMore information can be found in the section about the FAIR data principles.\n\n\nUnstructured metadata\nMost data documentation is an example of unstructured metadata. Unstructured metadata are mainly intended to provide more detailed information about the data and is primarily readable for humans. The type of research and the nature of the data influence what kind of unstructured metadata is necessary. Unstructured metadata are attached to the data in a file. The format of the file is chosen by the researcher. More explanation about structured metadata can be found on the metadata page.\n\nREADME\nA README file provides information about data and is intended to ensure that data can be correctly interpreted, by yourself or by others. A README file is required whenever you are archiving or publishing data.\nExample of READMEs\n\nGuidelines for creating a README file – 4TU.ResearchData\nGuide to writing “readme”-style metatada - Cornell Data Services\nGuidelines for researchers of the VU Faculty of Behavioural and Movement Sciences on what a README file should contain\n\n\n\nCodebook\nA Codebook is another way to describe the contents, structure and layout of the data. A well documented codebook is intended to be complete and self-explanatory and contains information about each variable in a data file. A codebook must be submitted along with the data.\nThere are several guides for creating a codebook available:\n\nCreating a codebook - Kent State University\nCreating a codebook - for researchers at the VU Faculty for Behavioural and Movement Sciences\nCodebook - Amsterdam Public Health\nDDI-Codebook - Data Documentation Initiative Alliance\n\nMetadata provide information about your data. Structured metadata are intended to provide this information in a standardised way. The structured metadata are readable for both humans and machines. It can be used by data catalogues, for example DataCite Commons.\nThe standardisation of metadata involves the following aspects:\n\nElements: rules about the fields that must be used to describe an object, for example the title, author and publicationDate.\nValues: rules about the values that must be used within specific elements. Controlled vocabularies, classifications and Persistent Identifiers are used to reduce ambiguity and ensure consistency, for example by using a term from a controlled vocabulary like the Medical Subject HEadings (MeSH) as a subject and an Persistent Identifier such as an ORCID to identify a person.\nFormats: rules about the formats used to exchange metadata, for example JSON or XML.\n\n\n\n\nMetadata standards\nMetadata standards allow for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At the VU the following standards are used:\n\nYoda uses the DataCite metadata standard\nDataverseNL uses the Dublin Core metadata standard\nThe VU Research Information System PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology.\n\n\nControlled Vocabularies & Classifications\nControlled vocabularies are lists of terms created by domain experts to refer to a specific phenomenon or event. Controlled vocabularies are intended to reduce ambiguity that is inherent in normal human languages where the same concept can be given different names and to ensure consistency. Controlled vocabularies are used in subject indexing schemes, subject headings, thesauri, taxonomies and other knowledge organisation systems. Some vocabularies are very internationally accepted and standardised and may even become an ISO standard or a regional standard/classification. Controlled vocabularies can be broad in scope or very limited to a specific field. When a Data Management Plan template includes a question on the used ontology (if any), what is usually meant is: is there a specific vocabulary or classification system used? The National Bioinformatics Infrastructure Sweden gives some more explanation about controlled vocabularies and ontologies here. In short, an ontology does not only describe terms, but also indicates relationships between these terms.\nExamples of controlled vocabularies are:\n\nCDWA (Categories for the Description of Works of Art)\nGetty Thesaurus of Geographic names\nNUTS (Nomenclature of territorial units for statistics)\nMedical Subject HEadings (MeSH)\nThe Environment Ontology (EnvO)\n\nMany examples of vocabularies and classification systems can be found at the FAIRsharing.org website. It has a large list for multiple disciplines. If you are working on new concepts or new ideas and are using or creating your own ontology/terminology, be sure to include them as part of the metadata documentation in your dataset (for example as part of your codebook).\n\n\nMetadata levels\nFinally a distinction can be made on the level of description. Metadata can be about the data as a whole or about part of the data. It can depend on the research domain and the tools that are used on how many levels the data can be described. In repositories like Yoda and DataverseNL it is common practice to only create structured metadata on the level of the data as a whole. The Consortium of European Social Science Data Archives (CESSDA) explains this distinction for several types of data in their Data Management Expert Guide.\n\n\n\nFlowchart indicating a project with a Folder a and Folder b, where Folder a has File 1 and File 2. The project, Folder a, and File 1, have linked metadata to them.\n\n\n\n\nDataset registration\nWhen you want to make sure that your dataset is findable it is recommended that the elements of the description of your dataset are made according to a certain metadata standard that allows for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At the VU the following standards are used:\n\nDataverseNL and DANS use the Dublin Core metadata standard\nThe VU Research Portal PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology. Go to the overview of metadata standards. More important tips are available at Dataset & Publication."
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Guides",
    "section": "",
    "text": "What is a guide?\n\n\n\nThese guides help you find answers to questions that come up while doing research. They help guide you through various topics at once.\nMissing a guide? You can submit questions you are dealing with using the Contribution portal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you discover and reuse existing research data?\n\n\nThere is so much data out there, that we want to help you find your way more easily.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you ensure data protection and security during collection, storage, and transfer?\n\n\nLearn about how to secure research data at any stage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you ensure data provenance and accurate data analysis?\n\n\nWhere data and results come from matters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you ensure research data is FAIR?\n\n\nMaking your data Findable, Accessible, Interopable, Reusable is more doable than you might think.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you set up research data management from the start?\n\n\nA good plan is half the work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat data assets should you publish and what data assets should you archive?\n\n\nArchival must happen, publishing can happen.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat research data services and support are available for VU researchers?\n\n\nFind some resources that can help you along your research journey.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-09-05hackathon.html",
    "href": "blog/2024-09-05hackathon.html",
    "title": "Second Handbook Hackathon",
    "section": "",
    "text": "On September 5th, 2024, all authors participated in the second hackathon for the Research Support Handbook. For this hackathon, we focused on quality assurance. Quality assurance is crucial in ensuring that all parties at VU Amsterdam feel confident adopting this evolving, community-led resource as an official university page. Our goal is to migrate the handbook to rdm.vu.nl in the near future, which requires us to everything is in tip-top shape.\nCollectively, we proofread the migrated LibGuide pages. The result of this are seven GitHub issues and nine pull requests for revisions. These include making sure all images are migrated into the handbook, ensuring appropriate page titles, and refining the contribution guide.\nDuring the hackathon, we observed a lot of links in the old LibGuide resource, are broken. We did some much needed maintenance work during the hackathon, given that the Research Support Handbook will replace the LibGuide resource. We identified broken links and replaced them where possible. There is a need to make this a less manual process in the future and will seek out automations to help us maintain the links moving forward. Additionally, there are many internal links that need updating, changing from LibGuide URLs to GitHub pages. We also discovered several links accessible only with university credentials, which we believe detract from the user experience; these links will require special labeling and contextualization.\nThe general sentiment at the end of the hackathon is that there is still lots of work to do. The community standards are manifesting themselves, and it is clear that pathways need more work to be production ready. These discussions surfaced a wish to have more frequent hackathons at this time, until the standards are set and the included content is ready for rdm.vu.nl."
  },
  {
    "objectID": "blog/2024-09-05hackathon.html#hackathon-issues",
    "href": "blog/2024-09-05hackathon.html#hackathon-issues",
    "title": "Second Handbook Hackathon",
    "section": "Hackathon Issues",
    "text": "Hackathon Issues\n\nhttps://github.com/ubvu/open-handbook/issues/123\nhttps://github.com/ubvu/open-handbook/issues/128\nhttps://github.com/ubvu/open-handbook/issues/125\nhttps://github.com/ubvu/open-handbook/issues/124\nhttps://github.com/ubvu/open-handbook/issues/123\nhttps://github.com/ubvu/open-handbook/issues/130\nhttps://github.com/ubvu/open-handbook/issues/131"
  },
  {
    "objectID": "blog/2024-09-05hackathon.html#hackathon-pull-requests",
    "href": "blog/2024-09-05hackathon.html#hackathon-pull-requests",
    "title": "Second Handbook Hackathon",
    "section": "Hackathon Pull Requests",
    "text": "Hackathon Pull Requests\n\nhttps://github.com/ubvu/open-handbook/pull/126\nhttps://github.com/ubvu/open-handbook/pull/110\nhttps://github.com/ubvu/open-handbook/pull/127\nhttps://github.com/ubvu/open-handbook/pull/129\nhttps://github.com/ubvu/open-handbook/pull/132\nhttps://github.com/ubvu/open-handbook/pull/133\nhttps://github.com/ubvu/open-handbook/pull/134\nhttps://github.com/ubvu/open-handbook/pull/135"
  },
  {
    "objectID": "blog/2024-11-01hackathon.html",
    "href": "blog/2024-11-01hackathon.html",
    "title": "Fourth Handbook Hackathon",
    "section": "",
    "text": "On October 30th, 2024, all authors participated in the fourth hackathon for the Research Support Handbook. Since the third hackathon, we migrated to rdm.vu.nl, which is a huge milestone! 🎉\nFor the fourth hackathon, we focused on tying up loose ends that we accumulated. The migration means we are now in a stable state, yet there is always more work to be done. During this hackathon, we focused on picking up stale discussions, reviewing open pull requests, and generally closing issues that we took too long to revisit.\nWe considered how (un)balanced the various Topics had gotten, because we want readers to be able to form consistent expectations. We observed that some topics are related to tools, others to concepts. Some topics are concise, whereas others are lengthy.\nOne breakout group focused on creating templates for both kinds of topics, resulting in an initial structure that will make it easier to start new topics in the future. Specifically we propose the following template structure for tools (for example, Qualtrics, HPC, DMPonline):\nAnd the following structure for concepts (for example, DMP, data citation, data storage):\nWe will follow up with a new hackathon in late november."
  },
  {
    "objectID": "blog/2024-11-01hackathon.html#issues-worked-on",
    "href": "blog/2024-11-01hackathon.html#issues-worked-on",
    "title": "Fourth Handbook Hackathon",
    "section": "Issues worked on",
    "text": "Issues worked on\nhttps://github.com/ubvu/open-handbook/issues/232\nhttps://github.com/ubvu/open-handbook/issues/233"
  },
  {
    "objectID": "blog/2024-11-01hackathon.html#pull-requests-worked-on",
    "href": "blog/2024-11-01hackathon.html#pull-requests-worked-on",
    "title": "Fourth Handbook Hackathon",
    "section": "Pull Requests worked on",
    "text": "Pull Requests worked on\nhttps://github.com/ubvu/open-handbook/pull/229\nhttps://github.com/ubvu/open-handbook/pull/231\nhttps://github.com/ubvu/open-handbook/pull/234\nhttps://github.com/ubvu/open-handbook/pull/235\nhttps://github.com/ubvu/open-handbook/pull/236\nhttps://github.com/ubvu/open-handbook/pull/237"
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html",
    "href": "blog/2024-10-22post-mortem.html",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "",
    "text": "On Friday October 18th 2024, we experienced around ten hours of downtime on the rdm.vu.nl website. The downtime started around 10AM after merging changes to the handbook and was resolved the same day, by 8PM.1"
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#root-cause",
    "href": "blog/2024-10-22post-mortem.html#root-cause",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "Root cause",
    "text": "Root cause\nThe downtime started after merging changes to the handbook in commit 669b065. These changes themselves, did not cause the downtime. The root cause was an incorrect configuration in the deployment of the webpage, which inadvertently removed the rdm.vu.nl domain name from the GitHub settings every time we made changes in the handbook and redeployed the website. This resulted in 404 errors that the page could not be found.\nWe first observed this issue on Thursday, one day prior to the downtime, in commit 678ff88. We proposed a fix for this issue in #220, before the downtime started."
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#how-could-the-downtime-happen-if-the-fix-was-clear",
    "href": "blog/2024-10-22post-mortem.html#how-could-the-downtime-happen-if-the-fix-was-clear",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "How could the downtime happen if the fix was clear?",
    "text": "How could the downtime happen if the fix was clear?\nThe fix for the domain specification was not merged in time for two reasons.\n\nReason 1\nAt this time, we require two reviews before merging changes to the handbook. The fix was proposed at 1.11PM on Thursday, and did not receive the required reviews by the time the downtime happened (reason 1). However, this was a technical administration task and could have been merged immediately, as this supercedes regular review procedures.\n\n\nReason 2\nThe domain specification fix was not immediately merged to allow time to pass and ensure the fix was appropriate upon further reflection. This is because administrator (chartgerink?) both proposed the fix and would also be the one to supercede the “two reviews” requirement. Due to travel, the administrator forgot about it in the morning, and only saw the messages about the downtime at 8PM. At that time, the fix was quickly merged and the downtime resolved."
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#improvements",
    "href": "blog/2024-10-22post-mortem.html#improvements",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "Improvements",
    "text": "Improvements\nThe domain configuration is corrected and the deployment ensures the domain name is re-added to the GitHub settings every time there are changes to the handbook. This is now automated, which ensures that the domain name will not be removed inadvertently when future changes are incorporated.\nHowever, we also learned that critical administration fixes should not be left open for longer than is absolutely necessary. Here it was left open for longer than absolutely necessary due to travel, and a second administrator could have caught this issue sooner. This means we should work towards resilient reporting mechanisms to escalate such critical issues, and build capacity in the editor team to ensure no one person is responsible for merging critical fixes that are already available."
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#acknowledgements",
    "href": "blog/2024-10-22post-mortem.html#acknowledgements",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank the editors for dealing with the stress from this inadvertent issue. We also thank the community for their patience as we only recently migrated to rdm.vu.nl and figure out these initial unexpected hurdles."
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#references",
    "href": "blog/2024-10-22post-mortem.html#references",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "References",
    "text": "References\n\nRoot cause for the downtime first observed in commit 678ff88\nProposed a fix for the root cause in pull Request #220\nDowntime started at commit 669b065 is where the\nResolved downtime in commit 678ff88"
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#footnotes",
    "href": "blog/2024-10-22post-mortem.html#footnotes",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nubvu.github.io/open-handbook remained online during this entire time, and functions indirectly as a backup.↩︎"
  },
  {
    "objectID": "editors-guide.html",
    "href": "editors-guide.html",
    "title": "Editor’s guide",
    "section": "",
    "text": "Welcome to the Editor’s guide to the Handbook! This page contains resources around how the editors work.\nTemporary initial editors are:"
  },
  {
    "objectID": "editors-guide.html#what-does-an-editor-do",
    "href": "editors-guide.html#what-does-an-editor-do",
    "title": "Editor’s guide",
    "section": "What does an editor do?",
    "text": "What does an editor do?\nEditors tie together all the strings in this community handbook. We keep a bird’s eye view to ensure that what you read makes sense. Editors also help ensure the style and quality of the different pages are similar.\nEach editor commits themselves to providing timely reviews of topics, guides, or both.1 This commitment is for a limited time and can be renewed. We also welcome more editors at any time, given that we do not expect all editors to review everything.\nAs we go on this journey together, we may assign more specific responsibilities as they emerge."
  },
  {
    "objectID": "editors-guide.html#quality-standards",
    "href": "editors-guide.html#quality-standards",
    "title": "Editor’s guide",
    "section": "Quality standards",
    "text": "Quality standards\nAs editors, we maintain a bunch of quality standards, both automated and not automated. If you are reading this as a contributor, you will greatly help us out by taking these into account.\nHere are some quality standards we maintain throughout the handbook:\n\nAcronyms must be written in full at least once on the page where they are used\nAll changes to the handbook are made via pull requests. Each change needs approval from two editors.\nAll images must have alt text\nLinks must be descriptive (no “click here” links)\nNo writing in name of the handbook (for example, “we recommend repository X”)\n\n\nTopics\nFor topics we maintain an additional set of standards:\n\nTopics must be nouns or noun phrases\nAll topics must be title capitalized (see also the helpful tool CapitalizeMyTitle.com)\nNo include statements are allowed in topics\n\nInclude statements must be prefaced with the relevant section heading, as the title of a topic is not reproduced.\n\nNo use of special Quarto code is allowed. Only use regular markdown in topics.\n\n\n\nGuides\nFor guides we maintain other standards:\n\nGuides must be phrased as questions that the reader will get answers to"
  },
  {
    "objectID": "editors-guide.html#how-to-keep-an-overview-of-everything",
    "href": "editors-guide.html#how-to-keep-an-overview-of-everything",
    "title": "Editor’s guide",
    "section": "How to keep an overview of everything?",
    "text": "How to keep an overview of everything?\nWith so many issues and pull requests, it is easy to get lost as an editor. We have a project management board that can be helpful identifying what is going on at this time:\n\nTopics\nGuides"
  },
  {
    "objectID": "editors-guide.html#etiquette",
    "href": "editors-guide.html#etiquette",
    "title": "Editor’s guide",
    "section": "Etiquette",
    "text": "Etiquette\nAs editors, we may have to make tough calls at times. It is important for us to make people feel welcome and appreciated, even if their contribution is not immediately included. That being said, we reciprocate the consideration given to us. We operate under a generosity policy, and if reciprocated, we stay generous.\n\nGitHub etiquette\nAs editors, we also maintain a certain GitHub etiquette. It is necessary to make managing a project with so many moving pieces and contributors. Important is:\n\nNew topics or guides must be linked to the issue proposing it\nEach pull request should have a clear purpose and stick to it (for example, no editing a guide when proposing a topic)\n\nItem 2 also means changes should be branch specific, as pull requests are based off branches. It makes it much harder to review things if there are many different kinds of changes, as we editors will need to keep track of all of this.\nSimplicity is our friend. Simplicity helps us from making mistakes."
  },
  {
    "objectID": "editors-guide.html#protocols",
    "href": "editors-guide.html#protocols",
    "title": "Editor’s guide",
    "section": "Protocols",
    "text": "Protocols\n\nHackathon protocol\nWe sometimes run hackathons to create space to contribute. We run hackathons as follows:\nPreparation:\n\nHackathons are two hours long\nHackathons are run using Zoom\nEach hackathon has a theme (it helps focus people’s energy on something and can inspire participation)\nWe use a collaborative note taking document that requires no logging in, which is the central place to navigate the hackathon\nAssign a host\n\nDuring:\n\nOpen 10 breakout rooms in zoom, allowing participants to freely move around\nThe host…\n\n…Welcomes everyone with an icebreaker question\n…Shares the link to the note taking document when people join\n…Sets the stage for the hackathon when it starts\n…Announces a break at the halfway mark\n\nKeep track of all the issues and pull requests opened for the speed blog\n\nAfter:\n\nFinish up the speed blog for the hackathon within one week of the hackathon. It does not have to be perfect and is primarily to document that the hackathon happened and some of the things that were done."
  },
  {
    "objectID": "editors-guide.html#footnotes",
    "href": "editors-guide.html#footnotes",
    "title": "Editor’s guide",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEach editor chooses which of these they want to focus on. Editors get auto-invited to review the changes. Timely means within a week.↩︎"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "You can contribute to the VU CS Department RDM and Open Science Handbook by making small edits, writing entirely new topics, or writing guides. All contributions are welcome and appreciated, small and large. If you are in need of specific information, you can skip ahead using the table of contents."
  },
  {
    "objectID": "contributing.html#contributing-portal",
    "href": "contributing.html#contributing-portal",
    "title": "Contributing",
    "section": "Contributing portal",
    "text": "Contributing portal\nWe offer a portal to reduce the barriers to contribute to the VU CS Department RDM and Open Science Handbook. You only need an internet connection and articulate what you want us to include. No accounts necessary 😊\n\n\n\n\n\n\nNote\n\n\n\nOpen the contribution portal by clicking here or copy-pasting: https://ez-github-contributor.netlify.app/\n\n\nYou can report issues you find with the VU CS Department RDM and Open Science Handbook using the “Report a problem” tab. This is a way for you to share your feedback with us.\nYou can propose new topics or guides to the VU CS Department RDM and Open Science Handbook using the “Propose new page” tab. This will be considered for inclusion. Please mention whether it should be a topic or a guide. The text editor allows you to use rich text formatting.\n\n\n\n\n\n\nWarning\n\n\n\nThe portal does not save your work. Use the portal when you are ready to submit your work, but do not use it to manage your submissions.\n\n\n\n\n\nScreenshot of the contributor portal\n\n\nIf you want to be credited with contributing, please share your name. If you’d like to hear back about what was done with your feedback or proposal, please also provide a direct way to contact you."
  },
  {
    "objectID": "contributing.html#contributing-via-github",
    "href": "contributing.html#contributing-via-github",
    "title": "Contributing",
    "section": "Contributing via GitHub",
    "text": "Contributing via GitHub\n\n\n\n\n\n\nNote\n\n\n\nFor the next steps you need a GitHub account to contribute. You can create one directly on GitHub.\n\n\n\nSuggesting edits\nThe easiest and quickest way to contribute to the book is make suggested edits. On each page you will find a button reading “Edit this page” (usually on the right).\n\n\n\nScreenshot of a handbook topic, with a red box on the right hand side of the page indicating where to find the “Edit this page” button\n\n\nWhen you click that, you will immediately be taken to GitHub to edit the text of that specific page. You may be prompted to create a fork (forking) in case these are your first edits.\n\n\n\nScreenshot of the GitHub file editor, with some changes made and the “Commit changes” button active\n\n\nOnce you made your edits, you are ready to commit (save) your changes and submit your pull request, requesting those changes to be included in the handbook.\n\n\nAdding a topic\nTo add a new topic, you need to create a new file ending in .qmd in the topics folder (e.g., topics/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the “New file” button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nThe topic itself needs to be written in Markdown. Every topic must be a noun/noun phrase and contain the title as such:\n---\ntitle: Example topic\n---\nSection headings are second level headings (e.g., ## Section). You can add all needed information as you want, but please mind that topics are supposed to short and self-contained for readers of the VU CS Department RDM and Open Science Handbook.\nAfter that, you are ready to submit your pull request! The reviewers will help you place the topic in the right place of the book.\n\n\nAdding a guide\nTo add a new guide, you need to create a new file ending in .qmd in the guides folder (e.g., guides/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the “New file” button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nEvery guide title must reflect the question the guide answers. Add the title by adding the following information at the top of your document:\n---\ntitle: How do I create a guide?\n---\nSection headings are second level headings (e.g., ## Section). The guide itself needs to be written in Markdown.\nYou can re-use topics literally in your guides. For each topic you want to include, you can either mention so on a line surrounded by whitespaces:\nINSERT TOPIC: DATA MANAGEMENT PLAN\nThis will tell the editorial team to include that topic there. Please be specific in naming the topic. You can also directly include the topic yourself directly using the following code:\n\n## Topic name\n\n    ```{.include shift-heading-level-by=2}\n    ../topics/replace-with-filename.qmd\n    ```\nYou need to count the heading level in your guide to identify your shift number. In this case, there are two ## so we shift by two. You can verify the filename directly, but it should correspond to each word separated by a minus sign (for example, data-management-plan.qmd).\nAfter that, you are ready to submit your pull request!\n\n\nSubmit a pull request\nOnce you have made suggested changes, a pull request is the way for you to ask for your changes to be incorporated into the VU CS Department RDM and Open Science Handbook. The handbook editors will review what you wrote, ask some questions, and accept or decline your contributions.\nWe recommend keeping your suggested changes small or limited in scope, and explaining why you are suggesting these changes. It is more likely your changes are included when you are fixing a typo or adding a paragraph, and less likely if you are revising the entire handbook. It is also more likely they are included if you explain why you are suggesting the changes, rather than dropping by and making edits without any context.\nIf you are adding a new topic or guide, it is definitely recommended to open an issue first to see whether there is a need for it (and maybe you’ll find collaborators!).\nDuring the review process you may be asked to update your changes, or revisions may be added by the people maintaining the handbook. It is helpful if you keep an eye on your GitHub account to ensure timely responses to help the process along. By contributing, you become part of the process :blush:\n\n\nWriting text\nThe book is created using Markdown - you can get familiarized with the basic syntax on the Markdown website. The getting started quick items are:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n\nYou simply write text as you are used to. To make something *italic*, **bold**, or ***bold and italic***.\n\n&gt; this is how you add quotes\n\n- or lists\n- that can go on \n- and on\nIf you want to add code, use references, create links, or footnotes - it is all possible. We will expand examples here based on your needs, so if you need help, let us know by reporting an issue!\n\nAdding relative links\nOften, you will want to link to other pages or sections in the VU CS Department RDM and Open Science Handbook. Instead of going to the website, and pasting the link from there (for example, https://ubvu.github.io/open-handbook/contributing.html), you can add what is called “relative links.”\nRelative links require three concepts:\n\nWorking directory: The folder in which the file you are editing is located\n./ = indicates the current folder\n../ = indicates the folder one level up\n\nThis Contributing guide is located in the “root” directory, and there is no upper folder. If we wanted to link to a topic, we would use ./topics/example-topic.qmd. This would create a relative link to the example file.\n\n\n\n\n\n\nNote\n\n\n\nRelative links link to the .qmd files, never to the .html pages. These only exist when the pages are rendered!\n\n\nIf we were editing a topic, and we wanted to link out to a guide, we would need to use ../guides/example-guide.qmd. This because we would be in the topic folder for that file, and need to navigate one level up (../) and then down into the guides folder.\n\nSection links\nWhenever we link to a specific guide or topic, you can also link to a specific section. This helps you point readers to what you want them to read, and helps them find the information they need.\nThe easiest way to find these section links is to navigate to the relevant page, and click on the link icon next to the heading. This will cause your URL to change.\n\n\n\nScreenshot indicating the link icon next to a heading, and the updated URL as a result\n\n\nYou add the #adding-a-guide (as applicable in your case) to the end of your relative link, and you will have created a relative section link! :blush:\nIf the section is on the same, you can drop the relative link altogether and keep only the part after the # (for example, #adding-a-guide).\n\n\n\n\nAdding images\nIn markdown, you can easily add images and alt text at the same time. We require alt text on all images, and if you are contributing an image, you can best describe its value in the text.\nYou add images by using:\n![Alt text](URL)\nIf you want the image to be hosted in the Research Support Handbook, use the following steps:\n\nAdd the image you want to the public/ folder\nMark the exact filename\nUse ../public/&lt;filename&gt; as the URL for the image (for example ../public/image.png)\n\n\n\nMore information about GitHub\nWe use GitHub to create this website automatically, and to manage all the incoming updates. You do not need to know how it works entirely, but we want to help you understand some things so you are not confused.\n\nRepository\nA repository on GitHub is like a folder on your computer. This can be many things, depending on what files it contains.\nWhen we mention a repository here, we mean that we want you to look at a specific folder. The repository for this website for example can be found on GitHub directly. You will always be contributing to a repository, in order to contribute to the handbook.\n\n\nForking\nA repository is owned by one or multiple people on GitHub. If you are not one of them, you can create a copy of the repository (folder) to make your edits in. This act of creating a copy is called “forking.”\nWhen you create a copy, you do not have to worry about accidentally removing or destroying the handbook. Your changes are not reflected in the website until you submit a pull request."
  },
  {
    "objectID": "contributing.html#adding-references",
    "href": "contributing.html#adding-references",
    "title": "Contributing",
    "section": "Adding references",
    "text": "Adding references\nIf you want to include references throughout the handbook, we recommend you do so in the following way.\n\nAdd the BibTex\nYou can find the relevant BibTeX information using a tool like the DOI to BibTeX converter. Counterintuitively, it also works on ISBNs for example.\nAfter you found the BibTeX information, you add it to the references.bib file (preferably all the way at the bottom). Example BibTeX information is:\n@ARTICLE{example-code,\n  title     = \"Example Title\",\n  author    = \"Author, Example\"\n  journal   = \"Example Journal\",\n  year      =  2042,\n  copyright = \"https://creativecommons.org/licenses/by/4.0\",\n  language  = \"en\"\n}\n\n\nAdd the citation\nTo add the citation to a page, you use [@example-code] or @example-code.\n@example-code will result in an in-text citation, like “Author (2042).”\n[@example-code] will result in a regular citation such as “(Author, 2042)”.\nFor more details on citations, see also the Quarto help page on citations."
  },
  {
    "objectID": "contributing.html#rendering-handbook-locally",
    "href": "contributing.html#rendering-handbook-locally",
    "title": "Contributing",
    "section": "Rendering handbook locally",
    "text": "Rendering handbook locally\nSometimes you may want to preview the changes you are making to the handbook. That is possible in most cases, but requires you to install some software. You need to install Quarto and assuming a successful installation, you then need to run the following code in your terminal:\n# Clone the git repository\ngit repo clone https://github.com/ubvu/open-handbook\n# Go into the right folder\ncd open-handbook\n# Render the handbook\nquarto render .\nWe do not guarantee this will work immediately, but should cover most instances. If you are looking to contribute and want to render things locally, try this first, and if you run into any issues, let us know in an issue report. We’re happy to try our best if you share your error messages :blush:"
  },
  {
    "objectID": "RDM-guidelines.html",
    "href": "RDM-guidelines.html",
    "title": "RDM Guidelines",
    "section": "",
    "text": "What is a guide?\n\n\n\nThese guides help you find answers to questions that come up while doing research. They help guide you through various topics at once.\nMissing a guide? You can submit questions you are dealing with using the Contribution portal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat research resources are available for VU researchers related to research data management?\n\n\nA page that summarized all the links.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "downloads.html",
    "href": "downloads.html",
    "title": "Downloads",
    "section": "",
    "text": "Files to download for your research.\n\n\n\nOn this page you can find documents, templates and examples while writing your research proposals and doing your research.\nMissing a document? You can report the missing documents using the Contribution portal.\n\n\nVU’s DMP template (latest version) download\nInfomed Consent Form (template) download"
  },
  {
    "objectID": "topics.html",
    "href": "topics.html",
    "title": "Topics",
    "section": "",
    "text": "What is a topic?\n\n\n\nA topic is a specific subject that can be helpful to know about in your daily research. Each page can be read on its own. These pages are a quick way to learn about specific things.\nMissing a topic? You can submit suggestion using the Contribution portal.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nCitation File Format (CFF)\n\n\n1 min\n\n\n\n\nData Citation\n\n\n1 min\n\n\n\n\nData Collection\n\n\n6 min\n\n\n\n\nData Licensing\n\n\n2 min\n\n\n\n\nData Management Plan (DMP)\n\n\n11 min\n\n\n\n\nData Management Section\n\n\n1 min\n\n\n\n\nData Protection\n\n\n5 min\n\n\n\n\nData Publication\n\n\n2 min\n\n\n\n\nData Storage\n\n\n5 min\n\n\n\n\nData documentation\n\n\n3 min\n\n\n\n\nDataset and software registration in PURE\n\n\n3 min\n\n\n\n\nFAIR Principles\n\n\n4 min\n\n\n\n\nFinding Existing Data\n\n\n5 min\n\n\n\n\nHigh-Performance Computing Facilities\n\n\n21 min\n\n\n\n\nMetadata\n\n\n4 min\n\n\n\n\nPersistent identifier\n\n\n1 min\n\n\n\n\nQualtrics\n\n\n3 min\n\n\n\n\nResearch Data Management (RDM)\n\n\n1 min\n\n\n\n\nSURF Research Cloud\n\n\n3 min\n\n\n\n\nSafe Data Transportation and Transfer\n\n\n4 min\n\n\n\n\nSciStor\n\n\n2 min\n\n\n\n\nSoftware Licensing\n\n\n3 min\n\n\n\n\nStoring vs. Archiving Data\n\n\n5 min\n\n\n\n\nTrainings\n\n\n7 min\n\n\n\n\nYoda\n\n\n3 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "OS-guidelines.html",
    "href": "OS-guidelines.html",
    "title": "Open Science",
    "section": "",
    "text": "What is a guide?\n\n\n\nThese guides help you find answers to questions that come up while doing research. They help guide you through various topics at once.\nMissing a guide? You can submit questions you are dealing with using the Contribution portal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat research resources are available for VU researchers for Open Science?\n\n\nA page that summarized all the links.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "News",
    "section": "",
    "text": "Shuai Wang and colleagues won the Open Science Community Amsterdam Award (OSCA)!\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-05-23welcome.html",
    "href": "blog/2024-05-23welcome.html",
    "title": "Hello world!",
    "section": "",
    "text": "This is the first blog entry on the Research Support Handbook. We will be posting more at a later time, and are looking forward to your contributions as well.\nWe will follow up with more details later."
  },
  {
    "objectID": "blog/2024-09-30hackathon.html",
    "href": "blog/2024-09-30hackathon.html",
    "title": "Third Handbook Hackathon",
    "section": "",
    "text": "In the third hackathon for the Research Support Handbook, held on September 25th, 2024, contributors gathered to make significant progress towards migrating the handbook to the VU domain (rdm.vu.nl). The event centered around sprinting to a finish line by refining existing content and ensuring accessibility standards are met.\nKey activities included:\nOverall, the hackathon was a productive session, and the handbook is now in its final stage, with the team preparing for the official launch on &lt;rdm.vu.nl&gt;. In next hackathons, we may focus on splitting existing topics and adding new ones, as our list of idea topics is growing rapidly."
  },
  {
    "objectID": "blog/2024-09-30hackathon.html#hackathon-issues",
    "href": "blog/2024-09-30hackathon.html#hackathon-issues",
    "title": "Third Handbook Hackathon",
    "section": "Hackathon Issues",
    "text": "Hackathon Issues\nhttps://github.com/ubvu/open-handbook/issues/175 https://github.com/ubvu/open-handbook/issues/178"
  },
  {
    "objectID": "blog/2024-09-30hackathon.html#hackathon-pull-requests",
    "href": "blog/2024-09-30hackathon.html#hackathon-pull-requests",
    "title": "Third Handbook Hackathon",
    "section": "Hackathon Pull Requests",
    "text": "Hackathon Pull Requests\nhttps://github.com/ubvu/open-handbook/pull/118 https://github.com/ubvu/open-handbook/pull/165 https://github.com/ubvu/open-handbook/pull/176 https://github.com/ubvu/open-handbook/pull/177 https://github.com/ubvu/open-handbook/pull/179 https://github.com/ubvu/open-handbook/pull/180 https://github.com/ubvu/open-handbook/pull/181 https://github.com/ubvu/open-handbook/pull/184 https://github.com/ubvu/open-handbook/pull/186 https://github.com/ubvu/open-handbook/pull/188"
  },
  {
    "objectID": "blog/2024-06-27hackathon.html",
    "href": "blog/2024-06-27hackathon.html",
    "title": "First Handbook Hackathon",
    "section": "",
    "text": "On June 27th, 2024, the first hackathon for the Research Support Handbook took place with all the post authors. For this hackathon, we focused on non-GitHub based contributions, to make it as easy as possible to contribute. To make getting started with contributing easier, we created a choose your own adventure game. We document some lessons and clarifications below, in addition to the twelve reported problems and suggested changes.\nThe workshop helped articulate the dynamic relation between topics and pathways. Topics are contained pages around a specific subject; pathways are a collection of topics. This means that pathways include the topics directly and that this content should be up to date at any given time. When topics are changed, pathways are dynamically updated, making sure there are no discrepancies. The only situation where this may not be the case, is when a pathway is still a work in progress and the topics are not yet properly linked.\nPathways will become more efficient to create as we include more topics in the handbook. Given that pathways are primarily collections of topics, this means that there is barely any new content in there, if any at all. As we include more topics (eight at this time), pathways can focus more and more on the structuring of content, and focus less on creating the content itself.\nWith new contributions, contributors surfaced the need to preview the changes to the handbook. We documented two ways to render the handbook for such previews: (1) creating a Pull Request automatically deploys a preview website and (2) running quarto render locally on the code. Option 1 requires no additional software to be installed, but requires some knowledge of GitHub. Option 2 does not require much knowledge of GitHub, but requires the Quarto software to be installed. There was also the note that deploying the handbook using GitHub pages required a change to the URL, which may cause issues when merging the changes back into the main handbook. This highlights that ensuring reliable previews of contributed content is of importance to some contributors to the handbook.\nLastly, the hackathon surfaced many questions and discussions around the collaborative decisions that will need to be made. When does a topic become too long and should it be split up into multiple topics? Can a topic include subtopics? How is the GitHub environment maintained? How much technical expertise is necessary to ensure the content does not go offline? What contributor roles are there and who has which role? How do roles get distributed and can people volunteer for them? This highlights the engagement with the handbook, and we encourage everyone (including ourselves) to generously surface these discussions in issues or in a next hackathon.\nIn summary, the first hackathon is a success! This is the start of the next phase of the handbook journey, moving from design and scaffolding to nurturing and growing the contents. There will be more hackathons, and these will be announced on this blog and on other channels at VU Amsterdam. Until the next one!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Open Handbook is a project started by Research Data Support in early 2024. After planning and design phases, we launched the initial version of the resource at the Research Support Days in May 2024.\nThe Open Handbook centralizes resources that VU researchers need to do their work. The Open Handbook also provides everyone with direct guides to change resources in case anything has become outdated.\nPreviously such resources were spread out across many different pages at VU and were hard to update. The Open Handbook is curated by us all, and reviewed by specialists. This way we can help each other.\n\n\nThe Open Handbook was initiated by Lena Karvovskaya, Jessica Hrudey, Elisa, and Jolien Scholten. The initial infrastructure for the Open Handbook was built by Liberate Science. Guide images are by Bres and Bittner (2024).\nWe want to specifically call out the following folk who contributed outside of GitHub:\n\nDiogenes Cruz de Arcelino\nJochem Lybaart\nJochem Nijs\nRebecca Silva dos Santos\n\n\n\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the all-contributors specification. Contributions of any kind are welcome!\n\n\n\n   Alex-van-der-Jagt\n\n\n   Dimitri-Unger\n\n\n   Elisa-on-GitHub\n\n\n   Jolien-S\n\n\n   Karvovskaya\n\n\n   KirianneG\n\n\n   chartgerink\n\n\n\n\n   jensdebruijn\n\n\n   jhrudey\n\n\n   peer35\n\n\n   vansteph\n\n\n   TMHofstra\n\n\n   ELNijland\n\n\n   gus-mxx\n\n\n\n\n   emilybarabas-vu\n\n\n   timveken\n\n\n   reinout538\n\n\n   MarcelRas-391\n\n\n   tmunker"
  },
  {
    "objectID": "about.html#contributors",
    "href": "about.html#contributors",
    "title": "About",
    "section": "",
    "text": "The Open Handbook was initiated by Lena Karvovskaya, Jessica Hrudey, Elisa, and Jolien Scholten. The initial infrastructure for the Open Handbook was built by Liberate Science. Guide images are by Bres and Bittner (2024).\nWe want to specifically call out the following folk who contributed outside of GitHub:\n\nDiogenes Cruz de Arcelino\nJochem Lybaart\nJochem Nijs\nRebecca Silva dos Santos\n\n\n\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the all-contributors specification. Contributions of any kind are welcome!\n\n\n\n   Alex-van-der-Jagt\n\n\n   Dimitri-Unger\n\n\n   Elisa-on-GitHub\n\n\n   Jolien-S\n\n\n   Karvovskaya\n\n\n   KirianneG\n\n\n   chartgerink\n\n\n\n\n   jensdebruijn\n\n\n   jhrudey\n\n\n   peer35\n\n\n   vansteph\n\n\n   TMHofstra\n\n\n   ELNijland\n\n\n   gus-mxx\n\n\n\n\n   emilybarabas-vu\n\n\n   timveken\n\n\n   reinout538\n\n\n   MarcelRas-391\n\n\n   tmunker"
  },
  {
    "objectID": "news/2024-06-04OSCAR.html",
    "href": "news/2024-06-04OSCAR.html",
    "title": "Shuai Wang and colleagues won the Open Science Community Amsterdam Award (OSCA)!",
    "section": "",
    "text": "Through the OSCAWARDS, open science in Amsterdam is highlighted. All groups and projects with open science related aspects or challenges can be nominated for the awards.\nA project by Shuai Wang, Ronald Siebes, Jacco van Ossenbruggen, Tobias Kuhn, a bachelor student Navroop K Singh, together with colleagues from the Open Science team in the university library won the Open Science Community Amsterdam Awards (OSCA). They received a grant of 400 euros. In the project, they studied the use of FAIR Implementation Profiles for making suggestions while researchers write their Data Management Plans.\nThe paper has been accepted for publication at the MTSR conference. A preprint is available here."
  },
  {
    "objectID": "guides/process-and-analyze.html",
    "href": "guides/process-and-analyze.html",
    "title": "How can you ensure data provenance and accurate data analysis?",
    "section": "",
    "text": "Provenance describes the origin of an object. Data provenance refers to the knowledge of where data originate, where they were collected, by whom, for what reason, and similar aspects that help to understand how the data were originally gathered, processed and altered. In daily use, the term “data provenance” refers to a record trail that accounts for the origin of a piece of data (in a database, document or repository) together with an explanation of how and why it got to the present place (Encyclopedia of Database Systems, pp 608-608). You can also call it the process of keeping records of changes in the data. The need for Data Provenance increases as the reuse of datasets becomes more common in research. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields (Wikipedia).\n\nResearchers regularly use a lab notebook or a journal to document their hypotheses, experiments and initial analysis or interpretation of these experiments. If you manually change data in a dataset, this should also be documented. Sometimes records of changes in data can be kept by adding notes to programmes or scripts that are used.\n\nElectronic Lab Journals or Electronic Lab Notebooks are used to meticulously describe and document the process of analysis. Mostly used used in a laboratory environment,; biolab, chemical lab, etc.\nFor computational analyses, Computational Notebooks like Jupyter notebook are used, where you can describe the analysis steps alongside the computer code in different languages like Python, R, Spark, etc. It is important to document steps and changes in your code by writing comments. This way, others and future you can understand how your code works.\nThe Open Science Framework connects different storage types you already use (SURFdrive, Dataverse, etc) and logs automatically all changes of all the steps you make while you progress. With the fine grained history-log and version control system of OSF, you can see all steps you made. You can store and archive the whole provenance trail for citable reproducibility.\n\nFinally, when a dataset contains personal data, data provenance can help researchers to understand the specifics and the context in which the data were gathered, also to be able to assess whether or not the informed consent given for the first research, is applicable.\nFor every step of your data analysis, good Data Documentation is necessary."
  },
  {
    "objectID": "guides/process-and-analyze.html#data-provenance",
    "href": "guides/process-and-analyze.html#data-provenance",
    "title": "How can you ensure data provenance and accurate data analysis?",
    "section": "",
    "text": "Provenance describes the origin of an object. Data provenance refers to the knowledge of where data originate, where they were collected, by whom, for what reason, and similar aspects that help to understand how the data were originally gathered, processed and altered. In daily use, the term “data provenance” refers to a record trail that accounts for the origin of a piece of data (in a database, document or repository) together with an explanation of how and why it got to the present place (Encyclopedia of Database Systems, pp 608-608). You can also call it the process of keeping records of changes in the data. The need for Data Provenance increases as the reuse of datasets becomes more common in research. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields (Wikipedia).\n\nResearchers regularly use a lab notebook or a journal to document their hypotheses, experiments and initial analysis or interpretation of these experiments. If you manually change data in a dataset, this should also be documented. Sometimes records of changes in data can be kept by adding notes to programmes or scripts that are used.\n\nElectronic Lab Journals or Electronic Lab Notebooks are used to meticulously describe and document the process of analysis. Mostly used used in a laboratory environment,; biolab, chemical lab, etc.\nFor computational analyses, Computational Notebooks like Jupyter notebook are used, where you can describe the analysis steps alongside the computer code in different languages like Python, R, Spark, etc. It is important to document steps and changes in your code by writing comments. This way, others and future you can understand how your code works.\nThe Open Science Framework connects different storage types you already use (SURFdrive, Dataverse, etc) and logs automatically all changes of all the steps you make while you progress. With the fine grained history-log and version control system of OSF, you can see all steps you made. You can store and archive the whole provenance trail for citable reproducibility.\n\nFinally, when a dataset contains personal data, data provenance can help researchers to understand the specifics and the context in which the data were gathered, also to be able to assess whether or not the informed consent given for the first research, is applicable.\nFor every step of your data analysis, good Data Documentation is necessary."
  },
  {
    "objectID": "guides/process-and-analyze.html#data-processing",
    "href": "guides/process-and-analyze.html#data-processing",
    "title": "How can you ensure data provenance and accurate data analysis?",
    "section": "Data processing",
    "text": "Data processing\n\nData cleaning\nThe process of detecting and correcting (or removing) corrupt or inaccurate information or records, is called data cleaning. In essence, it refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting this data (Wikipedia). Depending on the type of analysis that is done, different pieces of software can be used to do this data cleaning. More often than not, the same software can also be used to perform the analysis. Licensed software may sometimes also be installed on personal computers or laptops.\n\nSoftware especially designed to clean re-used data is OpenRefine. It cleans starting and trailing blank spaces in cell field, clusters values based on similarities (e.g. in free text fields: Alphen a/d Rhijn, alfen ad rijn, etc. can be easily clustered), normalise data fields into one standard, etc. See below for several tutorials.\nIn some cases, researchers write their own scripts (in programming languages such as Python, R or SQL) to clean data, in which case the process must be documented. Researchers should include their scripts when they archive the datasets to allow for replication and verification.\nExtra background information:\n\nEMGO Quality Handbook on data cleaning\nMaking sense of data I: a practical guide to exploratory data analysis and data mining / Glenn J. Myatt, Wayne P. Johnson, 2014 (eBook)\nOpen Refine\n\nData Carpentry Open Refine website\nTutorial by the Programming Historian\nIntroduction to Digital Humanities with Open Refine\n\n\nFor every step of your data cleaning, good documentation and clarifying the data provenance is necessary.\n\n\nData transcription\nIt is common in many fields to hold interviews, focus group sessions, or make other observations that were recorded - video or audio. If indeed you have done so, and you need to have the text transcribed, there are several ways to do this. One option is to do this by hand, although this is very time-consuming.\nAnother option is to pay a transcription service to make the transcription or to use specialised software. The VU has drawn up processing agreements with one transcription service, Transcript Online, and one transcription software service, Amberscript.\nYou can find more information on the VU Library page on what these transcription options do, how they work, how much they cost, and how they can be used.\n\n\nAnonymisation/Pseudonymisation\nProcessing of personal data requires you as a researcher to make sure that any personal data collected from a human subject is according to the EU GDPR regulation. Anonymisation and Pseudonymisation are two ways to make personal data less easy to identify, in other words, it allows you to de-identify personal data.\nThere are various online tools that may help facilitate these processes. The VU has therefore recommended Amnesia as one of the tools to assist in the anonysmisation/pseudonymistaion of data.\nVU Amsterdam is preparing a decision guide on anonymisation and pseudonymisation."
  },
  {
    "objectID": "guides/process-and-analyze.html#data-analysis",
    "href": "guides/process-and-analyze.html#data-analysis",
    "title": "How can you ensure data provenance and accurate data analysis?",
    "section": "Data analysis",
    "text": "Data analysis\n\nData Analysis\nAlthough data analysis is an ongoing process throughout the research project, this page focuses on the analysis of the data subsequent to its collection. To ensure that research is empirical and verifiable, it is crucial that researchers keep records (data documentation) of every step made during the data analysis.\nData analysis converts raw/processed data into information that is useful for understanding. Many steps may be required to gain useful information from raw data. The process of processing and analysing data may require computing power not readily available or specific storage and protection options. If multiple parties are involved in the analysis, data sharing may also be necessary.\nData analysis often requires the use of specialised software.The software offered and licensed by the university currently includes: Stata, SPSS, and Atlas.TI. For open software, see below.\nIn some cases researchers write their own scripts to analyse the data. At the VU, most scripts are written in R, Python and SQL.\nIf you want to read up on data analysis you should check out what journal articles and books the VU library has available on the subject:\n\nAll sources: Data analysis\nQuantitative data analysis\nQualitative data analysis\nBig data\nData mining\n\n\n\nOpen Software\nUsing open software increases the Accessiblity, Interoperability and Reusability of your data. For that reason, we recommend that you use open software as much as possible for your data analysis. This could be software, code or scripts that you have written yourself - where possible, please make this software public, so your analysis is reproducible. Examples of open software are R and Python, which can be used instead of proprietary, commercial software such as SPSS and Matlab.\nResearchers often write their software themselves. There are also organisations that specialise in writing research software, such as the eScience Center. The eScience Center offers the software they built for free use online. Their software is tagged with a DOI and stored in Zenodo as well as GitHub.\nIf you use software for analysing personal or otherwise sensitive data, you need a processing agreement with the developer if the software does not run locally. You can contact your 🔒 Privacy Champion if you are not sure if you need one, and for help to set up a processing agreement.\nThere are several ways in which to start using open software:\n\nFor Python: you should install Anaconda and launch the Jupyter Notebook from the Navigator.\nFor R: you should install Anaconda and launch R Studio from the Navigator.\nUse the Software Carpentries to learn the basics of programming in Python and R and version control with Git\nRead the recommendations for FAIR Software.\n\nThe VU has several research groups that offer their code online. You can find them here:\n\nThe Systems Bioinformatics research group, on GitHub\nThe Computational Lexicology & Terminology Lab, on GitHub\nThe course Python for Text Analysis, on GitHub\nVU RDM Tech IT group, on GitHub\nA list of RDM tools, on GitHub\n\n\n\nCompute services\n\nIf your pc or laptop takes too much time performing your analysis, it is time to scale up to a higher level. There are several options for employees and students who require more computing power than their own desktop or laptop can provide.\nSeveral options are detailed below. 🔒 Contact IT for Research for advice on which solution could best fit your workflow\n\nHigh Performance Computing (HPC)\n\n\n\nA set of servers in an undescript room\n\n\nRoughly speaking, you should try to get access to the HPC when you need to stick a post-it on your laptop or PC that says: “do not touch, analysis ongoing”. Or when you want to run analyses parallel to each other, because they take too long. It is important to consider such a situation at the very beginning of your research or when writing your Data Management Plan: is it conceivable that your dataset will become so large or your analysis so complicated that you will need HPC? Please note that this can occur for any discipline and any sort of data, qualitative and quantitative. If you may need HPC, you also need to reconsider your analysis methods. Programmes like SPSS and Excel do not run well on a HPC, and you would need to (learn to) write scripts in R or Python. If you want to know if using HPC may be necessary or useful for your project, you can contact IT for Research to ask for more information (select the “Onderzoek service domain”).\n\n\nSURF Snellius Compute Cluster\nSnellius is the Dutch National supercomputer hosted at SURF. The system facilitates scientific research carried out in many Universities, independent research institutes, governmental organizations, and private companies in the Netherlands.\nIt’s a service comprising a wide range of resources, compilers and, such as R statistics and MATLAB, and libraries. SURF continually adjusts the service to the needs of the user community. For example, Snellius Compute Cluster includes accelerators (very fast processors),high memory nodes and GPU nodes. \nYou can find more information on the SURF Snellius Wiki.\n\n\nBAZIS Compute Cluster\nIT for Research (ITvO) offers access to your own Linux computational cluster at the VU. BAZIS is a managed service for high performance computing (HPC). Research groups can add their own compute server hardware to BAZIS, ITvO will take care of configuring and maintaining the software stack on your servers.\nBAZIS also has several “community” nodes for use by all VU researchers, sponsored by the VU HPC Council.\nBAZIS is connected to SciStor  providing easy access to your research data and analysis result.\n\n\nVU JupyterHub for education\nIf you are not yet ready to take the leap to cluster computing and work with Python consider JupyterHub. VU IT has built a Jupyter Notebook environment meant mainly for Education purposes, but accessible for researchers as well on https://hub.compute.vu.nl/\n\n\n(Virtual) servers\nThere are also several options to run applications in a server environment. This is useful if for example you use software that does not work on HPC, you want to run a web service, you want to create a research environment for your project. There are several options available for researchers.\n\nSciCloud\nIT for Research (ITvO) offers a virtual server environment where you can run your own server (Linux or Windows). ITvO installs the basic operating system and you are free to install needed software. Web services can be made accessible on the internet. You can find more information and a request form on the 🔒 VU service portal\n\n\nSURF Research Cloud\nSURF also offers a virtual server environment. Several environments with pre-installed software can easily be installed from a catalog. Find more information on the SURF wiki.\n\n\nDedicated hardware\nSometimes your workload needs dedicated hardware. ITvO offers the option to host your own server hardware in our on-campus data center. Please 🔒 Contact IT for Research to discuss possibilities."
  },
  {
    "objectID": "guides/publish-and-share.html",
    "href": "guides/publish-and-share.html",
    "title": "What data assets should you publish and what data assets should you archive?",
    "section": "",
    "text": "In the Data Management Plan the researcher describes if the data will be stored for the mid or the long term.\n\n\n\nA monochrome picture of two women operating the ENIAC, an early computer\n\n\n\n\nAccording to the VU RDM Policy, all publication-related data should be archived for at least ten years for verification and replication of research. For this purpose, Vrije Universiteit Amsterdam offers researchers two options to archive their data in one of the organisational repositories (DataverseNL and Yoda). Other archival options may be used depending on the discipline as described in faculty data management policy documents.\n\n\n\nData relevant for future research should be archived for the long term. A dataset is relevant for future research when at least one of the following general criteria applies:\n1. The data have a scientific or historical value\n2. The data are unique\n3. Others may want to reuse the data\n4. The data cannot be reproduced\nResearchers should bear in mind that repositories can charge for archiving data. These costs can vary according to the data volume and the archive used. It is important that you consider in advance how you will budget for these costs. Whatever archiving option is used, proper descriptions of the dataset(s) and adding metadata are important.\n\n\n\n\nVU Amsterdam requests that researchers archive the data used in a publication in a repository for at least ten years after the release of the publication (see also VU Policies & Regulations). There are a lot of digital archives and many more keep appearing.\nThe right archival option depends on the nature of the data and the field of science as described in faculty or departmental data management policy documents. The university offers 2 different general repositories for data archiving.\n\nThe RDM Support Desk and faculty data stewards can help researchers with the selection of a repository that meets all the relevant criteria of privacy (sensitivity), dataset size, etc.\nDataverseNL - an online platform for the publication of citable research data in a semi-open environment. DataverseNL allows users to link publications to datasets directly, and to share the data through online archives such as DANS.\n\nSpecifications:\n\nFor publishing research data on the internet\nThe researcher publishing the data decides whether access to the data is public or restricted\nNot suitable for privacy or otherwise sensitive information\nEnables researchers to publish open data according to grant providers’ regulations\nGenerates a link (persistent identifier), e.g. for data citations in publications\nRetention period is at least 10 years\n\nYoda - besides active storage, Yoda also has an archive function: the vault. You can use the vault in two ways:\n\nFor archiving data securely; data are only available for verification purposes and may be access only by special request. A special procedure will be followed if anyone requests access to the data in order to verify them.\nFor publishing data; data can be available for anyone, or on request. The data will get a persistent identifier as well.\n\nBefore sending data to the vault, you will need to add metadata. A data steward, metadata specialist or functional manager can help you with the metadata and the entire process of sending data to the vault. Please get in touch with the RDM Support Desk to find this help.\n\n\n\nThere is a difference between archiving and publishing data. When we talk about archiving data, we mean that data are deposited securely, in a fixed state, in a location that is not accessible to the public or even a colleague at the VU. Archiving often happens for data that are confidential - for privacy or other reasons - and that should not be accessible publicly. Archiving is usually done for verification purposes, or, in case of medical research, to comply with the preservation requirements within the WMO.\nPublishing refers to depositing data in a public repository that allows others to view, access and download your data. You can set certain restrictions, but as a rule of thumb, publishing should only happen for data that are not confidential at all. That includes data that have been anonymised, or were not personal to begin with, and data that were never otherwise confidential. If you cannot publish any data at all, we do usually recommend trying to publish some documentation, such as data collection protocols, scripts, codebooks, etc. In this way, others can see how the research was carried out, even if they cannot simply access the data.\nUse the image below to remind yourself of the difference between archiving and publishing, and read the data publication page to find out what aspects are important when you decide to publish your data.\n\n\n\nA sketch diagram by Scriberia illustrating the archive or publish data journey\n\n\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\n\nBesides the repositories offered by the VU, there are many others. Unless you are working with personal or otherwise confidential data and you need to archive them in Yoda, you are, in principle, free to choose a different repository from the ones hosted by the VU.\nThere can be various reasons to decide to use a different repository, including funder requirements, preferences of research partners, and a repository being a common choice in your field. For example, Dutch archaeologists mostly use DANS Data Stations to deposit and publish their data. Using a repository that is a common choice in your field will make your data more findable for your colleagues and increase the visibility of your work as a researcher. Some of the data repositories most commonly used in the Netherlands include:\n\nDANS Data Stations: a domain-agnostic research data repository hosted by the Data Archiving and Networked Services, an institute of NWO and KNAW. DANS also develops policies, services and new infrastructures for research data and provides researchers with advice on how to preserve their data. VU researchers are also welcome to deposit their data at DANS-EASY;\n4TU.ResearchData: a repository for science, engineering and design data hosted by the 4TU Federation. This is a consortium of the four Dutch technical universities: TU Delft, TU Eindhoven, University of Twente and Wageningen University and Research. VU researchers are also welcome to deposit their data at 4TU;\nZenodo: a domain-agnostic research data repository hosted by CERN in Switzerland and funded by the European Commission. Zenodo does not only host data, but also presentations, conference procedures and policy documents. It is also possible to archive GitHub repositories directly into Zenodo, by which you contribute to Open Science by making a snapshot of your code available in its current form and for the long term;\nOSF (Open Science Framework): a data management and research dissemination platform. The VU is an institutional member of the OSF, which means that you can sign up (and in) using your VU account by clicking on the Institution Button on the sign in/up pages. You can use the OSF to create registrations and preregistrations for your research, to publish preprints, and publish and share data and documentation. You can also link other repositories such as DataverseNL to your OSF project. The same goes for GitHub and storage options such as Research Drive and Surfdrive. Do be careful about what you connect! A full guide for VU OSF users, including instructions about connecting external storage can be found here.\n\nYou can also find repositories via the Registry of Research Data Repositories. When you are choosing a repository, it is important to check that it provides all the services you need. A good way to find out is to check if a repository as a Core Trust Seal, which is a form of certification for quality repositories. But if a repository does not have the Core Trust Seal, it does not necessarily mean it is not a good repository. As a minimum, you should check that:\n\nThe repository provides a persistent identifier, such as a DOI;\nThe repository enables you to add rich metadata to your dataset and ideally follows an internationally recognised metadata standard, such as Dublin Core or DataCite;\nThe repository offers functionality to publish data with an embargo or under restrictions, if you need that;\nThe repository allows you to add a licence to the dataset;\nThe repository is funded sustainably for at least the next 50 years;\nAnd, in some cases, that the repository’s servers are located in the EU.\n\nMore recommendations for choosing a data repository can be found on CESSDA.\nIf you would like advice about what would be a good place for you to archive your research data, you can always reach out to the RDM Support Desk."
  },
  {
    "objectID": "guides/publish-and-share.html#selecting-an-archive",
    "href": "guides/publish-and-share.html#selecting-an-archive",
    "title": "What data assets should you publish and what data assets should you archive?",
    "section": "",
    "text": "In the Data Management Plan the researcher describes if the data will be stored for the mid or the long term.\n\n\n\nA monochrome picture of two women operating the ENIAC, an early computer\n\n\n\n\nAccording to the VU RDM Policy, all publication-related data should be archived for at least ten years for verification and replication of research. For this purpose, Vrije Universiteit Amsterdam offers researchers two options to archive their data in one of the organisational repositories (DataverseNL and Yoda). Other archival options may be used depending on the discipline as described in faculty data management policy documents.\n\n\n\nData relevant for future research should be archived for the long term. A dataset is relevant for future research when at least one of the following general criteria applies:\n1. The data have a scientific or historical value\n2. The data are unique\n3. Others may want to reuse the data\n4. The data cannot be reproduced\nResearchers should bear in mind that repositories can charge for archiving data. These costs can vary according to the data volume and the archive used. It is important that you consider in advance how you will budget for these costs. Whatever archiving option is used, proper descriptions of the dataset(s) and adding metadata are important.\n\n\n\n\nVU Amsterdam requests that researchers archive the data used in a publication in a repository for at least ten years after the release of the publication (see also VU Policies & Regulations). There are a lot of digital archives and many more keep appearing.\nThe right archival option depends on the nature of the data and the field of science as described in faculty or departmental data management policy documents. The university offers 2 different general repositories for data archiving.\n\nThe RDM Support Desk and faculty data stewards can help researchers with the selection of a repository that meets all the relevant criteria of privacy (sensitivity), dataset size, etc.\nDataverseNL - an online platform for the publication of citable research data in a semi-open environment. DataverseNL allows users to link publications to datasets directly, and to share the data through online archives such as DANS.\n\nSpecifications:\n\nFor publishing research data on the internet\nThe researcher publishing the data decides whether access to the data is public or restricted\nNot suitable for privacy or otherwise sensitive information\nEnables researchers to publish open data according to grant providers’ regulations\nGenerates a link (persistent identifier), e.g. for data citations in publications\nRetention period is at least 10 years\n\nYoda - besides active storage, Yoda also has an archive function: the vault. You can use the vault in two ways:\n\nFor archiving data securely; data are only available for verification purposes and may be access only by special request. A special procedure will be followed if anyone requests access to the data in order to verify them.\nFor publishing data; data can be available for anyone, or on request. The data will get a persistent identifier as well.\n\nBefore sending data to the vault, you will need to add metadata. A data steward, metadata specialist or functional manager can help you with the metadata and the entire process of sending data to the vault. Please get in touch with the RDM Support Desk to find this help.\n\n\n\nThere is a difference between archiving and publishing data. When we talk about archiving data, we mean that data are deposited securely, in a fixed state, in a location that is not accessible to the public or even a colleague at the VU. Archiving often happens for data that are confidential - for privacy or other reasons - and that should not be accessible publicly. Archiving is usually done for verification purposes, or, in case of medical research, to comply with the preservation requirements within the WMO.\nPublishing refers to depositing data in a public repository that allows others to view, access and download your data. You can set certain restrictions, but as a rule of thumb, publishing should only happen for data that are not confidential at all. That includes data that have been anonymised, or were not personal to begin with, and data that were never otherwise confidential. If you cannot publish any data at all, we do usually recommend trying to publish some documentation, such as data collection protocols, scripts, codebooks, etc. In this way, others can see how the research was carried out, even if they cannot simply access the data.\nUse the image below to remind yourself of the difference between archiving and publishing, and read the data publication page to find out what aspects are important when you decide to publish your data.\n\n\n\nA sketch diagram by Scriberia illustrating the archive or publish data journey\n\n\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\n\n\nBesides the repositories offered by the VU, there are many others. Unless you are working with personal or otherwise confidential data and you need to archive them in Yoda, you are, in principle, free to choose a different repository from the ones hosted by the VU.\nThere can be various reasons to decide to use a different repository, including funder requirements, preferences of research partners, and a repository being a common choice in your field. For example, Dutch archaeologists mostly use DANS Data Stations to deposit and publish their data. Using a repository that is a common choice in your field will make your data more findable for your colleagues and increase the visibility of your work as a researcher. Some of the data repositories most commonly used in the Netherlands include:\n\nDANS Data Stations: a domain-agnostic research data repository hosted by the Data Archiving and Networked Services, an institute of NWO and KNAW. DANS also develops policies, services and new infrastructures for research data and provides researchers with advice on how to preserve their data. VU researchers are also welcome to deposit their data at DANS-EASY;\n4TU.ResearchData: a repository for science, engineering and design data hosted by the 4TU Federation. This is a consortium of the four Dutch technical universities: TU Delft, TU Eindhoven, University of Twente and Wageningen University and Research. VU researchers are also welcome to deposit their data at 4TU;\nZenodo: a domain-agnostic research data repository hosted by CERN in Switzerland and funded by the European Commission. Zenodo does not only host data, but also presentations, conference procedures and policy documents. It is also possible to archive GitHub repositories directly into Zenodo, by which you contribute to Open Science by making a snapshot of your code available in its current form and for the long term;\nOSF (Open Science Framework): a data management and research dissemination platform. The VU is an institutional member of the OSF, which means that you can sign up (and in) using your VU account by clicking on the Institution Button on the sign in/up pages. You can use the OSF to create registrations and preregistrations for your research, to publish preprints, and publish and share data and documentation. You can also link other repositories such as DataverseNL to your OSF project. The same goes for GitHub and storage options such as Research Drive and Surfdrive. Do be careful about what you connect! A full guide for VU OSF users, including instructions about connecting external storage can be found here.\n\nYou can also find repositories via the Registry of Research Data Repositories. When you are choosing a repository, it is important to check that it provides all the services you need. A good way to find out is to check if a repository as a Core Trust Seal, which is a form of certification for quality repositories. But if a repository does not have the Core Trust Seal, it does not necessarily mean it is not a good repository. As a minimum, you should check that:\n\nThe repository provides a persistent identifier, such as a DOI;\nThe repository enables you to add rich metadata to your dataset and ideally follows an internationally recognised metadata standard, such as Dublin Core or DataCite;\nThe repository offers functionality to publish data with an embargo or under restrictions, if you need that;\nThe repository allows you to add a licence to the dataset;\nThe repository is funded sustainably for at least the next 50 years;\nAnd, in some cases, that the repository’s servers are located in the EU.\n\nMore recommendations for choosing a data repository can be found on CESSDA.\nIf you would like advice about what would be a good place for you to archive your research data, you can always reach out to the RDM Support Desk."
  },
  {
    "objectID": "guides/publish-and-share.html#data-publication",
    "href": "guides/publish-and-share.html#data-publication",
    "title": "What data assets should you publish and what data assets should you archive?",
    "section": "Data Publication",
    "text": "Data Publication\n\nOpen Access and Open Science\nOpen Access publishing means that you make your publication freely accessible online to everyone without restrictions. VU believes that government-funded research should be available free of charge to as many people as possible.\nOpen Access publishing is one component of Open Science. The European Commission has defined open science as follows: “Open Science represents a new approach to the scientific process based on cooperative work and new ways of diffusing knowledge by using digital technologies and new collaborative tools. The idea captures a systemic change to the way science and research have been carried out for the last fifty years: shifting from the standard practices of publishing research results in scientific publications towards sharing and using all available knowledge at an earlier stage in the research process” (Definition taken from Nationaal Programma Open Science). This includes making openly available research data, methods and documentation where possible. As such, RDM and the practices outlined in the VU CS Department RDM and Open Science Handbook are a precondition of Open Science. You can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science and join the Open Science Community Amsterdam, the community of VU employees interested in Open Science (joint with the University of Amsterdam).\n\n\nPublishing your data in a data journal\nInstead of archiving research data in a data repository, you may choose to publish an article about your data collection. This is not necessarily common for all disciplines. Some examples of data journals where you can publish your data and dataset, are:\n\nScientific Data - Nature\nGeoscience Data Journal\nGigascience\nJournal of Physical and Chemical Reference Data\nEarth System Science Data\nJournal of Open Archaeology Data\nJournal of Open Psychology Data\n\n\nPersistent Identifier\nA persistent identifier (PID) is a durable reference to a digital dataset document, website or other object. It is a kind of ISBN for digital files. By using a persistent identifier, you make sure that your dataset will be findable well into the future. A DOI or Handle are the commonly used PIDs. The data archiving options at the VU commonly offer DOIs.\nMost data archives or repositories offer a persistent identifier and generate this automatically when research data are archived. For example, this is the case for DataverseNL at the VU. In Yoda at the VU, assigning a PID is possible, but does not happen automatically. Please get in touch with the RDM Support Desk if you have questions about assigning a PID when you archive data in Yoda.\n\n\nLicensing the data\nA data licence agreement is a legal instrument that lets others know what they can and cannot do with your research data (and any documentation. scripts and metadata that are published with the data). It is important to consider what kind of limitations are relevant. An important component can be a guideline on how people should cite the dataset. Other components could be:\n\nCan people make copies or even distribute copies\nWho should be contacted if you need access to re-use data\nEtc.\n\n\n\n\nAn image of open data, made up of public domain icons\n\n\nIn principle, Dataverse allows you to choose your terms of use. Some data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition). If you need help with drawing up license agreements, you can contact the IXA office.\nAdditional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE license selection wizard, which Pawel Kamocki (et al.) released under an open source license.1\n\nPublishing research software under an appropriate license is crucial for its accessibility, usability, and further integration into research. Choosing a license usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked.\nA software license states how other people may re-use your code and under which circumstances. For research software, it is recommended (and often required by funders) that licenses are as permissible as possible.\nThere are many licenses out there; below we list some very frequently used licenses in research software. However, if none of these licenses fit your case, there are several tools that can help you to choose a suitable software license. If you need guidance in choosing a licence for your software, get in touch with the RDM Support Desk.\n\n\nMIT License\nThe MIT License is a popular choice, due to its readability and permissiveness. It allows users to reuse the software for any purpose, including using, copying, modifying, and distributing it, provided they include the original copyright notice and license text.\nHowever, its permissiveness means that derivative works can be closed-source and do not need to mention that they use your code, which might not align with all scientific openness goals or general.\n\n\nGNU GPLv3\nThe GNU General Public License (GPLv3) is another option, designed to ensure that the software and any derivatives remain open-source.\nThis encourages collaborative improvement of software. Any software that includes GPL-licensed code must also be open-source under the GPLpotentially deterring commercial use or integration with proprietary software. In conclusion, when you want your code to be used by others, but only the code that uses your code is also open source, this is the way to go.\n\n\nApache License 2.0\nThe Apache License 2.0 allows for modification and distribution of the software and its derivative works, with the requirement that changes to the original code are documented.\nIt is a more complex license than the MIT License and can be incompatible with GPL-licensed software. The specifics of this go beyond the scope of the handbook.\n\n\nAdding a license to GitHub\nOn GitHub you add a license on creating a new repository, by selecting the license from the drop-down menu. If your repository already exists, add a new file called “LICENSE” using the “+”-button on top of the repository (see below).\n\n\n\nLocation of file creation button\n\n\nOne the next page, start type LICENSE as the file name, and a button to “Choose a licence template” should automatically pop up. Follow the steps provided by GitHub to finish adding the license to the repository.\nYou should now see your license shown on the main page of your repository.\n\n\nFurther considerations\n\nIf you are reusing software or libraries written by someone else, you must stick to the clauses of the licence given to the original software/library;\nWhen choosing a licence, do not just think about what others may do with the software, but also what you might want to do with the software in the future."
  },
  {
    "objectID": "guides/publish-and-share.html#dataset-registration",
    "href": "guides/publish-and-share.html#dataset-registration",
    "title": "What data assets should you publish and what data assets should you archive?",
    "section": "Dataset Registration",
    "text": "Dataset Registration\n\nRegistration & Findability\nWhen you have finished finalizing a dataset and are ready to archive it, there are many options available. Depending on the research and choices made earlier the archive provides the option to fill in descriptive fields for a dataset. The descriptions in the archives often are automatically created using metadata standards like DataCite or Dublin Core, or some other type of standard. See also the item Metadata in this LibGuide.\nWhen registering a dataset in an archive it is important to use unique identifiers to allow for increased findability and easy attribution & citation. Examples of this are:\n\nPersonal names: try to consistently use the same notation for all researchers and assistants that are included as authors\nORCID: using a unique identifier like this for all authors is recommended. More information is available here.\nInstitutonal names: avoid using different versions (or language versions) of participating Institutes/organizations and departments. In the case of the VU the official written name is: Vrije Universiteit Amsterdam. For each organization or Institute that is included: try to make sure that the official name is used each time.\n\nSome archives also allow you to preregister your project/dataset. Examples are:\n\nOpen Science Framework (OSF) Registration\nZenodo & registration\n\n\n\nRegister your Dataset in PURE\nJust like your publications, data that you have collected for your research constitute research output, too. Therefore you are required to record your datasets in PURE. Your datasets can be of interest to others, which can in turn lead to new collaboration opportunities. Datasets recorded in PURE also appear in reports that are used for research evaluations. Even if access to your dataset is closed, you are required to register your dataset in PURE. It is a record of the research, data collection and analysis that you have carried out.\n\nBenefits of recording your dataset in PURE\n\nIt increases the visibility and findability of your datasets\nIt contributes to re-use and transparency\nIt boosts your collaboration opportunities\nIt counts towards research evaluations and assessments\n\n\n\nHow to register your dataset in PURE?\n\n\n\nScreenshot: adding a dataset to your PURE profile\n\n\n\nLog into the VU Research Portal (PURE) using your VU or VUmc credentials\nClick on the “+” (plus) icon next to selecting “Datasets” in the overview\nYou can fill in the form using this manual (NL)/manual (EN), and read more about the various metadata in use (generic and subject specific)\nClick on “Save” to store the registration"
  },
  {
    "objectID": "guides/publish-and-share.html#footnotes",
    "href": "guides/publish-and-share.html#footnotes",
    "title": "What data assets should you publish and what data assets should you archive?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the source code, see https://github.com/ufal/public-license-selector/↩︎"
  },
  {
    "objectID": "guides/collect-and-store.html",
    "href": "guides/collect-and-store.html",
    "title": "How can you ensure data protection and security during collection, storage, and transfer?",
    "section": "",
    "text": "Data collection may consist of the re-use of existing data and/or the generation of new data.\nFor data to be considered valid and reliable, data collection should occur consistently and systematically throughout the course of the research project. Within disciplines, there are established methodologies, procedures and techniques that help researchers ensure high quality of collected data. In general, important aspects of data collection include:\n\nStandardisation: codebooks & protocols\nStructure / organisation of the data\nData quality assurance methods\nDocumentation & metadata\nStorage & protection\n\nSystematic data collection is essential for ensuring the reproducibility of research. When data is collected in a consistent and organized manner, it improves the quality and reliability of the research, making the data easier to share and reproduce by others. High-quality data also contributes to making data FAIR (Findable, Accessible, Interoperable, and Reusable), as well-organized and well-documented data is more likely to be reused effectively. The principles of making data FAIR are discussed in detail under the topic FAIR Principles.\n\nData Collection Tools\nThe tools being used in research to collect data are immensely diverse. For that reason, we will not provide an exhaustive overview here. What is important for data collection tools in relation to RDM is where such tools store the data that you collect and in which format. The storage location is particularly important when you are working with personal data. For example, the privacy legislation in the United States is very different from the European General Data Protection Regulation (GDPR). Hence, personal data collected in a Dutch research institute may not be stored on American servers. It is important to keep that in mind when you are contemplating which tool to use for your data collection.\nIf you are collecting personal data and you decide to use a tool for which no contract exists between VU Amsterdam and the provider of the software or tool, a service agreement and a processing agreement must be drawn up. Contact the 🔒 privacy champion of your faculty for more information and a model processing agreement.\n\nQuestionnaire tools\nThe Faculty of Behavioural and Movement Sciences has developed a document with tips for safe use of the questionnaire tools Qualtrics and Survalyzer. The document was made for FGB researchers specifically but can also be helpful for others. Consult this document if you need a questionnaire tool to collect your data.\n\n\n\nData Collection in Collaboration\nSome research projects involve the participation of multiple organisations or institutes and may include even cross-border co-operation. When data is collected by several organisations, a Data Management Plan should provide information on who is responsible for which part of the data collection and storage. It should also provide information on how specific data collections are related to which part(s) of the research goal(s). Describing this precisely will help you to determine if a consortium agreement or joint controller agreement is necessary. You see a general example of such a specification in the table below:\n\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nResponsible organization for collection\nData origin\nData purpose\n\n\n\n\nRaw data\nCommunity level surveys\nVrije Universiteit Amsterdam\nAmsterdam, The Hague, Rotterdam\nIdentifying perceived problems, System responsiveness\n\n\nRaw data\nTrials & Focus Group Interviews\nLondon School of Hygiene and Tropical Medicine (LSHTM)\nGermany, Switzerland\nTrials to evaluate programs on . . ., Focus Group interviews to identify barriers to . . .\n\n\nRaw data\nPollution measurements using fish\nOceanographic Institute of Sweden\nCoastal waters, Northeast Spain\nEstablish pollution levels of plastic\n\n\n\n\n\nData Collection Protocols\nRegardless of the field of study or preference for defining data (quantitative, qualitative), accurate data collection is essential to maintaining the integrity (structure) of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors.\nThere are two approaches for reducing and/or detecting errors in data which can help to preserve the integrity of your data and ensure scientific validity. These are:\n\nQuality assurance - activities that take place before data collection begins\nQuality control - activities that take place during and after data collection\n\nQuality assurance precedes data collection and its main focus is ‘prevention’ (i.e., forestalling problems with data collection). Prevention is the most cost-effective activity to ensure the integrity of data collection. This proactive measure is best demonstrated by the standardization of protocol developed in a comprehensive and detailed procedures manual for data collection.\nWhile quality control activities (detection/monitoring and action) occur during and after data collection, the details should be carefully documented in the procedures manual. A clearly defined communication structure is a necessary pre-condition for monitoring and tracking down errors. Quality control also identifies the required responses, or ‘actions’ necessary to correct faulty data collection practices and also minimise future occurrences.\nSome sources for protocols:\n\nHANDS Handbook for Adequate Natural Data Stewardship by the Federation of Dutch University Medical Centers (UMCs)\nProtocols.io - an open access repository of protocols\nProtocols Online - website with protocols available on the internet, sorted by discipline.\nSpringer Protocols - free and subscribed protocols collected by Springer.\n\n\n\nStorage During Research\nVU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by the VU;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup.\n\n\nStandard services offered by the VU\nVU IT offers several services for employees to store their files. Examples are:\n\n🔒 OneDrive: personal storage for all VU employees and part of the Microsoft 365 platform. OneDrive allows you to store files locally and in the Microsoft cloud, and share folders and documents with colleagues. Since this is personal storage, tied to someone’s personal VU account, we don’t usually recommend storing research data in OneDrive: if the account holder leaves the VU, the account and all the data on it, disappear.\n🔒 Teams. Faculties, divisions and departments have their own Team - part of the Microsoft 365 platform - where they store shared documents and where they can interact and chat. Projects may also request a project team. But note that Teams is not always the best location to store your research data and has several limitations, especially when it comes to working with non-Microsoft file formats, large volumes of data, interacting with data, and collaborating with partners outside of the VU. Contact the RDM Support Desk to find out more about the suitability of Teams for your project.\n🔒 Surfdrive: is a personal cloud storage service for the Dutch education and research community, offering staff, researchers and students an easy way to store, synchronise and share files in the secure and reliable SURF community cloud. All users receive storage space of up to 500 GB. Surfdrive is automatically offered to all VU employees. Since Surfdrive is personal storage, like OneDrive, we do not usually recommend it for research data\n\n\n\nResearch data-specific storage options\nThe options above are standard data storage options at the VU to which all employees have access. But the VU also offers storage specifically for research data. Some of them are hosted locally at the VU, while others are SURF cloud services. When selecting a cloud-based service it is important to remember to check where the data will be hosted. If the research project involves sensitive data it may be necessary to choose cloud-based options that guarantee that the data will stay in the EEA or on servers based in the EEA.\n\n🔒 SciStor (short for ‘Storage for Scientists’): This is storage hosted by IT for Research (ITvO) and allows for inexpensive storage of large volumes of data. There are various levels of security possible and various ways to get access to the files. SciStor is only intended for ongoing research, not for archiving.\nYoda (short for Your Data) is a cloud storage at SURF and is suitable for storing large-scale and sensitive datasets. Yoda also supports collaborating on projects in and outside the VU and adding contextual information (metadata) about your dataset as you go. Yoda is usually the best choice if your research data are very sensitive.\n🔒 Research Drive is a cloud storage at Surf for research projects and is suitable for collaboration in and outside the VU, for storing sensitive data and large-scale research projects. You can also encrypt data in Research Drive using several tools. You are able to request storage space in Research Drive via a 🔒 web form in the selfservice portal (VU employees only). Research Drive is the best choice if you need to manage access rights on a folder level. More general information about Research Drive can be found here, and its wiki pages, including tutorials, are here.\n\nThere are differences between Research Drive and Yoda and each one may support certain projects better than others. The storage finder can help you to get an idea of what would be the best choice for your project, but get in touch with the RDM Support Desk for more details.\n\n\nSending research data to partners\nSome projects may require data sharing with partners. Although Research Drive and Yoda support sharing data all through the project, it may also be the case that some data only need to be sent to a partner once. There are some secure options to send data to research partners:\n\n🔒 Surf Filesender: cloud service that allows you to send files of 1 Terabyte to other researchers and encrypted files of up to 250 GB.\n🔒 Zivver: All employees of Vrije Universiteit Amsterdam can use Zivver, the encryption programme that allows you to send email or data (sensitive or otherwise) securely by email. Attachments will also be encrypted and can be several Terabytes in size (max = 5 TB). Specific information on how to get and use Zivver are available on the selfservice portal. General explanations on how to use it are available at the Zivver website.\n\n\n\nWhat is Data Protection?\nProtection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‘security’ is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‘when are the measures that you take secure enough?’. In order to answer this, please be aware that there are three entities that have an opinion about what is ‘secure enough’, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‘GDPR and Privacy’ under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of the VU. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents’ contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management.\n\n\n\nData Protection\nThere can be many reasons why the data of a project needs to be kept protected:\n\nSensitivity of the data collected\nProtection of the research data from competition\nCommercial reasons / Intellectual property\nEtc.\n\n\n\n\nAn image of a lock composed of code in a matrix green style.\n\n\nThere are also many levels of security that may be implemented, depending on the needs. Sometimes it will be enough to use a password-protected cloud-based server. In extreme cases encryption may be needed and also when data is transmitted between researchers or organisations. You should contact the RDM Support Desk to discuss available options, who may connect you to legal experts where sensitive data is concerned. Check the Data Storage topic for links to find out more on campus solutions and cloud-based options.\n\nSee also the Safe Data Transfer topic for more information on how to transport and transfer data.\n\nIt is important to protect your data during the entire data life cycle. To find out whether your data are secure during all stages of your research, think about your data flow: where do your data originate and where do they go to? If data need to be transported from one physical place to the other, or need to be transferred from one device to another, these actions should happen in a secure way.\n\nTransferring digital data\n\nOnline connection on campus\nIf data collection takes place through a certain measurement device (e.g. MRI scanner, EEG scanner, eye tracker), the data need to be transferred from the measurement device to the storage location that you will use during your research project. Make sure that this transfer takes place in a secure way and also make a plan for the data on the measurement device; find out whether they need to be destroyed or can remain there.\n\n\nOnline connection outside campus (with and without VUnetID)\nIf you are doing fieldwork outside the campus and you have reliable and secure internet access, it is a good idea to upload the data to a storage location that is regularly backed up and secure, in order to prevent data loss. If you have a VUnetID, you can for example use:\n\nResearch Drive to securely and easily store and share research data.\nSURFfilesender to send you data to a colleague or consortium partner, who can store your data in an appropriate place\n\nYou can find more information about each of these storage options in the Data Storage topic.\nIf you need to receive data from colleagues in your project who don’t have access to these tools (e.g. because they are students, don’t work for a Dutch educational institution, or have no VUnetID), Research Drive, Yoda, SURFfilesender and secure emailing with Zivver can also be used:\n\nResearch Drive: This cloud storage service provided by SURF enables researchers from VU and external researchers to manage files and folders in a shared storage location.\nSURFfilesender: as a SURFfilesender user, you can send a voucher to someone who doesn’t have access to this tool. This person can use this voucher to send documents to you. These files can be encrypted.\nYoda: This cloud storage service provided by SURF enables researchers from VU and external researchers to manage files and folders in a shared storage location.\n🔒 Zivver is an email plugin with which you can encrypt emails and attachments.\n\n\n\nOffline data outside campus\nIf you are doing fieldwork in an area with limited internet access, you might use a portable device to initially store your data during the phase of data collection, such as a USB drive or an external hard drive. These data can be transferred to a storage location that is connected to the internet (e.g. Research Drive, Yoda) later. Please make sure that the data on such portable devices are secured, by using encryption (and by transporting them safely by using a lockable briefcase or backpack).\n\n\n\nTransporting physical data\nIf physical objects need to be transported, you should check with the data manager at your department (if available) what options are available. Special briefcases that can be locked or secure backpacks may need to be used to keep informed consent forms or other sensitive data objects (USB drives etc.) secure during transport. A checklist may help to ensure all objects will be taken along.\n\n\nData transportation and transfer across borders\nSome countries have rules to control the movement of encryption technology that enter or exit their borders. If you need to travel with an encrypted laptop to secure your data, for example during fieldwork abroad, please keep this in mind. If you need to transfer data in and out of such countries, please get advice on encryption and secure transportation at the IT Service Desk.\n\n\nSupport\nIf you have general questions about how to protect your data when transporting or transferring them, you can contact the IT Service Desk. In case of complex situations for which you need tailored support, you can consult the IT Relationship Manager representing the research domain, who can request capacity at IT for setting up an information security plan. Such a plan is usually based on documents which need to be completed beforehand, like a Data Protection Impact Assessment and a Data Classification."
  },
  {
    "objectID": "OS-guidelines/open-science-resources.html#support",
    "href": "OS-guidelines/open-science-resources.html#support",
    "title": "What research resources are available for VU researchers for Open Science?",
    "section": "Support",
    "text": "Support\n\nResearch Data Services\n\n\n\nImage showing the relations between VU Research Data Support Offices, at the center with grants, legal, library, security, IT for Research, and IXA around it.\n\n\nResearch Data Management is supported by various departments at the VU. These departments will help all VU researchers. There are also faculty specific support departments for research data support; they support their own faculty members.\nHere you find references to other organisational units and departments that can help you with matters related to collecting and managing data.\nVU research data support (for all researchers)*\n\nGrant office\nLibrary\nLegal\n🔒 IT for Research\nIXA"
  },
  {
    "objectID": "group-guidelines/UCDS.html",
    "href": "group-guidelines/UCDS.html",
    "title": "User-centric Data Science (UCDS)",
    "section": "",
    "text": "The UCDS is a good group!\nShuai Wang is the RDM contact person."
  },
  {
    "objectID": "topics/finding-existing-data.html",
    "href": "topics/finding-existing-data.html",
    "title": "Finding Existing Data",
    "section": "",
    "text": "Anything that can be used for analysis can be considered “data(sets)”. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g. raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your 🔒 Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only)."
  },
  {
    "objectID": "topics/finding-existing-data.html#re-using-existing-data",
    "href": "topics/finding-existing-data.html#re-using-existing-data",
    "title": "Finding Existing Data",
    "section": "",
    "text": "Anything that can be used for analysis can be considered “data(sets)”. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g. raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your 🔒 Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only)."
  },
  {
    "objectID": "topics/finding-existing-data.html#sources-for-finding-existing-datasets",
    "href": "topics/finding-existing-data.html#sources-for-finding-existing-datasets",
    "title": "Finding Existing Data",
    "section": "Sources for Finding Existing Datasets",
    "text": "Sources for Finding Existing Datasets\nThe number of datasets that are available grows rapidly. Datasets are made available in many formats, by many people or organizations. Some datasets are raw files and some are specifically organised and formatted as databases that require a licence or subscription to use them. The library of the Vrije Universiteit Amsterdam has collected links to some of the data repositories used and has licensed several databases.\n\nPopular Free and Licensed Databases: These can be found with LibSearch Advanced.\n\nIf you need help finding & using free or licensed sources you can contact the Research Data Services Helpdesk. For students and personnel in the fields of economics, finance, or organisation science a separate LibGuide has been created to help them find and use/re-use data.\nYou can also start looking for data in these four places:\n\nThe literature. Research articles may point you to the data that they are based on. Sometimes, (part of) the data are added to the article as supplementary files, and sometimes the data are published separately in a data repository. In the latter case, the article usually provides a clear reference to the published dataset. Some datasets may even be specifically published in Data Journals.\nScientific data repositories. Data repositories are platforms used to access and archive research data. Universities often provide a repository for data archiving, but other platforms arranged by discipline or by country also exist. Some repositories are only accessible to consortium members, whereas others are free of charge. Many universities in the Netherlands use DataverseNL to archive datasets for the mid-term. Long-term archiving is provided by the national research data archives DANS and 4TU.Research Data. In Europe, B2SHARE and Zenodo are platforms used to access research data. Data repositories can be accessed by searching by topic or country using Re3data, a data repository registry. The VU has its own research portal, PURE, where researchers register their datasets. You can find instructions on how to register your own dataset in PURE on the Dataset Registration page of this LibGuide.\nData search engines. Search engines allow you to quickly browse data sets and supplementary data files published by researchers. They cover data sets from many sources. This makes them useful for quick orientation on a topic. Example of a search engines are: DataCite, Google DataSet Search.\nData portals of (governmental) organisations. Organisations that regularly collect (statistical) data sometimes offer these data through their own portal. An example is Eurostat, which collects and disseminates statistics at the European level, by country and by theme. Some of these websites have been linked in the Finding data LibGuide."
  },
  {
    "objectID": "topics/finding-existing-data.html#data-sources-for-vu-researchers",
    "href": "topics/finding-existing-data.html#data-sources-for-vu-researchers",
    "title": "Finding Existing Data",
    "section": "Data Sources for VU Researchers",
    "text": "Data Sources for VU Researchers\nResearchers from the Vrije Universiteit Amsterdam have also developed some databases containing data collected during research. See here for some examples:\n\nNederlands Tweelingenregister (Netherlands Twin Register) The database contains data on twins and their families and was created to do research on the relationship between genetics and growth, development, personality, behaviour, diseases, mental health and all kinds of risks.\nGeoplaza VU - the portal for all matters related to GIS (Geographical Information Systems) and geodata at the VU University Amsterdam. It offers students and employers a platform to exchange, examine and download digital map material.\nDutch monasteries - database with information about Dutch monasteries of the Middle Ages.\nSlave owners in Amsterdam 1863 - the place of living of owners of slaves in Amsterdam in 1863, visualized in GeoPlaza.\nDeaths at the Borders Database - collection of official, state-produced evidence on people who died while attempting to reach southern EU countries from the Balkans, the Middle East, and North & West Africa, and whose bodies were found in or brought to Europe.\nDatasets published by VU Researchers can be found at the VU Research Portal."
  },
  {
    "objectID": "topics/selecting-data.html",
    "href": "topics/selecting-data.html",
    "title": "Storing vs. Archiving Data",
    "section": "",
    "text": "There is a difference between storing and archiving data. Storing refers to putting the data in a safe location while the research is ongoing. Because you are still working on the data, the data still change from time to time: they are cleaned, and analysed, and this analysis generates output. As the image below illustrates, storing could be like cooking a dish: you are cleaning and combining ingredients.\nArchiving, on the other hand, refers to putting the data in a safe place after the research is finished. The data are in a fixed state, they don’t change anymore. Archiving is done for verification purposes: so others can check that your research is sound. Or: it is done so that others can reuse the resulting dataset. There is also a difference between archiving and publishing, but in essence, archiving and publishing happen at a similar moment and for both, data do not change anymore.\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807"
  },
  {
    "objectID": "topics/selecting-data.html#selecting-data-for-archiving",
    "href": "topics/selecting-data.html#selecting-data-for-archiving",
    "title": "Storing vs. Archiving Data",
    "section": "Selecting Data for Archiving",
    "text": "Selecting Data for Archiving\nThere are various reasons to archive your data: replication, longitudinal research, data being unique or expensive to collect, re-usability and acceleration of research inside or outside your own discipline. It is VU policy to archive your data for (at least) 10 years after the last publication based on the dataset. Part of preparing your dataset for archiving is appraising and selecting your data.\n\nMake a selection before archiving your data\nDuring your research you may accumulate a lot of data, some of which will be eligible for archiving. It is impossible to preserve all data infinitely. Archiving all digital data leads to high costs for storage itself and for maintaining and managing this ever-growing volume of data and their metadata; it may also lead to decline in discoverability (see the website of the Digital Curation Centre). For those reasons, it is crucial that you make a selection.\n\n\nRemove redundant and sensitive data\nSelecting data means making choices about what to keep for the long term, and what data to archive securely and what data to publish openly. This means that you have to decide whether your dataset contains data that need to be removed or separated. Reasons to exclude data from publishing include (but are not limited to):\n\ndata are redundant\ndata concern temporary byproducts which are irrelevant for future use\ndata contain material that is sensitive, for example personal data in the sense of the GDPR, like consent forms, voice recordings, DNA data; state secrets; data that are sensitive to competition in a commercial sense. These data need to be separated from other data and archived securely\npreserving data for the long term is in breach of contractual arrangements with your consortium partners or other parties involved\n\nIn preparing your dataset for archiving, the first step is to determine which parts of your data are sensitive, which can then be separated from the other data. Redundant data can be removed altogether.\n\n\nDifferent forms of datasets for different purposes\nOnce you have separated the sensitive data from the rest of your dataset, you have to think about what to do with these sensitive materials. In some cases they may be destroyed, but you may also opt for archiving multiple datasets. For example, you may want to archive your dataset in more than one form depending on the purpose. For example:\n\nOne for reusability to share\nA second one that contains the sensitive data, and needs to be handled differently.\n\nFor the first, the non-sensitive data can be stored in an archive under restricted or open access conditions, so that you can share it and link it to publications. For the second, you need to make a separate selection, so the sensitive part can be stored safely in a secure archive (a so-called offline or dark archive). In the metadata of both archives you can create stable links between the two datasets using persistent identifiers.\n\n\nWhat to appraise for archiving\nThere are several factors that determine what data to select for archiving. For example, whether data are unique, expensive to reproduce, or if your funder requires that you make your data publicly available. This might also help you or your department to think about a standard policy or procedures for what needs to be kept, what is vital for reproducing research or reuse in future research projects.\nMore information on selecting data:\n\nTjalsma, H. & Rombouts, J. (2011). Selection of research data: Guidelines for appraising and selecting research data. Data Archiving and Networked Services (DANS).\nDigital Curation Centre (DCC): Whyte, A. & Wilson, A. (2010). How to appraise and select research data for curation. DCC How-to Guides. Edinburgh: Digital Curation Centre.\nResearch Data Netherlands: Data selection."
  },
  {
    "objectID": "topics/selecting-data.html#data-set-packaging-which-files-should-be-part-of-my-dataset",
    "href": "topics/selecting-data.html#data-set-packaging-which-files-should-be-part-of-my-dataset",
    "title": "Storing vs. Archiving Data",
    "section": "Data Set Packaging: Which Files should be Part of my Dataset?",
    "text": "Data Set Packaging: Which Files should be Part of my Dataset?\nA dataset consists of the following documents:\n\nRaw or cleaned data (if the cleaned data has been archived, the provenance documentation is also required)\nProject documentation\nCodebook or protocol\nLogbook or lab journal (when available, dependent on the discipline)\nSoftware (& version) needed to open the files when no preferred formats for the data can be provided\n\nSee the topic Metadata for more information about documenting your data.\nDepending on the research project it may be that more than one dataset is stored in more than one repository. Make sure that each consortium partner that collects data also stores all necessary data that is required for transparency and verification. A Consortium Agreement and Data Management Plan will include information on who is responsible for archiving the data."
  },
  {
    "objectID": "topics/qualtrics.html",
    "href": "topics/qualtrics.html",
    "title": "Qualtrics",
    "section": "",
    "text": "Qualtrics is a cloud-based subscription and software platform (SaaS) that enables users to create and manage online surveys. The Qualtrics survey tool can support a variety of (complex) survey design requirements by providing such functionalities as different question types, display and branching logic configuration and the use of embedded data and API’s.\nVU Amsterdam holds an academic license which includes non-standard, advanced features and functions, including but not limited to:"
  },
  {
    "objectID": "topics/qualtrics.html#license-limitation",
    "href": "topics/qualtrics.html#license-limitation",
    "title": "Qualtrics",
    "section": "License Limitation",
    "text": "License Limitation\nWhilst all VU students, employees and researchers are eligible to make use of a Qualtrics license, VU’s Qualtrics license usage is limited for creating, managing and collecting data for scientific research purposes only.\nQualtrics should not be used for non-scientific purposes such as the creation of registration or attendance lists, and course evaluation surveys for example; instead users should go to other available tools such as MS Forms."
  },
  {
    "objectID": "topics/qualtrics.html#creating-and-accessing-user-accounts",
    "href": "topics/qualtrics.html#creating-and-accessing-user-accounts",
    "title": "Qualtrics",
    "section": "Creating and Accessing User Accounts",
    "text": "Creating and Accessing User Accounts\nThe VU Qualtrics Central Brand (hereafter VU’s Central Brand) at [vuamsterdam.eu.qualtrics.com] allows for user’s auto-enrolment, an automated account creation, and registration process. Any user with an enabled VU email address may auto-enroll on VU’s Central Brand. The email address will function as the account’s username.\nVU’s Central Brand makes use of Single-Sign On (SSO). This means that users can log in to Qualtrics using VU’s internal login system. Multi-Factor Authentication (MFA) is mandatory to all Qualtrics users utilizing the vuamsterdam brand. MFA acts as an additional layer of security to prevent unauthorized users from accessing accounts and resources, even when the password has been stolen. Qualtrics will use multi-factor authentication to validate user identity and provide quick and convenient access to authorized users.\n\nTo create a user account and access the Qualtrics environment at the Central Brand, please proceed as follows:\n\nGo to: https://vuamsterdam.eu.qualtrics.com\nOn the login page, choose: Vrije Universiteit SSO\nLog in with your VU account credentials (email address and password)\n\n\n\n\nVU’s central brand login page\n\n\nIt is recommended to save VU’s Central Brand as a Favourite or Bookmark link for future reference."
  },
  {
    "objectID": "topics/qualtrics.html#note-to-existing-users",
    "href": "topics/qualtrics.html#note-to-existing-users",
    "title": "Qualtrics",
    "section": "Note to Existing Users",
    "text": "Note to Existing Users\n\nSecurity Settings: Login Error Disabled Account\nSecurity settings are in place that disable a user account after a certain amount of inactivity. This threshold is currently 12 months. This setting follows security requirements regarding User Account Management.\n\n\n\nNotification of disabled Qualtrics account\n\n\nA disabled account status does not affect its data.\nUsers who receive an error when trying to log back in after extended periods of inactivity ([User account is disabled]) should contact the RDM Support Desk to have their accounts re-enabled."
  },
  {
    "objectID": "topics/qualtrics.html#getting-started",
    "href": "topics/qualtrics.html#getting-started",
    "title": "Qualtrics",
    "section": "Getting Started",
    "text": "Getting Started\nThe following resources are available for you to get started with Qualtrics:\n\nQualtrics Basecamp\nbasecamp.qualtrics.com, the online learning platform where you have access to learning instructions on how to use Qualtrics.\n\n\nQualtrics Knowledge Base\nQualtrics offers access to an extensive library of knowledge base articles, including detailed instructions on how to use and configure (advance) features.\nQualtrics Knowledge Base library can be accessed directly at qualtrics.com/support\n\n\nQualtrics Community\nAsk questions to the Qualtrics community (platform of all Qualtrics users).\nWhen logging in choose “Sign in with SSO”, when asked “Enter your company’s Organization ID” fill in vuamsterdam."
  },
  {
    "objectID": "topics/fair-principles.html",
    "href": "topics/fair-principles.html",
    "title": "FAIR Principles",
    "section": "",
    "text": "This page discusses what the FAIR principles (Wilkinson et al. 2016) are, why they are important and how you can work in line with these principles at VU."
  },
  {
    "objectID": "topics/fair-principles.html#what-are-the-fair-principles",
    "href": "topics/fair-principles.html#what-are-the-fair-principles",
    "title": "FAIR Principles",
    "section": "What are the FAIR principles?",
    "text": "What are the FAIR principles?\nThe FAIR principles were formulated in 2016 to guide researchers in increasing the Findability, Accessibility, Interoperability and Reusability of their data (see the publication in the journal Scientific Data and the summary of the principles). The goal is to ensure that scholarly data can be used as widely as possible – accelerating scientific discoveries and benefiting society in the process.\nA lot of good resources exist already that explain the FAIR principles very well:\n\nGO FAIR provides a clear overview of the FAIR principles\nThe Turing Way has a great information page about FAIR, containing a lot of references to other useful sources\nThe story A FAIRy tale explains all principles in an understable way\n\nThe FAIR principles were rapidly adopted by Dutch and European funding agencies. If you receive a research grant from NWO, ZonMw, or the European Commission, you will be asked to make your data FAIR."
  },
  {
    "objectID": "topics/fair-principles.html#how-can-you-benefit-from-working-in-line-with-the-fair-principles",
    "href": "topics/fair-principles.html#how-can-you-benefit-from-working-in-line-with-the-fair-principles",
    "title": "FAIR Principles",
    "section": "How can you benefit from working in line with the FAIR principles?",
    "text": "How can you benefit from working in line with the FAIR principles?\nYou do not need to apply all FAIR principles at once to start benefiting from making your data FAIR. Applying even just some of the principles will increase the visibility and impact of your data, leading to:\n\nIncreased citations of the datasets themselves and your research\nImproved reproducibility of your research\nCompliance with funder and publisher requirements\n\nMaking your data FAIR will also make it possible for you to easily find, access and reuse your own data in the future. You may be the first and most important beneficiary of making your own data FAIR."
  },
  {
    "objectID": "topics/fair-principles.html#making-data-fair-how-to-get-started-in-three-easy-steps",
    "href": "topics/fair-principles.html#making-data-fair-how-to-get-started-in-three-easy-steps",
    "title": "FAIR Principles",
    "section": "Making data FAIR – how to get started in three easy steps?",
    "text": "Making data FAIR – how to get started in three easy steps?\n\nStart with a data management plan\nA DMP is a living document in which you specify what kinds of data you will use in your project, and how you will process, store and archive them. Preparing a data management plan should be your first step in the process to make data FAIR. The DMP template will ask questions that enable you to systematically address the things that need to be done to make your data FAIR. Writing a DMP is also a requirement from funding agencies and some faculties at the VU. At the VU, you can use DMPonline to create and share DMPs.\n\n\nDescribe and document your data\nTo be findable, data need to be described with appropriate metadata. Metadata can include keywords, references to related papers, the researchers’ ORCID identifiers, and the codes for the grants that supported the research. You will need to provide such metadata when you are uploading data to a repository (see below). You increase findability by filling out as many metadata fields as possible and by providing rich descriptions in terminology that is common in your field.\nTo be reusable, data need to be accompanied by documentation describing how the data was created, structured, processed, and so on. It is good practice to integrate writing documentation during the research process. It will be easier and take less time compared to when you try to do this at the end. Having documentation on the research process will also help you to redo parts of your data cleaning actions or data analysis if necessary.\nIf you have questions about metadata and documentation, contact the RDM Support Desk and we will be happy to help you and to provide advice.\n\n\nMake your data available through a trustworthy repository\nIf you choose a repository that: assigns a persistent identifier to both the data and the metadata; attaches metadata to the data according to standard metadata schemas; releases data with a license; and provides access to the data and metadata via an open and standard communication protocol (such as http) – then your data will meet many, if not most, of the FAIR principles.\nThe VU provides three repositories which meets all of these conditions:\n\nDataverseNL\nYoda - Yoda information page and Yoda publication platform\nOpen Science Framework (OSF)\n\nCosts for using these repositories for datasets up to 500 GB are covered by the faculty. There are costs involved for you department or project if a datasets is larger than 500 GB. See the storage cost model for details."
  },
  {
    "objectID": "topics/fair-principles.html#what-if-i-cannot-share-my-data",
    "href": "topics/fair-principles.html#what-if-i-cannot-share-my-data",
    "title": "FAIR Principles",
    "section": "What if I cannot share my data?",
    "text": "What if I cannot share my data?\nData do not need to be open to be FAIR. The FAIR principles allow for controlled access, which can be important for certain types of data, such as personal data, medical data, competitive company data. The guiding principle is always that data should be as “as open as possible, as closed as necessary”. If data cannot be openly shared, because they are too sensitive, then “the FAIR approach would be to make the metadata publicly available and provide information about the conditions for accessing the data itself.”"
  },
  {
    "objectID": "topics/scistor.html",
    "href": "topics/scistor.html",
    "title": "SciStor",
    "section": "",
    "text": "The storage service SciStor (Storage for Scientists) is intended for storing large amounts of research data. SciStor can be used from any VU workstation and, via utilities, from home or on the road. The access rights can be set relatively fine-grained.\nSciStor is hosted by IT for Research (ITvO) on the VU campus and is therefore close to lab equipment, workstations, the BAZIS HPC cluster and SciCloud.\nIf desired, automatic backups can be made of the data.\nBecause SciStor is mainly intended for high performance on-campus use, access is only possible with a VUnetId."
  },
  {
    "objectID": "topics/scistor.html#storage-space-reservation",
    "href": "topics/scistor.html#storage-space-reservation",
    "title": "SciStor",
    "section": "Storage space reservation",
    "text": "Storage space reservation\nStorage space on SciStor is provided on a reservation basis. This means that the amount of space requested is guaranteed to be available. If desired, the reservation can be adjusted up or down very quickly, subject to a few exceptions. The owner of the SciStor share receives monthly usage reports. The report provides insight on used and available space."
  },
  {
    "objectID": "topics/scistor.html#backups",
    "href": "topics/scistor.html#backups",
    "title": "SciStor",
    "section": "Backups",
    "text": "Backups\nSciStor shares can be backed up using the ‘snapshot’ technique. This makes it possible, among other things, for Windows users of a SciStor share to retrieve data from the backup themselves. The backup data is stored at an off-campus datacenter to ensure your data protected against disasters."
  },
  {
    "objectID": "topics/scistor.html#requesting-a-scistor-share",
    "href": "topics/scistor.html#requesting-a-scistor-share",
    "title": "SciStor",
    "section": "Requesting a SciStor share",
    "text": "Requesting a SciStor share\nSciStor is available for all VU research groups. Minimum storage space that can be requested is 100 GB, for a minimum of three months. The capacity can be increased or decreased in units of 100 GB if needed.\nYou can find the request form on 🔒 ServiceNow, go to: IT &gt; My work field &gt; Research &gt; SciStor &gt; Realisation of new storage for research (SciStor)\nAfter submitting the application, IT for Research will contact you to schedule an interview to discuss naming the SciStor share, how the backups work, who should have access, etc.\nMost SciStor configurations can be delivered within one or two days. More complex configurations may take a little longer."
  },
  {
    "objectID": "topics/metadata.html",
    "href": "topics/metadata.html",
    "title": "Metadata",
    "section": "",
    "text": "Metadata provide information about your data. Structured metadata are intended to provide this information in a standardised way. The structured metadata are readable for both humans and machines. It can be used by data catalogues, for example DataCite Commons.\nThe standardisation of metadata involves the following aspects:"
  },
  {
    "objectID": "topics/metadata.html#metadata-standards",
    "href": "topics/metadata.html#metadata-standards",
    "title": "Metadata",
    "section": "Metadata standards",
    "text": "Metadata standards\nMetadata standards allow for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At the VU the following standards are used:\n\nYoda uses the DataCite metadata standard\nDataverseNL uses the Dublin Core metadata standard\nThe VU Research Information System PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology."
  },
  {
    "objectID": "topics/metadata.html#controlled-vocabularies-classifications",
    "href": "topics/metadata.html#controlled-vocabularies-classifications",
    "title": "Metadata",
    "section": "Controlled Vocabularies & Classifications",
    "text": "Controlled Vocabularies & Classifications\nControlled vocabularies are lists of terms created by domain experts to refer to a specific phenomenon or event. Controlled vocabularies are intended to reduce ambiguity that is inherent in normal human languages where the same concept can be given different names and to ensure consistency. Controlled vocabularies are used in subject indexing schemes, subject headings, thesauri, taxonomies and other knowledge organisation systems. Some vocabularies are very internationally accepted and standardised and may even become an ISO standard or a regional standard/classification. Controlled vocabularies can be broad in scope or very limited to a specific field. When a Data Management Plan template includes a question on the used ontology (if any), what is usually meant is: is there a specific vocabulary or classification system used? The National Bioinformatics Infrastructure Sweden gives some more explanation about controlled vocabularies and ontologies here. In short, an ontology does not only describe terms, but also indicates relationships between these terms.\nExamples of controlled vocabularies are:\n\nCDWA (Categories for the Description of Works of Art)\nGetty Thesaurus of Geographic names\nNUTS (Nomenclature of territorial units for statistics)\nMedical Subject HEadings (MeSH)\nThe Environment Ontology (EnvO)\n\nMany examples of vocabularies and classification systems can be found at the FAIRsharing.org website. It has a large list for multiple disciplines. If you are working on new concepts or new ideas and are using or creating your own ontology/terminology, be sure to include them as part of the metadata documentation in your dataset (for example as part of your codebook)."
  },
  {
    "objectID": "topics/metadata.html#metadata-levels",
    "href": "topics/metadata.html#metadata-levels",
    "title": "Metadata",
    "section": "Metadata levels",
    "text": "Metadata levels\nFinally a distinction can be made on the level of description. Metadata can be about the data as a whole or about part of the data. It can depend on the research domain and the tools that are used on how many levels the data can be described. In repositories like Yoda and DataverseNL it is common practice to only create structured metadata on the level of the data as a whole. The Consortium of European Social Science Data Archives (CESSDA) explains this distinction for several types of data in their Data Management Expert Guide.\n\n\n\nFlowchart indicating a project with a Folder a and Folder b, where Folder a has File 1 and File 2. The project, Folder a, and File 1, have linked metadata to them."
  },
  {
    "objectID": "topics/metadata.html#dataset-registration",
    "href": "topics/metadata.html#dataset-registration",
    "title": "Metadata",
    "section": "Dataset registration",
    "text": "Dataset registration\nWhen you want to make sure that your dataset is findable it is recommended that the elements of the description of your dataset are made according to a certain metadata standard that allows for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At the VU the following standards are used:\n\nDataverseNL and DANS use the Dublin Core metadata standard\nThe VU Research Portal PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology. Go to the overview of metadata standards. More important tips are available at Dataset & Publication."
  },
  {
    "objectID": "topics/safe-data-transfer.html",
    "href": "topics/safe-data-transfer.html",
    "title": "Safe Data Transportation and Transfer",
    "section": "",
    "text": "It is important to protect your data during the entire data life cycle. To find out whether your data are secure during all stages of your research, think about your data flow: where do your data originate and where do they go to? If data need to be transported from one physical place to the other, or need to be transferred from one device to another, these actions should happen in a secure way.\n\nTransferring digital data\n\nOnline connection on campus\nIf data collection takes place through a certain measurement device (e.g. MRI scanner, EEG scanner, eye tracker), the data need to be transferred from the measurement device to the storage location that you will use during your research project. Make sure that this transfer takes place in a secure way and also make a plan for the data on the measurement device; find out whether they need to be destroyed or can remain there.\n\n\nOnline connection outside campus (with and without VUnetID)\nIf you are doing fieldwork outside the campus and you have reliable and secure internet access, it is a good idea to upload the data to a storage location that is regularly backed up and secure, in order to prevent data loss. If you have a VUnetID, you can for example use:\n\nResearch Drive to securely and easily store and share research data.\nSURFfilesender to send you data to a colleague or consortium partner, who can store your data in an appropriate place\n\nYou can find more information about each of these storage options in the Data Storage topic.\nIf you need to receive data from colleagues in your project who don’t have access to these tools (e.g. because they are students, don’t work for a Dutch educational institution, or have no VUnetID), Research Drive, Yoda, SURFfilesender and secure emailing with Zivver can also be used:\n\nResearch Drive: This cloud storage service provided by SURF enables researchers from VU and external researchers to manage files and folders in a shared storage location.\nSURFfilesender: as a SURFfilesender user, you can send a voucher to someone who doesn’t have access to this tool. This person can use this voucher to send documents to you. These files can be encrypted.\nYoda: This cloud storage service provided by SURF enables researchers from VU and external researchers to manage files and folders in a shared storage location.\n🔒 Zivver is an email plugin with which you can encrypt emails and attachments.\n\n\n\nOffline data outside campus\nIf you are doing fieldwork in an area with limited internet access, you might use a portable device to initially store your data during the phase of data collection, such as a USB drive or an external hard drive. These data can be transferred to a storage location that is connected to the internet (e.g. Research Drive, Yoda) later. Please make sure that the data on such portable devices are secured, by using encryption (and by transporting them safely by using a lockable briefcase or backpack).\n\n\n\nTransporting physical data\nIf physical objects need to be transported, you should check with the data manager at your department (if available) what options are available. Special briefcases that can be locked or secure backpacks may need to be used to keep informed consent forms or other sensitive data objects (USB drives etc.) secure during transport. A checklist may help to ensure all objects will be taken along.\n\n\nData transportation and transfer across borders\nSome countries have rules to control the movement of encryption technology that enter or exit their borders. If you need to travel with an encrypted laptop to secure your data, for example during fieldwork abroad, please keep this in mind. If you need to transfer data in and out of such countries, please get advice on encryption and secure transportation at the IT Service Desk.\n\n\nSupport\nIf you have general questions about how to protect your data when transporting or transferring them, you can contact the IT Service Desk. In case of complex situations for which you need tailored support, you can consult the IT Relationship Manager representing the research domain, who can request capacity at IT for setting up an information security plan. Such a plan is usually based on documents which need to be completed beforehand, like a Data Protection Impact Assessment and a Data Classification."
  },
  {
    "objectID": "topics/trainings.html",
    "href": "topics/trainings.html",
    "title": "Trainings",
    "section": "",
    "text": "It is easy to get overwhelmed with all the trainings available. On this page we provide a list of trainings available."
  },
  {
    "objectID": "topics/trainings.html#bytes-and-bites",
    "href": "topics/trainings.html#bytes-and-bites",
    "title": "Trainings",
    "section": "Bytes and Bites",
    "text": "Bytes and Bites\nDo you want to meet other researchers, improve your programming skills, or ask questions related to programming? Pick up your laptop and come to Bytes & Bites. We’re back in full swing for another edition of our coding cafe Bytes & Bites. At Bytes & Bites anyone is welcome, whether you are a beginner or advanced programmer, whether you write in R or in C++. And of course, you can’t program and work with “Bytes” without any tasty “Bites”! We’ll make sure there is pizza or snacks available!\n\n\nMore information: https://ubvu.github.io/bytes-and-bites/\nTopics: Programming, Python, R, Community, Software, Coding\nTarget audience: Students, researchers, data stewards\nStatus: Monthly\nDuration: 120 minutes\nOnline/in-person: In-person"
  },
  {
    "objectID": "topics/trainings.html#data-analysis-with-r-data-carpentry-workshop-for-programming-beginners",
    "href": "topics/trainings.html#data-analysis-with-r-data-carpentry-workshop-for-programming-beginners",
    "title": "Trainings",
    "section": "Data Analysis with R — Data Carpentry workshop for programming beginners",
    "text": "Data Analysis with R — Data Carpentry workshop for programming beginners\nAre you analysing tabular data in your research? Would you like to learn how to use the programming language R to make your work more effective and efficient? This workshop is for absolute programming beginners and introduces basic steps for the analysis and visualization of tabular data with R Studio. You will:\n\nOrganize tabular data, handle date formatting, carry out quality control and quality assurance and export data to use with downstream applications.\nExplore, summarize, and clean tabular data reproducibly.\nImport data, calculate summary statistics, and create publication-quality graphics using the programming language R\n\n\n\nRegistration form: https://vu-nl.libcal.com/calendar/universitylibrary?t=g&q=data%20carpentry&cid=7052&cal=7052&inc=0\nTraining materials: https://datacarpentry.org/lessons/#social-science-curriculum\nTopics: Software skills, Data analysis, Plotting, R, OpenRefine, Spreadsheets\nTarget audience: Researchers, students\nStatus: Available on set moments\nDuration: 28 hours over four days\nOnline/in-person: In-person and online (self-study)"
  },
  {
    "objectID": "topics/trainings.html#escape-room-data-horror",
    "href": "topics/trainings.html#escape-room-data-horror",
    "title": "Trainings",
    "section": "Escape Room: Data Horror",
    "text": "Escape Room: Data Horror\nResolve the data horror of professor Hutseephluts and secure the grant! This online escape room challenges everyone to tackle the horrors of research data management. Will you be able to escape within an hour?\nThe Data Horror Escape Room was made for the Data Horror week 2020.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.6949510\nLicence: CC-BY-SA-4.0\nTopics: Research data, Escape room, Workshop, Data management, Research data management, FAIR\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 60 minutes\nOnline/in-person: Online"
  },
  {
    "objectID": "topics/trainings.html#escape-room-open-science-horror",
    "href": "topics/trainings.html#escape-room-open-science-horror",
    "title": "Trainings",
    "section": "Escape Room: Open Science Horror",
    "text": "Escape Room: Open Science Horror\nHelp the cyborgs by publishing their code the right way — the open science way! This online escape room challenges everyone to tackle the horrors of open science and open access publishing. Will you be able to save the cyborgs and finally get a coffee?\nThe Open Science Horror Escape Room was made for the Data Horror week 2021.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.6963493\nLicence: CC-BY-4.0\nTopics: Open science, Escape room, Workshop, Open access, Research data management, FAIR\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 30-60 minutes\nOnline/in-person: Online"
  },
  {
    "objectID": "topics/trainings.html#escape-room-software-horror",
    "href": "topics/trainings.html#escape-room-software-horror",
    "title": "Trainings",
    "section": "Escape Room: Software Horror",
    "text": "Escape Room: Software Horror\nThe only thing between you and certain doom — getting your software management in order! This online escape room challenges everyone to tackle the horrors of software management and open software publishing. Will you be able to save your own soul and publish in Frontiers in Hell?\nThe Software Horror Escape Room was made for the Data Horror week 2022.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.7350527\nLicence: CC-BY-4.0\nTopics: Software management, Escape room, Workshop, Software, Research data management, FAIR,\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 60 minutes\nOnline/in-person: Online"
  },
  {
    "objectID": "topics/trainings.html#lego-workshop",
    "href": "topics/trainings.html#lego-workshop",
    "title": "Trainings",
    "section": "Lego Workshop",
    "text": "Lego Workshop\nThis workshop offers a hands-on experience in the importance of careful documentation during research. Participants will discover the pitfalls in communicating research progress through written media. This offers a fun introduction in writing well structured contextual metadata such as research logs, protocols, machine settings and general README files.\nThe data package offers a powerpoint and general guidelines in hosting the workshop.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.10174000\nLicence: CC-BY-4.0\nTopics: Metadata, Contextual metadata, LEGO, Workshop\nTarget audience: Researchers\nStatus: Active\nDuration: 30-90 minutes\nOnline/in-person: In-person"
  },
  {
    "objectID": "topics/trainings.html#open-science-framework-osf-workshop",
    "href": "topics/trainings.html#open-science-framework-osf-workshop",
    "title": "Trainings",
    "section": "Open Science Framework (OSF) Workshop",
    "text": "Open Science Framework (OSF) Workshop\nThis is a hands-on course to get started with the Open Science Framework (OSF). You won’t need any experience with the tool beforehand. We will show the differences and similarities between OSF and other tools at the VU (such as Yoda). We will make a preregistration of a (mock) OSF research.\nThe OSF is an open-source project management tool that supports researchers throughout their entire project lifecycle. As a collaboration tool, OSF helps research teams work on projects privately or make the whole project publicly accessible for broad dissemination. As a workflow system, OSF enables connections to the many scientific tools researchers already use, streamlining their process and increasing efficiency. You may even use OSF as a portfolio tool for sharing your work as a prepublication with potential collaborators.\n\n\nTraining materials: https://osf.io/ab923/\nTopics: OSF, Preprint, Publishing, Archiving, RDM tools, Data management\nTarget audience: Students, researchers, data stewards\nStatus: Bi-annually during the Support Training Days\nDuration: 120 minutes\nOnline/in-person: Online"
  },
  {
    "objectID": "topics/trainings.html#open-science-against-humanity",
    "href": "topics/trainings.html#open-science-against-humanity",
    "title": "Trainings",
    "section": "Open Science against Humanity",
    "text": "Open Science against Humanity\nThis card game is based on “Cards Against Humanity” and teaches basic concepts of Open Science, Research Data Management, Software Management, FAIR principles, and Research Ethics in a fun and entertaining way. The white cards describe research related situations or statements relevant for researchers. The black cards contain potential answers or prompts that have a connection to Open Science. The goal of the game is to pair the white cards (prompts) and the black cards in the funniest, most provocative, or smartest way you can. Playing the card game online or with a physical deck creates awareness of issues around resesarch practices and allows for discussions around Open Science.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.10017280\nLicence: CC-BY-4.0\nTopics: Open science, Card game, Workshop, Game, Research data management, FAIR, Software management\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 30-60 minutes\nOnline/in-person: Online and in-person"
  },
  {
    "objectID": "topics/trainings.html#open-loves-science",
    "href": "topics/trainings.html#open-loves-science",
    "title": "Trainings",
    "section": "Open loves Science",
    "text": "Open loves Science\nOpen Science aims to improve, streamline and elevate science to something bigger. Why? Out of love for science of course!\nIn Open loves Science, players are invited to engage in playful and deep conversations about Open Science. The card game features an encyclopedia of issues that pervade this movement of research reform and ask participants to consider their role and values in changing academic culture. Like Open Science, this game is about connection. Players are meant to look into each other’s eyes, engage in conversation, and better understand one another.\n“Open loves Science” was made for the Data love week 2024.\n\n\nTraining materials: https://nlesc.github.io/open-loves-science/\nLicence: CC-BY-4.0\nTopics: Open science, Card game, Workshop, Game, Research data management, FAIR, Software management\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 30-60 minutes\nOnline/in-person: Online and in-person"
  },
  {
    "objectID": "topics/trainings.html#software-carpentries",
    "href": "topics/trainings.html#software-carpentries",
    "title": "Trainings",
    "section": "Software Carpentries",
    "text": "Software Carpentries\nThe Software Carpentries are hands-on workshops that teach basic skills needed to program in a reproducible way.\nA Software Carpentry workshop covers lessons on:\n\nPlotting and Programming in Python or R\nThe Unix Shell\nVersion Control with Git and GitHub\n\nThe lessons are designed for programming beginners and do not require any experience. You program along, learn by helping one another, and apply what you have learned in exercises.\n\n\nRegistration form: https://vu-nl.libcal.com/calendar/universitylibrary?t=g&q=software%20carpentry&cid=7052&cal=7052&inc=0\nTraining materials: https://software-carpentry.org/lessons/\nTopics: Coding, Software skills, Python, R, Bash, Unix Shell, Git, GitHub, Version control\nTarget audience: Researchers, students\nStatus: Available on set moments\nDuration: 28 hours over four days\nOnline/in-person: In-person and online (self-study)"
  },
  {
    "objectID": "topics/researchcloud.html",
    "href": "topics/researchcloud.html",
    "title": "SURF Research Cloud",
    "section": "",
    "text": "SURF Research Cloud is a portal where you easily build a virtual research environment. You can use preconfigured workspaces and datasets or add them yourself. Institutions, research communities and suppliers can contribute to Research Cloud’s functionality and catalogue by integrating their computing and data services.\nSURF Research Cloud is hosted at the SURF datacenter in Amsterdam."
  },
  {
    "objectID": "topics/researchcloud.html#what-is-it",
    "href": "topics/researchcloud.html#what-is-it",
    "title": "SURF Research Cloud",
    "section": "",
    "text": "SURF Research Cloud is a portal where you easily build a virtual research environment. You can use preconfigured workspaces and datasets or add them yourself. Institutions, research communities and suppliers can contribute to Research Cloud’s functionality and catalogue by integrating their computing and data services.\nSURF Research Cloud is hosted at the SURF datacenter in Amsterdam."
  },
  {
    "objectID": "topics/researchcloud.html#what-can-it-be-used-for",
    "href": "topics/researchcloud.html#what-can-it-be-used-for",
    "title": "SURF Research Cloud",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nSURF Research Cloud enables you to run 1 or more servers with applications that can be accessible over the internet.\n\nHosting web applications, these can be connected to SRAM for secure authentication. You can easily start more servers to increase availability and preformance.\nRunning a desktop environment with analysis tools, use your favorite desktop tools on an environment with more performance. Unlike SciCloud GPU resources are available. You can easily connect an environment to your data on Research Drive or Yoda.\nHosting a highly secure research environment where sensitive data cannot leave the VRE: SURF SANE."
  },
  {
    "objectID": "topics/researchcloud.html#are-there-costs-involved",
    "href": "topics/researchcloud.html#are-there-costs-involved",
    "title": "SURF Research Cloud",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nYou will need to apply for a wallet with credits, see How to request access below.\nIn 2025 the credit cost is:\n\n1.03 credits per CPU hour (only if the system is active).\n21 credits per GPU hour (only if the system is active).\nHDD storage 681 credits per TB per month, SSD storage 1525 credits per TB per month.\n\nSee the SURF services and rates document."
  },
  {
    "objectID": "topics/researchcloud.html#how-to-request-access",
    "href": "topics/researchcloud.html#how-to-request-access",
    "title": "SURF Research Cloud",
    "section": "How to request access",
    "text": "How to request access\nAll VU researchers can get access to SURF Research Cloud. To start building an environment in Research Cloud you need 2 things:\n\n1. An SRAM Collaboration connected to Research Cloud\n\nFirst apply for a new SRAM Collaboration (CO): log in to SRAM and click the “Request collaboration” button.\nThe SRAM admins at IT for Research (ITvO) will contact you to discuss your specific needs and help you to get started.\nOnce the Collaboration request is approved you can connect the Research Cloud Application to the new collaboration.\nInvite the colleagues that need to work in your VRE to your new CO.\n\n\n\n2. A Wallet with credits\nThere are 2 ways to obtain credits:\n\nApply for a NWO Small Compute Grant by following the links on the Small Compute applications page. In the request form select “SURF Research Cloud - HPC Cloud”.\nIf for whatever reason your application for this grant is denied, you could also claim credits from the VU Research Capacity Computing Service contract with SURF contract. Please contact IT for Research for details on how to obtain these credits.\n\nOnce the request is granted you will have access to a wallet in Research Cloud and can start to build an environment."
  },
  {
    "objectID": "topics/researchcloud.html#getting-started",
    "href": "topics/researchcloud.html#getting-started",
    "title": "SURF Research Cloud",
    "section": "Getting started",
    "text": "Getting started\nDocumentation for Research Cloud can be found on the SURF User Knowledge Base.\nIT for Research can assist you in setting up a new Research Cloud environment."
  },
  {
    "objectID": "topics/researchcloud.html#contact",
    "href": "topics/researchcloud.html#contact",
    "title": "SURF Research Cloud",
    "section": "Contact",
    "text": "Contact\nWondering if SURF Research Cloud fits your research needs? Please contact IT for Research"
  },
  {
    "objectID": "topics/cff.html",
    "href": "topics/cff.html",
    "title": "Citation File Format (CFF)",
    "section": "",
    "text": "A Citation File Format (CFF) is a computer file that contains all information needed to cite something. For example, a dataset or a piece of software.\nSeveral data and software repositories, such as Zenodo and GitHub, support this format and enable others to easily select the citation information in their preferred format. Most reference managers can work with these standardized citation files, and automatically format the references in a document.\n\n\n\nA screenshot of the interface to cite a GitHub repository\n\n\nAn example, CFF-file is given below, but to enable you to easily create a CFF-file you can use this website.\ncff-version: 1.2.0\nmessage: \"If you use this data, please cite it as below.\"\nauthors:\n - family-names: Druskat\n   given-names: Stephan\n   orcid: https://orcid.org/1234-5678-9101-1121\ntitle: \"My Research Software\"\nversion: 2.0.4\nidentifiers:\n  - type: doi\n    value: 10.5281/zenodo.1234\ndate-released: 2021-08-11"
  },
  {
    "objectID": "topics/data-management-section.html",
    "href": "topics/data-management-section.html",
    "title": "Data Management Section",
    "section": "",
    "text": "Many funders require researchers to include a section in their project proposal about Research Data Management, in which they explain whether existing data will be reused, whether new data will be collected or generated during the project, and how they plan to structure, archive and share their data. Depending on requirements of the funder, the paragraph can be short or more extensive.\nFunders may have different requirements for the data management section in the project proposal. Always check what your funder asks for. Below is a list of information on data management sections from main Dutch funding bodies.\n\nNWO\nZonMw\n\nWe recommend you to ask advice from the RDM Support Desk when writing your data management section."
  },
  {
    "objectID": "topics/dataset-and-software-registration.html",
    "href": "topics/dataset-and-software-registration.html",
    "title": "Dataset and software registration in PURE",
    "section": "",
    "text": "When you have finished finalizing a dataset or research software and are ready to archive it, there are many options available. Depending on the research and choices made earlier the archive provides the option to fill in descriptive fields for a dataset. The descriptions in the archives often are automatically created using metadata standards like DataCite or Dublin Core, or some other type of standard. See also section Metadata\nWhen registering a dataset or research software in an archive it is important to use unique identifiers to allow for increased findability and easy attribution & citation. Examples of this are:\n\nPersonal names: try to consistently use the same notation for all researchers and assistants that are included as authors\nORCID: using a unique identifier like this for all authors is recommended. More information is available here.\nInstitutional names: avoid using different versions (or language versions) of participating Institutes/organisations and departments. In the case of the VU the official written name is: Vrije Universiteit Amsterdam. For each organisation or Institute that is included: try to make sure that the official name is used each time.\n\nSome archives also allow you to preregister your project/dataset/software. Examples are:\n\nOpen Science Framework (OSF) Registration\nZenodo & registration"
  },
  {
    "objectID": "topics/dataset-and-software-registration.html#registration-findability",
    "href": "topics/dataset-and-software-registration.html#registration-findability",
    "title": "Dataset and software registration in PURE",
    "section": "",
    "text": "When you have finished finalizing a dataset or research software and are ready to archive it, there are many options available. Depending on the research and choices made earlier the archive provides the option to fill in descriptive fields for a dataset. The descriptions in the archives often are automatically created using metadata standards like DataCite or Dublin Core, or some other type of standard. See also section Metadata\nWhen registering a dataset or research software in an archive it is important to use unique identifiers to allow for increased findability and easy attribution & citation. Examples of this are:\n\nPersonal names: try to consistently use the same notation for all researchers and assistants that are included as authors\nORCID: using a unique identifier like this for all authors is recommended. More information is available here.\nInstitutional names: avoid using different versions (or language versions) of participating Institutes/organisations and departments. In the case of the VU the official written name is: Vrije Universiteit Amsterdam. For each organisation or Institute that is included: try to make sure that the official name is used each time.\n\nSome archives also allow you to preregister your project/dataset/software. Examples are:\n\nOpen Science Framework (OSF) Registration\nZenodo & registration"
  },
  {
    "objectID": "topics/dataset-and-software-registration.html#register-your-datasetsoftware-in-pure",
    "href": "topics/dataset-and-software-registration.html#register-your-datasetsoftware-in-pure",
    "title": "Dataset and software registration in PURE",
    "section": "Register your Dataset/Software in PURE",
    "text": "Register your Dataset/Software in PURE\nJust like your publications, data that you have collected for your research constitute research output, too. Therefore you are required to record your datasets and research software in PURE. Your datasets and research software can be of interest to others, which can in turn lead to new collaboration opportunities. Datasets and research software recorded in PURE also appear in reports that are used for research evaluations. Even if access to your dataset and research software is closed, you are required to register your dataset and research software in PURE. It is a record of the research, data collection and analysis that you have carried out.\n\nBenefits of recording your dataset/software in PURE\n\nIt increases the visibility and findability of your datasets and research software\nIt contributes to re-use and transparency\nIt boosts your collaboration opportunities\nIt counts towards research evaluations and assessments\n\n\n\nHow to register your dataset/software in PURE?\n\n\n\nAn image of PURE, indicating where to add a new dataset or software\n\n\n\nLog into the VU Research Portal (PURE) using your VU credentials\nClick on the “+” (plus) icon next to selecting “Datasets/Software” in the overview\nYou can fill in the form using the following manuals and read more about the various metadata in use (generic and subject specific):\n\ndataset manual (NL)\nsoftware manual (EN)\n\nClick on “Save” to store the registration"
  },
  {
    "objectID": "topics/software-licensing.html",
    "href": "topics/software-licensing.html",
    "title": "Software Licensing",
    "section": "",
    "text": "Publishing research software under an appropriate license is crucial for its accessibility, usability, and further integration into research. Choosing a license usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked.\nA software license states how other people may re-use your code and under which circumstances. For research software, it is recommended (and often required by funders) that licenses are as permissible as possible.\nThere are many licenses out there; below we list some very frequently used licenses in research software. However, if none of these licenses fit your case, there are several tools that can help you to choose a suitable software license. If you need guidance in choosing a licence for your software, get in touch with the RDM Support Desk."
  },
  {
    "objectID": "topics/software-licensing.html#mit-license",
    "href": "topics/software-licensing.html#mit-license",
    "title": "Software Licensing",
    "section": "MIT License",
    "text": "MIT License\nThe MIT License is a popular choice, due to its readability and permissiveness. It allows users to reuse the software for any purpose, including using, copying, modifying, and distributing it, provided they include the original copyright notice and license text.\nHowever, its permissiveness means that derivative works can be closed-source and do not need to mention that they use your code, which might not align with all scientific openness goals or general."
  },
  {
    "objectID": "topics/software-licensing.html#gnu-gplv3",
    "href": "topics/software-licensing.html#gnu-gplv3",
    "title": "Software Licensing",
    "section": "GNU GPLv3",
    "text": "GNU GPLv3\nThe GNU General Public License (GPLv3) is another option, designed to ensure that the software and any derivatives remain open-source.\nThis encourages collaborative improvement of software. Any software that includes GPL-licensed code must also be open-source under the GPLpotentially deterring commercial use or integration with proprietary software. In conclusion, when you want your code to be used by others, but only the code that uses your code is also open source, this is the way to go."
  },
  {
    "objectID": "topics/software-licensing.html#apache-license-2.0",
    "href": "topics/software-licensing.html#apache-license-2.0",
    "title": "Software Licensing",
    "section": "Apache License 2.0",
    "text": "Apache License 2.0\nThe Apache License 2.0 allows for modification and distribution of the software and its derivative works, with the requirement that changes to the original code are documented.\nIt is a more complex license than the MIT License and can be incompatible with GPL-licensed software. The specifics of this go beyond the scope of the handbook."
  },
  {
    "objectID": "topics/software-licensing.html#adding-a-license-to-github",
    "href": "topics/software-licensing.html#adding-a-license-to-github",
    "title": "Software Licensing",
    "section": "Adding a license to GitHub",
    "text": "Adding a license to GitHub\nOn GitHub you add a license on creating a new repository, by selecting the license from the drop-down menu. If your repository already exists, add a new file called “LICENSE” using the “+”-button on top of the repository (see below).\n\n\n\nLocation of file creation button\n\n\nOne the next page, start type LICENSE as the file name, and a button to “Choose a licence template” should automatically pop up. Follow the steps provided by GitHub to finish adding the license to the repository.\nYou should now see your license shown on the main page of your repository."
  },
  {
    "objectID": "topics/software-licensing.html#further-considerations",
    "href": "topics/software-licensing.html#further-considerations",
    "title": "Software Licensing",
    "section": "Further considerations",
    "text": "Further considerations\n\nIf you are reusing software or libraries written by someone else, you must stick to the clauses of the licence given to the original software/library;\nWhen choosing a licence, do not just think about what others may do with the software, but also what you might want to do with the software in the future."
  }
]